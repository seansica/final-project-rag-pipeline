{
  "top_performers": {
    "ragas_answer_accuracy": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k4",
      "score": 0.3333333333333333,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_context_relevance": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs2048-co0-k4",
      "score": 0.33653846153846156,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 2048,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_faithfulness": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs512-co100-k4",
      "score": 0.36436202686202684,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 512,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_response_relevancy": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co50-k4",
      "score": 0.8311475420121593,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "deepeval_faithfulness": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs2048-co0-k4",
      "score": 0.9852564102564102,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 2048,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "deepeval_geval": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k4",
      "score": 0.5393954786511862,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "bertscore_evaluator": {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs512-co50-k4",
      "score": 0.8863060130522802,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "character_length": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co50-k4",
      "original_ratio": 0.6709735115303856,
      "closeness_score": 2.9496220325263054,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "word_count": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co50-k4",
      "original_ratio": 0.6635758874025527,
      "closeness_score": 2.8866350915994774,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "overall": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k4",
      "overall_score": 0.7638888888888888,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    }
  },
  "all_metrics": [
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs256-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 256,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3108974358974359,
      "ragas_context_relevance_mean": 0.2467948717948718,
      "ragas_faithfulness_mean": 0.26703435453435453,
      "ragas_response_relevancy_mean": 0.8147573290226396,
      "deepeval_faithfulness_mean": 0.9724053724053724,
      "deepeval_geval_mean": 0.5179988176749349,
      "bertscore_evaluator_mean": 0.878748492552684,
      "character_length_output": 343.1794871794872,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5835700547848267,
      "character_length_closeness": 2.3450510716253934,
      "word_count_output": 49.717948717948715,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5772981880991774,
      "word_count_closeness": 2.3110603480190766,
      "ragas_answer_accuracy_rank": 0.7916666666666666,
      "ragas_context_relevance_rank": 0.04166666666666663,
      "ragas_faithfulness_rank": 0.20833333333333337,
      "ragas_response_relevancy_rank": 0.7916666666666666,
      "deepeval_faithfulness_rank": 0.7083333333333333,
      "deepeval_geval_rank": 0.75,
      "bertscore_evaluator_rank": 0.16666666666666663,
      "character_length_rank": 0.5833333333333333,
      "word_count_rank": 0.5416666666666667,
      "overall_score": 0.5092592592592592
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs256-co50-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 256,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.30128205128205127,
      "ragas_context_relevance_mean": 0.27884615384615385,
      "ragas_faithfulness_mean": 0.2991910866910866,
      "ragas_response_relevancy_mean": 0.7800855922103617,
      "deepeval_faithfulness_mean": 0.9138888888888889,
      "deepeval_geval_mean": 0.5094255805806344,
      "bertscore_evaluator_mean": 0.8787457912396162,
      "character_length_output": 342.02564102564105,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5741607885082811,
      "character_length_closeness": 2.2944241216327557,
      "word_count_output": 49.69230769230769,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5725449423320053,
      "word_count_closeness": 2.285949110591715,
      "ragas_answer_accuracy_rank": 0.7083333333333333,
      "ragas_context_relevance_rank": 0.45833333333333337,
      "ragas_faithfulness_rank": 0.45833333333333337,
      "ragas_response_relevancy_rank": 0.16666666666666663,
      "deepeval_faithfulness_rank": 0.0,
      "deepeval_geval_rank": 0.5416666666666667,
      "bertscore_evaluator_rank": 0.125,
      "character_length_rank": 0.5,
      "word_count_rank": 0.5,
      "overall_score": 0.38425925925925924
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs256-co100-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 256,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28846153846153844,
      "ragas_context_relevance_mean": 0.27564102564102566,
      "ragas_faithfulness_mean": 0.2856587856587856,
      "ragas_response_relevancy_mean": 0.786991610896246,
      "deepeval_faithfulness_mean": 0.9519230769230768,
      "deepeval_geval_mean": 0.505294676193158,
      "bertscore_evaluator_mean": 0.8782637042877002,
      "character_length_output": 356.8205128205128,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6058439706534509,
      "character_length_closeness": 2.4742919253656273,
      "word_count_output": 51.62820512820513,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.60041456722945,
      "word_count_closeness": 2.44149308054176,
      "ragas_answer_accuracy_rank": 0.41666666666666663,
      "ragas_context_relevance_rank": 0.375,
      "ragas_faithfulness_rank": 0.33333333333333337,
      "ragas_response_relevancy_rank": 0.33333333333333337,
      "deepeval_faithfulness_rank": 0.125,
      "deepeval_geval_rank": 0.5,
      "bertscore_evaluator_rank": 0.08333333333333337,
      "character_length_rank": 0.6666666666666667,
      "word_count_rank": 0.6666666666666667,
      "overall_score": 0.388888888888889
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs512-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 512,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3076923076923077,
      "ragas_context_relevance_mean": 0.25,
      "ragas_faithfulness_mean": 0.3203347578347579,
      "ragas_response_relevancy_mean": 0.8087268264856574,
      "deepeval_faithfulness_mean": 0.9423076923076923,
      "deepeval_geval_mean": 0.5257264731504685,
      "bertscore_evaluator_mean": 0.8799776572447556,
      "character_length_output": 341.64102564102564,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5763195979074249,
      "character_length_closeness": 2.3058454916912203,
      "word_count_output": 49.91025641025641,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5782755269941029,
      "word_count_closeness": 2.3162921319643157,
      "ragas_answer_accuracy_rank": 0.75,
      "ragas_context_relevance_rank": 0.08333333333333337,
      "ragas_faithfulness_rank": 0.625,
      "ragas_response_relevancy_rank": 0.75,
      "deepeval_faithfulness_rank": 0.04166666666666663,
      "deepeval_geval_rank": 0.7916666666666666,
      "bertscore_evaluator_rank": 0.41666666666666663,
      "character_length_rank": 0.5416666666666667,
      "word_count_rank": 0.5833333333333333,
      "overall_score": 0.5092592592592592
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs512-co100-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.30128205128205127,
      "ragas_context_relevance_mean": 0.2692307692307692,
      "ragas_faithfulness_mean": 0.36436202686202684,
      "ragas_response_relevancy_mean": 0.7832536757089965,
      "deepeval_faithfulness_mean": 0.9643162393162393,
      "deepeval_geval_mean": 0.5276535011201473,
      "bertscore_evaluator_mean": 0.8794008057851058,
      "character_length_output": 350.4102564102564,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6088457405401196,
      "character_length_closeness": 2.492806635897158,
      "word_count_output": 50.166666666666664,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6015778146521389,
      "word_count_closeness": 2.4484468177170164,
      "ragas_answer_accuracy_rank": 0.7083333333333333,
      "ragas_context_relevance_rank": 0.29166666666666663,
      "ragas_faithfulness_rank": 0.9583333333333334,
      "ragas_response_relevancy_rank": 0.25,
      "deepeval_faithfulness_rank": 0.5416666666666667,
      "deepeval_geval_rank": 0.8333333333333334,
      "bertscore_evaluator_rank": 0.33333333333333337,
      "character_length_rank": 0.7083333333333333,
      "word_count_rank": 0.7083333333333333,
      "overall_score": 0.5925925925925926
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs512-co50-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 512,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3173076923076923,
      "ragas_context_relevance_mean": 0.27884615384615385,
      "ragas_faithfulness_mean": 0.3267593517593518,
      "ragas_response_relevancy_mean": 0.8180361128815843,
      "deepeval_faithfulness_mean": 0.9615384615384616,
      "deepeval_geval_mean": 0.5284917013763107,
      "bertscore_evaluator_mean": 0.8793585224029346,
      "character_length_output": 350.0769230769231,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5952347038714705,
      "character_length_closeness": 2.4110021000650814,
      "word_count_output": 50.69230769230769,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5905983607473998,
      "word_count_closeness": 2.384349288147901,
      "ragas_answer_accuracy_rank": 0.9166666666666666,
      "ragas_context_relevance_rank": 0.45833333333333337,
      "ragas_faithfulness_rank": 0.6666666666666667,
      "ragas_response_relevancy_rank": 0.875,
      "deepeval_faithfulness_rank": 0.375,
      "deepeval_geval_rank": 0.875,
      "bertscore_evaluator_rank": 0.29166666666666663,
      "character_length_rank": 0.625,
      "word_count_rank": 0.625,
      "overall_score": 0.6342592592592593
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2980769230769231,
      "ragas_context_relevance_mean": 0.25961538461538464,
      "ragas_faithfulness_mean": 0.3412365412365412,
      "ragas_response_relevancy_mean": 0.8041913179421617,
      "deepeval_faithfulness_mean": 0.963980463980464,
      "deepeval_geval_mean": 0.5176665429069828,
      "bertscore_evaluator_mean": 0.8796680355683352,
      "character_length_output": 373.05128205128204,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6203490960959068,
      "character_length_closeness": 2.566399795253998,
      "word_count_output": 53.294871794871796,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6106562894099651,
      "word_count_closeness": 2.5041085498066025,
      "ragas_answer_accuracy_rank": 0.625,
      "ragas_context_relevance_rank": 0.125,
      "ragas_faithfulness_rank": 0.75,
      "ragas_response_relevancy_rank": 0.625,
      "deepeval_faithfulness_rank": 0.5,
      "deepeval_geval_rank": 0.625,
      "bertscore_evaluator_rank": 0.375,
      "character_length_rank": 0.75,
      "word_count_rank": 0.75,
      "overall_score": 0.5694444444444444
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co50-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2980769230769231,
      "ragas_context_relevance_mean": 0.28205128205128205,
      "ragas_faithfulness_mean": 0.3401866651866652,
      "ragas_response_relevancy_mean": 0.8311475420121593,
      "deepeval_faithfulness_mean": 0.9638583638583638,
      "deepeval_geval_mean": 0.5363146376111675,
      "bertscore_evaluator_mean": 0.8801407760534531,
      "character_length_output": 394.38461538461536,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6709735115303856,
      "character_length_closeness": 2.9496220325263054,
      "word_count_output": 56.52564102564103,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6635758874025527,
      "word_count_closeness": 2.8866350915994774,
      "ragas_answer_accuracy_rank": 0.625,
      "ragas_context_relevance_rank": 0.5833333333333333,
      "ragas_faithfulness_rank": 0.7083333333333333,
      "ragas_response_relevancy_rank": 0.9583333333333334,
      "deepeval_faithfulness_rank": 0.45833333333333337,
      "deepeval_geval_rank": 0.9166666666666666,
      "bertscore_evaluator_rank": 0.45833333333333337,
      "character_length_rank": 0.9583333333333334,
      "word_count_rank": 0.9583333333333334,
      "overall_score": 0.736111111111111
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3333333333333333,
      "ragas_context_relevance_mean": 0.2980769230769231,
      "ragas_faithfulness_mean": 0.36158933658933656,
      "ragas_response_relevancy_mean": 0.8187031938579312,
      "deepeval_faithfulness_mean": 0.9737891737891738,
      "deepeval_geval_mean": 0.5393954786511862,
      "bertscore_evaluator_mean": 0.8770919083020626,
      "character_length_output": 368.7564102564103,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6314912511539281,
      "character_length_closeness": 2.6419468586885158,
      "word_count_output": 53.41025641025641,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6279096627814718,
      "word_count_closeness": 2.61718212315867,
      "ragas_answer_accuracy_rank": 0.9583333333333334,
      "ragas_context_relevance_rank": 0.75,
      "ragas_faithfulness_rank": 0.9166666666666666,
      "ragas_response_relevancy_rank": 0.9166666666666666,
      "deepeval_faithfulness_rank": 0.7916666666666666,
      "deepeval_geval_rank": 0.9583333333333334,
      "bertscore_evaluator_rank": 0.0,
      "character_length_rank": 0.7916666666666666,
      "word_count_rank": 0.7916666666666666,
      "overall_score": 0.7638888888888888
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs2048-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 2048,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3141025641025641,
      "ragas_context_relevance_mean": 0.33653846153846156,
      "ragas_faithfulness_mean": 0.3501373626373626,
      "ragas_response_relevancy_mean": 0.7824134990406153,
      "deepeval_faithfulness_mean": 0.9852564102564102,
      "deepeval_geval_mean": 0.5179482135983082,
      "bertscore_evaluator_mean": 0.8788425960601904,
      "character_length_output": 386.8333333333333,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6487849065512596,
      "character_length_closeness": 2.768433595762545,
      "word_count_output": 55.64102564102564,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6396783417426354,
      "word_count_closeness": 2.700355158015155,
      "ragas_answer_accuracy_rank": 0.8333333333333334,
      "ragas_context_relevance_rank": 0.9583333333333334,
      "ragas_faithfulness_rank": 0.875,
      "ragas_response_relevancy_rank": 0.20833333333333337,
      "deepeval_faithfulness_rank": 0.9583333333333334,
      "deepeval_geval_rank": 0.7083333333333333,
      "bertscore_evaluator_rank": 0.20833333333333337,
      "character_length_rank": 0.8333333333333334,
      "word_count_rank": 0.8333333333333334,
      "overall_score": 0.7129629629629629
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs2048-co50-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2980769230769231,
      "ragas_context_relevance_mean": 0.3301282051282051,
      "ragas_faithfulness_mean": 0.3442867317867318,
      "ragas_response_relevancy_mean": 0.794519668410665,
      "deepeval_faithfulness_mean": 0.9745726495726496,
      "deepeval_geval_mean": 0.5138337829842601,
      "bertscore_evaluator_mean": 0.8782549080176231,
      "character_length_output": 387.3974358974359,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6613693278025995,
      "character_length_closeness": 2.8683649482045093,
      "word_count_output": 56.42307692307692,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6599925493404529,
      "word_count_closeness": 2.857082036726989,
      "ragas_answer_accuracy_rank": 0.625,
      "ragas_context_relevance_rank": 0.875,
      "ragas_faithfulness_rank": 0.8333333333333334,
      "ragas_response_relevancy_rank": 0.45833333333333337,
      "deepeval_faithfulness_rank": 0.875,
      "deepeval_geval_rank": 0.5833333333333333,
      "bertscore_evaluator_rank": 0.04166666666666663,
      "character_length_rank": 0.9166666666666666,
      "word_count_rank": 0.9166666666666666,
      "overall_score": 0.6805555555555557
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs2048-co100-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 2048,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3173076923076923,
      "ragas_context_relevance_mean": 0.2948717948717949,
      "ragas_faithfulness_mean": 0.3420121545121545,
      "ragas_response_relevancy_mean": 0.8085232103405459,
      "deepeval_faithfulness_mean": 0.9658119658119659,
      "deepeval_geval_mean": 0.5176845452007988,
      "bertscore_evaluator_mean": 0.8789391280748905,
      "character_length_output": 380.6923076923077,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6563892131091814,
      "character_length_closeness": 2.827968028896023,
      "word_count_output": 55.06410256410256,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6517015565609744,
      "word_count_closeness": 2.7909694231484363,
      "ragas_answer_accuracy_rank": 0.9166666666666666,
      "ragas_context_relevance_rank": 0.7083333333333333,
      "ragas_faithfulness_rank": 0.7916666666666666,
      "ragas_response_relevancy_rank": 0.7083333333333333,
      "deepeval_faithfulness_rank": 0.5833333333333333,
      "deepeval_geval_rank": 0.6666666666666667,
      "bertscore_evaluator_rank": 0.25,
      "character_length_rank": 0.875,
      "word_count_rank": 0.875,
      "overall_score": 0.7083333333333334
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs256-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 256,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2564102564102564,
      "ragas_context_relevance_mean": 0.2467948717948718,
      "ragas_faithfulness_mean": 0.25075138536677,
      "ragas_response_relevancy_mean": 0.7773876355773708,
      "deepeval_faithfulness_mean": 0.9694444444444446,
      "deepeval_geval_mean": 0.47226086817760643,
      "bertscore_evaluator_mean": 0.8845851979194543,
      "character_length_output": 360.34615384615387,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.728608405793284,
      "character_length_closeness": 1.3538973996998787,
      "word_count_output": 52.67948717948718,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.730629252377251,
      "word_count_closeness": 1.3502032181286763,
      "ragas_answer_accuracy_rank": 0.08333333333333337,
      "ragas_context_relevance_rank": 0.04166666666666663,
      "ragas_faithfulness_rank": 0.04166666666666663,
      "ragas_response_relevancy_rank": 0.08333333333333337,
      "deepeval_faithfulness_rank": 0.6666666666666667,
      "deepeval_geval_rank": 0.20833333333333337,
      "bertscore_evaluator_rank": 0.875,
      "character_length_rank": 0.375,
      "word_count_rank": 0.41666666666666663,
      "overall_score": 0.3101851851851852
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs256-co50-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 256,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2532051282051282,
      "ragas_context_relevance_mean": 0.266025641025641,
      "ragas_faithfulness_mean": 0.23737281237281233,
      "ragas_response_relevancy_mean": 0.8075495775971697,
      "deepeval_faithfulness_mean": 0.9745726495726496,
      "deepeval_geval_mean": 0.4800692890310906,
      "bertscore_evaluator_mean": 0.883242401557091,
      "character_length_output": 353.0128205128205,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7110662468514737,
      "character_length_closeness": 1.3868351269616168,
      "word_count_output": 52.15384615384615,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7401595737165978,
      "word_count_closeness": 1.3330497070717773,
      "ragas_answer_accuracy_rank": 0.0,
      "ragas_context_relevance_rank": 0.20833333333333337,
      "ragas_faithfulness_rank": 0.0,
      "ragas_response_relevancy_rank": 0.6666666666666667,
      "deepeval_faithfulness_rank": 0.875,
      "deepeval_geval_rank": 0.33333333333333337,
      "bertscore_evaluator_rank": 0.625,
      "character_length_rank": 0.41666666666666663,
      "word_count_rank": 0.33333333333333337,
      "overall_score": 0.3842592592592593
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs256-co100-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 256,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2724358974358974,
      "ragas_context_relevance_mean": 0.28205128205128205,
      "ragas_faithfulness_mean": 0.2576210826210826,
      "ragas_response_relevancy_mean": 0.8038062367253275,
      "deepeval_faithfulness_mean": 0.9467236467236466,
      "deepeval_geval_mean": 0.48875717021191706,
      "bertscore_evaluator_mean": 0.8837132706091955,
      "character_length_output": 361.94871794871796,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7292217410692068,
      "character_length_closeness": 1.3527740655376352,
      "word_count_output": 52.82051282051282,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7357765075871323,
      "word_count_closeness": 1.3408842861454249,
      "ragas_answer_accuracy_rank": 0.25,
      "ragas_context_relevance_rank": 0.5833333333333333,
      "ragas_faithfulness_rank": 0.08333333333333337,
      "ragas_response_relevancy_rank": 0.5833333333333333,
      "deepeval_faithfulness_rank": 0.08333333333333337,
      "deepeval_geval_rank": 0.41666666666666663,
      "bertscore_evaluator_rank": 0.7083333333333333,
      "character_length_rank": 0.33333333333333337,
      "word_count_rank": 0.375,
      "overall_score": 0.3796296296296296
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs512-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 512,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2980769230769231,
      "ragas_context_relevance_mean": 0.2692307692307692,
      "ragas_faithfulness_mean": 0.26443001443001446,
      "ragas_response_relevancy_mean": 0.8002751209262498,
      "deepeval_faithfulness_mean": 0.9564102564102566,
      "deepeval_geval_mean": 0.47827231271878134,
      "bertscore_evaluator_mean": 0.8854879240194956,
      "character_length_output": 375.2692307692308,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8064142029283414,
      "character_length_closeness": 1.2248684508588494,
      "word_count_output": 54.98717948717949,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.83553555778493,
      "word_count_closeness": 1.1826823730745568,
      "ragas_answer_accuracy_rank": 0.625,
      "ragas_context_relevance_rank": 0.29166666666666663,
      "ragas_faithfulness_rank": 0.16666666666666663,
      "ragas_response_relevancy_rank": 0.5416666666666667,
      "deepeval_faithfulness_rank": 0.29166666666666663,
      "deepeval_geval_rank": 0.29166666666666663,
      "bertscore_evaluator_rank": 0.9166666666666666,
      "character_length_rank": 0.125,
      "word_count_rank": 0.16666666666666663,
      "overall_score": 0.37962962962962954
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs512-co50-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 512,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2948717948717949,
      "ragas_context_relevance_mean": 0.28205128205128205,
      "ragas_faithfulness_mean": 0.2762464387464387,
      "ragas_response_relevancy_mean": 0.8155076722094167,
      "deepeval_faithfulness_mean": 0.9805555555555555,
      "deepeval_geval_mean": 0.48595277498957656,
      "bertscore_evaluator_mean": 0.8863060130522802,
      "character_length_output": 368.38461538461536,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7821232105673561,
      "character_length_closeness": 1.2624298678027028,
      "word_count_output": 53.57692307692308,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7795103979909364,
      "word_count_closeness": 1.2666077641848614,
      "ragas_answer_accuracy_rank": 0.45833333333333337,
      "ragas_context_relevance_rank": 0.5833333333333333,
      "ragas_faithfulness_rank": 0.29166666666666663,
      "ragas_response_relevancy_rank": 0.8333333333333334,
      "deepeval_faithfulness_rank": 0.9166666666666666,
      "deepeval_geval_rank": 0.375,
      "bertscore_evaluator_rank": 0.9583333333333334,
      "character_length_rank": 0.20833333333333337,
      "word_count_rank": 0.29166666666666663,
      "overall_score": 0.5462962962962963
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs512-co100-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2564102564102564,
      "ragas_context_relevance_mean": 0.28846153846153844,
      "ragas_faithfulness_mean": 0.3079670329670329,
      "ragas_response_relevancy_mean": 0.77402602552029,
      "deepeval_faithfulness_mean": 0.9558913308913309,
      "deepeval_geval_mean": 0.4753214786666166,
      "bertscore_evaluator_mean": 0.8833496578228779,
      "character_length_output": 360.61538461538464,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7035177764004057,
      "character_length_closeness": 1.4015067782121082,
      "word_count_output": 53.05128205128205,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7288547218897878,
      "word_count_closeness": 1.3534460434146975,
      "ragas_answer_accuracy_rank": 0.08333333333333337,
      "ragas_context_relevance_rank": 0.625,
      "ragas_faithfulness_rank": 0.5,
      "ragas_response_relevancy_rank": 0.04166666666666663,
      "deepeval_faithfulness_rank": 0.25,
      "deepeval_geval_rank": 0.25,
      "bertscore_evaluator_rank": 0.6666666666666667,
      "character_length_rank": 0.45833333333333337,
      "word_count_rank": 0.45833333333333337,
      "overall_score": 0.37037037037037046
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.27564102564102566,
      "ragas_context_relevance_mean": 0.266025641025641,
      "ragas_faithfulness_mean": 0.26242044992044994,
      "ragas_response_relevancy_mean": 0.7858349592430346,
      "deepeval_faithfulness_mean": 0.9736111111111111,
      "deepeval_geval_mean": 0.4718261169151622,
      "bertscore_evaluator_mean": 0.8842795995565561,
      "character_length_output": 376.47435897435895,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7959238270786813,
      "character_length_closeness": 1.2408120549367643,
      "word_count_output": 55.166666666666664,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.839191947226978,
      "word_count_closeness": 1.177590064608459,
      "ragas_answer_accuracy_rank": 0.33333333333333337,
      "ragas_context_relevance_rank": 0.20833333333333337,
      "ragas_faithfulness_rank": 0.125,
      "ragas_response_relevancy_rank": 0.29166666666666663,
      "deepeval_faithfulness_rank": 0.75,
      "deepeval_geval_rank": 0.16666666666666663,
      "bertscore_evaluator_rank": 0.75,
      "character_length_rank": 0.16666666666666663,
      "word_count_rank": 0.125,
      "overall_score": 0.32407407407407407
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co50-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.266025641025641,
      "ragas_context_relevance_mean": 0.2724358974358974,
      "ragas_faithfulness_mean": 0.29601061331830564,
      "ragas_response_relevancy_mean": 0.7792322267289595,
      "deepeval_faithfulness_mean": 0.9527777777777777,
      "deepeval_geval_mean": 0.47117167444182,
      "bertscore_evaluator_mean": 0.881738856052741,
      "character_length_output": 389.14102564102564,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8660453719478964,
      "character_length_closeness": 1.1414933883806602,
      "word_count_output": 56.30769230769231,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.882565808562021,
      "word_count_closeness": 1.1203655690229297,
      "ragas_answer_accuracy_rank": 0.125,
      "ragas_context_relevance_rank": 0.33333333333333337,
      "ragas_faithfulness_rank": 0.41666666666666663,
      "ragas_response_relevancy_rank": 0.125,
      "deepeval_faithfulness_rank": 0.16666666666666663,
      "deepeval_geval_rank": 0.125,
      "bertscore_evaluator_rank": 0.5,
      "character_length_rank": 0.04166666666666663,
      "word_count_rank": 0.08333333333333337,
      "overall_score": 0.21296296296296294
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2724358974358974,
      "ragas_context_relevance_mean": 0.2948717948717949,
      "ragas_faithfulness_mean": 0.29267954267954277,
      "ragas_response_relevancy_mean": 0.7876088262262392,
      "deepeval_faithfulness_mean": 0.9617521367521369,
      "deepeval_geval_mean": 0.5000349783864768,
      "bertscore_evaluator_mean": 0.8845463074170626,
      "character_length_output": 367.9871794871795,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7753366773104062,
      "character_length_closeness": 1.2733392300290434,
      "word_count_output": 54.166666666666664,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8199122609200906,
      "word_count_closeness": 1.2049466516994698,
      "ragas_answer_accuracy_rank": 0.25,
      "ragas_context_relevance_rank": 0.7083333333333333,
      "ragas_faithfulness_rank": 0.375,
      "ragas_response_relevancy_rank": 0.375,
      "deepeval_faithfulness_rank": 0.41666666666666663,
      "deepeval_geval_rank": 0.45833333333333337,
      "bertscore_evaluator_rank": 0.8333333333333334,
      "character_length_rank": 0.25,
      "word_count_rank": 0.20833333333333337,
      "overall_score": 0.4305555555555556
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs2048-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 2048,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28205128205128205,
      "ragas_context_relevance_mean": 0.3301282051282051,
      "ragas_faithfulness_mean": 0.275335810912734,
      "ragas_response_relevancy_mean": 0.7917065597364308,
      "deepeval_faithfulness_mean": 0.9692307692307691,
      "deepeval_geval_mean": 0.45738431743440744,
      "bertscore_evaluator_mean": 0.8825103999712528,
      "character_length_output": 394.0128205128205,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8943759970998058,
      "character_length_closeness": 1.1057347864238387,
      "word_count_output": 57.57692307692308,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.9199851373056418,
      "word_count_closeness": 1.075286001771174,
      "ragas_answer_accuracy_rank": 0.375,
      "ragas_context_relevance_rank": 0.875,
      "ragas_faithfulness_rank": 0.25,
      "ragas_response_relevancy_rank": 0.41666666666666663,
      "deepeval_faithfulness_rank": 0.625,
      "deepeval_geval_rank": 0.0,
      "bertscore_evaluator_rank": 0.5833333333333333,
      "character_length_rank": 0.0,
      "word_count_rank": 0.0,
      "overall_score": 0.3472222222222222
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs2048-co50-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.27564102564102566,
      "ragas_context_relevance_mean": 0.3333333333333333,
      "ragas_faithfulness_mean": 0.3177356831202985,
      "ragas_response_relevancy_mean": 0.7639732481392165,
      "deepeval_faithfulness_mean": 0.9612179487179486,
      "deepeval_geval_mean": 0.4610740214815202,
      "bertscore_evaluator_mean": 0.882353880466559,
      "character_length_output": 385.53846153846155,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8567195851686695,
      "character_length_closeness": 1.1537757045208494,
      "word_count_output": 56.794871794871796,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.896905381351067,
      "word_count_closeness": 1.1026508614495647,
      "ragas_answer_accuracy_rank": 0.33333333333333337,
      "ragas_context_relevance_rank": 0.9166666666666666,
      "ragas_faithfulness_rank": 0.5833333333333333,
      "ragas_response_relevancy_rank": 0.0,
      "deepeval_faithfulness_rank": 0.33333333333333337,
      "deepeval_geval_rank": 0.04166666666666663,
      "bertscore_evaluator_rank": 0.5416666666666667,
      "character_length_rank": 0.08333333333333337,
      "word_count_rank": 0.04166666666666663,
      "overall_score": 0.3194444444444444
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs2048-co100-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 2048,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2692307692307692,
      "ragas_context_relevance_mean": 0.30128205128205127,
      "ragas_faithfulness_mean": 0.31414326414326416,
      "ragas_response_relevancy_mean": 0.7989427936798342,
      "deepeval_faithfulness_mean": 0.9553418803418803,
      "deepeval_geval_mean": 0.46665271596745794,
      "bertscore_evaluator_mean": 0.8842872052620618,
      "character_length_output": 374.05128205128204,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.767209150969739,
      "character_length_closeness": 1.286654948352423,
      "word_count_output": 54.56410256410256,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7859557091343334,
      "word_count_closeness": 1.2563513126723864,
      "ragas_answer_accuracy_rank": 0.16666666666666663,
      "ragas_context_relevance_rank": 0.7916666666666666,
      "ragas_faithfulness_rank": 0.5416666666666667,
      "ragas_response_relevancy_rank": 0.5,
      "deepeval_faithfulness_rank": 0.20833333333333337,
      "deepeval_geval_rank": 0.08333333333333337,
      "bertscore_evaluator_rank": 0.7916666666666666,
      "character_length_rank": 0.29166666666666663,
      "word_count_rank": 0.25,
      "overall_score": 0.4027777777777778
    }
  ]
}