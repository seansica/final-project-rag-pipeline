{
  "top_performers": {
    "ragas_answer_accuracy": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "score": 0.3173076923076923,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_context_relevance": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "score": 0.1794871794871795,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_faithfulness": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "score": 0.20718864468864467,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_response_relevancy": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "score": 0.847033088491177,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "character_length": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-MiniLM-L6-v2-cs128-co0-k4",
      "original_ratio": 0.60757852956519,
      "closeness_score": 2.4849568759825758,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-MiniLM-L6-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "word_count": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "original_ratio": 0.6003123948039606,
      "closeness_score": 2.4408841939982304,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "overall": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "overall_score": 0.7833333333333333,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    }
  },
  "all_metrics": [
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2916666666666667,
      "ragas_context_relevance_mean": 0.17307692307692307,
      "ragas_faithfulness_mean": 0.20718864468864467,
      "ragas_response_relevancy_mean": 0.8239064470400831,
      "character_length_output": 340.96153846153845,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5782664130301632,
      "character_length_closeness": 2.3162432346730193,
      "word_count_output": 49.833333333333336,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5811208078036499,
      "word_count_closeness": 2.331658933787067,
      "ragas_answer_accuracy_rank": 0.7,
      "ragas_context_relevance_rank": 0.8,
      "ragas_faithfulness_rank": 0.9,
      "ragas_response_relevancy_rank": 0.6,
      "character_length_rank": 0.7,
      "word_count_rank": 0.7,
      "overall_score": 0.7333333333333334
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-distilroberta-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-distilroberta-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28525641025641024,
      "ragas_context_relevance_mean": 0.11858974358974358,
      "ragas_faithfulness_mean": 0.16394993894993898,
      "ragas_response_relevancy_mean": 0.82491640245399,
      "character_length_output": 345.53846153846155,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5760563201977967,
      "character_length_closeness": 2.304446513556349,
      "word_count_output": 50.1025641025641,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.575289705017403,
      "word_count_closeness": 2.300382603177211,
      "ragas_answer_accuracy_rank": 0.5,
      "ragas_context_relevance_rank": 0.30000000000000004,
      "ragas_faithfulness_rank": 0.30000000000000004,
      "ragas_response_relevancy_rank": 0.7,
      "character_length_rank": 0.6,
      "word_count_rank": 0.6,
      "overall_score": 0.5
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-MiniLM-L6-v2-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-MiniLM-L6-v2",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2916666666666667,
      "ragas_context_relevance_mean": 0.11217948717948718,
      "ragas_faithfulness_mean": 0.18037749287749286,
      "ragas_response_relevancy_mean": 0.8124370047340617,
      "character_length_output": 358.11538461538464,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.60757852956519,
      "character_length_closeness": 2.4849568759825758,
      "word_count_output": 51.38461538461539,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.598891076415151,
      "word_count_closeness": 2.4324453754981787,
      "ragas_answer_accuracy_rank": 0.7,
      "ragas_context_relevance_rank": 0.0,
      "ragas_faithfulness_rank": 0.7,
      "ragas_response_relevancy_rank": 0.30000000000000004,
      "character_length_rank": 0.9,
      "word_count_rank": 0.8,
      "overall_score": 0.5666666666666668
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3173076923076923,
      "ragas_context_relevance_mean": 0.15384615384615385,
      "ragas_faithfulness_mean": 0.18096764346764346,
      "ragas_response_relevancy_mean": 0.847033088491177,
      "character_length_output": 351.05128205128204,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5982938411554878,
      "character_length_closeness": 2.428916785715773,
      "word_count_output": 51.05128205128205,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6003123948039606,
      "word_count_closeness": 2.4408841939982304,
      "ragas_answer_accuracy_rank": 0.9,
      "ragas_context_relevance_rank": 0.4,
      "ragas_faithfulness_rank": 0.8,
      "ragas_response_relevancy_rank": 0.9,
      "character_length_rank": 0.8,
      "word_count_rank": 0.9,
      "overall_score": 0.7833333333333333
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-multi-qa-mpnet-base-dot-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-dot-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.30448717948717946,
      "ragas_context_relevance_mean": 0.15705128205128205,
      "ragas_faithfulness_mean": 0.17008130758130754,
      "ragas_response_relevancy_mean": 0.8304374540476422,
      "character_length_output": 340.6666666666667,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5704669826781273,
      "character_length_closeness": 2.275141936078249,
      "word_count_output": 49.46153846153846,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5691614339435166,
      "word_count_closeness": 2.268404075771975,
      "ragas_answer_accuracy_rank": 0.8,
      "ragas_context_relevance_rank": 0.5,
      "ragas_faithfulness_rank": 0.5,
      "ragas_response_relevancy_rank": 0.8,
      "character_length_rank": 0.5,
      "word_count_rank": 0.5,
      "overall_score": 0.6
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-multi-qa-mpnet-base-dot-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-dot-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2467948717948718,
      "ragas_context_relevance_mean": 0.16666666666666666,
      "ragas_faithfulness_mean": 0.167999592999593,
      "ragas_response_relevancy_mean": 0.7713761582069466,
      "character_length_output": 356.9230769230769,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7034651132668297,
      "character_length_closeness": 1.4016102278935239,
      "word_count_output": 52.43589743589744,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7414427203811504,
      "word_count_closeness": 1.3307734214163058,
      "ragas_answer_accuracy_rank": 0.19999999999999996,
      "ragas_context_relevance_rank": 0.7,
      "ragas_faithfulness_rank": 0.4,
      "ragas_response_relevancy_rank": 0.0,
      "character_length_rank": 0.0,
      "word_count_rank": 0.0,
      "overall_score": 0.21666666666666665
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-distilroberta-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-distilroberta-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2564102564102564,
      "ragas_context_relevance_mean": 0.11538461538461539,
      "ragas_faithfulness_mean": 0.14437321937321937,
      "ragas_response_relevancy_mean": 0.8194980638894026,
      "character_length_output": 347.87179487179486,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.6646126195095503,
      "character_length_closeness": 1.4823321875108255,
      "word_count_output": 50.98717948717949,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.6924921438403204,
      "word_count_closeness": 1.423503463730271,
      "ragas_answer_accuracy_rank": 0.4,
      "ragas_context_relevance_rank": 0.09999999999999998,
      "ragas_faithfulness_rank": 0.19999999999999996,
      "ragas_response_relevancy_rank": 0.5,
      "character_length_rank": 0.30000000000000004,
      "word_count_rank": 0.30000000000000004,
      "overall_score": 0.3
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2532051282051282,
      "ragas_context_relevance_mean": 0.16666666666666666,
      "ragas_faithfulness_mean": 0.1282120657120657,
      "ragas_response_relevancy_mean": 0.816334284757874,
      "character_length_output": 356.2435897435897,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7016567988152451,
      "character_length_closeness": 1.405171708701138,
      "word_count_output": 51.88461538461539,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7158021861539343,
      "word_count_closeness": 1.3777858748250056,
      "ragas_answer_accuracy_rank": 0.30000000000000004,
      "ragas_context_relevance_rank": 0.7,
      "ragas_faithfulness_rank": 0.0,
      "ragas_response_relevancy_rank": 0.4,
      "character_length_rank": 0.09999999999999998,
      "word_count_rank": 0.19999999999999996,
      "overall_score": 0.2833333333333333
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.23717948717948717,
      "ragas_context_relevance_mean": 0.1794871794871795,
      "ragas_faithfulness_mean": 0.17364163614163614,
      "ragas_response_relevancy_mean": 0.7796017189829734,
      "character_length_output": 350.55128205128204,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.6444340592467508,
      "character_length_closeness": 1.528037830352218,
      "word_count_output": 51.205128205128204,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.6530148456643667,
      "word_count_closeness": 1.5082618534702055,
      "ragas_answer_accuracy_rank": 0.09999999999999998,
      "ragas_context_relevance_rank": 0.9,
      "ragas_faithfulness_rank": 0.6,
      "ragas_response_relevancy_rank": 0.09999999999999998,
      "character_length_rank": 0.4,
      "word_count_rank": 0.4,
      "overall_score": 0.4166666666666667
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-MiniLM-L6-v2-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-MiniLM-L6-v2",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.23076923076923078,
      "ragas_context_relevance_mean": 0.11858974358974358,
      "ragas_faithfulness_mean": 0.14406010656010657,
      "ragas_response_relevancy_mean": 0.8063651142764143,
      "character_length_output": 353.1794871794872,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.693008650421208,
      "character_length_closeness": 1.4224576033322625,
      "word_count_output": 52.02564102564103,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7322129851467332,
      "word_count_closeness": 1.3473221568635088,
      "ragas_answer_accuracy_rank": 0.0,
      "ragas_context_relevance_rank": 0.30000000000000004,
      "ragas_faithfulness_rank": 0.09999999999999998,
      "ragas_response_relevancy_rank": 0.19999999999999996,
      "character_length_rank": 0.19999999999999996,
      "word_count_rank": 0.09999999999999998,
      "overall_score": 0.15
    }
  ]
}