[
    {
        "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k8",
        "config": {
            "rag_type": "cohere",
            "team_type": "engineering",
            "embedding_model": "all-mpnet-base-v2",
            "chunk_size": 2048,
            "chunk_overlap": 50,
            "top_k": 8,
            "retriever_type": "similarity",
            "retriever_kwargs": {
                "k": 8
            },
            "templates": {
                "engineering": "templates/engineering_template_3.txt",
                "marketing": "templates/marketing_template_2.txt"
            }
        },
        "success": true,
        "elapsed_time": 809.6879312992096,
        "evaluation_type": "ragas",
        "metrics": {
            "feedback": {
                "ragas_answer_accuracy": {
                    "mean": 0.3076923076923077,
                    "median": 0.25,
                    "std": 0.27303928206477823,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_context_relevance": {
                    "mean": 0.2980769230769231,
                    "median": 0.0,
                    "std": 0.4433082762279853,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_faithfulness": {
                    "mean": 0.39720695970695974,
                    "median": 0.30952380952380953,
                    "std": 0.3741544160353061,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_response_relevancy": {
                    "mean": 0.8040478422252482,
                    "median": 0.8608924813610506,
                    "std": 0.173153339074956,
                    "min": 0.2915481799246136,
                    "max": 0.9939430663700105,
                    "count": 78
                }
            },
            "text_comparison": {
                "character_length": {
                    "mean_output": 349.52564102564105,
                    "mean_reference": 612.7307692307693,
                    "mean_ratio": 0.5954309619862759
                },
                "word_count": {
                    "mean_output": 50.85897435897436,
                    "mean_reference": 89.91025641025641,
                    "mean_ratio": 0.598026043132044
                }
            }
        }
    },
    {
        "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k12",
        "config": {
            "rag_type": "cohere",
            "team_type": "engineering",
            "embedding_model": "all-mpnet-base-v2",
            "chunk_size": 2048,
            "chunk_overlap": 50,
            "top_k": 12,
            "retriever_type": "similarity",
            "retriever_kwargs": {
                "k": 12
            },
            "templates": {
                "engineering": "templates/engineering_template_3.txt",
                "marketing": "templates/marketing_template_2.txt"
            }
        },
        "success": true,
        "elapsed_time": 835.6353189945221,
        "evaluation_type": "ragas",
        "metrics": {
            "feedback": {
                "ragas_answer_accuracy": {
                    "mean": 0.30128205128205127,
                    "median": 0.25,
                    "std": 0.2743320529177647,
                    "min": 0.0,
                    "max": 0.75,
                    "count": 78
                },
                "ragas_context_relevance": {
                    "mean": 0.30128205128205127,
                    "median": 0.0,
                    "std": 0.4456845281527042,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_faithfulness": {
                    "mean": 0.44566035816035804,
                    "median": 0.4642857142857143,
                    "std": 0.3615237863204026,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_response_relevancy": {
                    "mean": 0.7897732702652596,
                    "median": 0.847719587907138,
                    "std": 0.18245157395096856,
                    "min": 0.0,
                    "max": 0.9999999999998054,
                    "count": 78
                }
            },
            "text_comparison": {
                "character_length": {
                    "mean_output": 360.9102564102564,
                    "mean_reference": 612.7307692307693,
                    "mean_ratio": 0.6106951591078805
                },
                "word_count": {
                    "mean_output": 52.1025641025641,
                    "mean_reference": 89.91025641025641,
                    "mean_ratio": 0.6059928726049272
                }
            }
        }
    },
    {
        "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-similarity_score_threshold-0.8",
        "config": {
            "rag_type": "cohere",
            "team_type": "engineering",
            "embedding_model": "all-mpnet-base-v2",
            "chunk_size": 2048,
            "chunk_overlap": 50,
            "top_k": 4,
            "retriever_type": "similarity_score_threshold",
            "retriever_kwargs": {
                "score_threshold": 0.8,
                "k": 4
            },
            "templates": {
                "engineering": "templates/engineering_template_3.txt",
                "marketing": "templates/marketing_template_2.txt"
            }
        },
        "success": true,
        "elapsed_time": 285.1004548072815,
        "evaluation_type": "ragas",
        "metrics": {
            "feedback": {
                "ragas_answer_accuracy": {
                    "mean": 0.30128205128205127,
                    "median": 0.25,
                    "std": 0.25596448096916785,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_context_relevance": {
                    "mean": 0.0,
                    "median": 0.0,
                    "std": 0.0,
                    "min": 0.0,
                    "max": 0.0,
                    "count": 78
                },
                "ragas_faithfulness": {
                    "mean": 0.0,
                    "median": 0.0,
                    "std": 0.0,
                    "min": 0.0,
                    "max": 0.0,
                    "count": 78
                },
                "ragas_response_relevancy": {
                    "mean": 0.8175573370813026,
                    "median": 0.846185766734816,
                    "std": 0.15499128779378782,
                    "min": 0.0,
                    "max": 0.9963686390872097,
                    "count": 78
                }
            },
            "text_comparison": {
                "character_length": {
                    "mean_output": 344.2564102564103,
                    "mean_reference": 612.7307692307693,
                    "mean_ratio": 0.5770455134973493
                },
                "word_count": {
                    "mean_output": 48.67948717948718,
                    "mean_reference": 89.91025641025641,
                    "mean_ratio": 0.5603889950637934
                }
            }
        }
    },
    {
        "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-similarity_score_threshold-0.5",
        "config": {
            "rag_type": "cohere",
            "team_type": "engineering",
            "embedding_model": "all-mpnet-base-v2",
            "chunk_size": 2048,
            "chunk_overlap": 50,
            "top_k": 4,
            "retriever_type": "similarity_score_threshold",
            "retriever_kwargs": {
                "score_threshold": 0.5,
                "k": 4
            },
            "templates": {
                "engineering": "templates/engineering_template_3.txt",
                "marketing": "templates/marketing_template_2.txt"
            }
        },
        "success": true,
        "elapsed_time": 548.7492790222168,
        "evaluation_type": "ragas",
        "metrics": {
            "feedback": {
                "ragas_answer_accuracy": {
                    "mean": 0.34294871794871795,
                    "median": 0.5,
                    "std": 0.27654223497615643,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_context_relevance": {
                    "mean": 0.2403846153846154,
                    "median": 0.0,
                    "std": 0.4117635253385137,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_faithfulness": {
                    "mean": 0.24598087098087099,
                    "median": 0.0,
                    "std": 0.360771661373168,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_response_relevancy": {
                    "mean": 0.821804604069871,
                    "median": 0.8369075718889716,
                    "std": 0.11825182762612074,
                    "min": 0.4543769065384368,
                    "max": 0.9999999999998885,
                    "count": 78
                }
            },
            "text_comparison": {
                "character_length": {
                    "mean_output": 359.2564102564103,
                    "mean_reference": 612.7307692307693,
                    "mean_ratio": 0.6050065515672239
                },
                "word_count": {
                    "mean_output": 51.53846153846154,
                    "mean_reference": 89.91025641025641,
                    "mean_ratio": 0.5961623292507979
                }
            }
        }
    },
    {
        "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-mmr",
        "config": {
            "rag_type": "cohere",
            "team_type": "engineering",
            "embedding_model": "all-mpnet-base-v2",
            "chunk_size": 2048,
            "chunk_overlap": 50,
            "top_k": 4,
            "retriever_type": "mmr",
            "retriever_kwargs": {
                "k": 4,
                "fetch_k": 8
            },
            "templates": {
                "engineering": "templates/engineering_template_3.txt",
                "marketing": "templates/marketing_template_2.txt"
            }
        },
        "success": true,
        "elapsed_time": 783.2747633457184,
        "evaluation_type": "ragas",
        "metrics": {
            "feedback": {
                "ragas_answer_accuracy": {
                    "mean": 0.3108974358974359,
                    "median": 0.25,
                    "std": 0.26477722023802647,
                    "min": 0.0,
                    "max": 0.75,
                    "count": 78
                },
                "ragas_context_relevance": {
                    "mean": 0.2948717948717949,
                    "median": 0.0,
                    "std": 0.43719811429395616,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_faithfulness": {
                    "mean": 0.3598827098827099,
                    "median": 0.25,
                    "std": 0.38241420936583786,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_response_relevancy": {
                    "mean": 0.8155726109459079,
                    "median": 0.8372977742747933,
                    "std": 0.1490481408728491,
                    "min": 0.0,
                    "max": 0.9999999999998979,
                    "count": 78
                }
            },
            "text_comparison": {
                "character_length": {
                    "mean_output": 364.0,
                    "mean_reference": 612.7307692307693,
                    "mean_ratio": 0.6147480632634384
                },
                "word_count": {
                    "mean_output": 53.743589743589745,
                    "mean_reference": 89.91025641025641,
                    "mean_ratio": 0.6202444384078193
                }
            }
        }
    },
    {
        "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-multi_query",
        "config": {
            "rag_type": "cohere",
            "team_type": "engineering",
            "embedding_model": "all-mpnet-base-v2",
            "chunk_size": 2048,
            "chunk_overlap": 50,
            "top_k": 4,
            "retriever_type": "multi_query",
            "retriever_kwargs": {
                "llm_for_queries": "cohere",
                "k": 4
            },
            "templates": {
                "engineering": "templates/engineering_template_3.txt",
                "marketing": "templates/marketing_template_2.txt"
            }
        },
        "success": true,
        "elapsed_time": 1220.173327922821,
        "evaluation_type": "ragas",
        "metrics": {
            "feedback": {
                "ragas_answer_accuracy": {
                    "mean": 0.27884615384615385,
                    "median": 0.25,
                    "std": 0.26414764057999085,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_context_relevance": {
                    "mean": 0.12179487179487179,
                    "median": 0.0,
                    "std": 0.3061692240242212,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_faithfulness": {
                    "mean": 0.38043322418322423,
                    "median": 0.3333333333333333,
                    "std": 0.3804321563860135,
                    "min": 0.0,
                    "max": 1.0,
                    "count": 78
                },
                "ragas_response_relevancy": {
                    "mean": 0.7886769011958357,
                    "median": 0.873773678117383,
                    "std": 0.24248011860134172,
                    "min": 0.0,
                    "max": 0.9999999999998778,
                    "count": 78
                }
            },
            "text_comparison": {
                "character_length": {
                    "mean_output": 940.6153846153846,
                    "mean_reference": 612.7307692307693,
                    "mean_ratio": 1.6183150613979214
                },
                "word_count": {
                    "mean_output": 130.96153846153845,
                    "mean_reference": 89.91025641025641,
                    "mean_ratio": 1.5470450377363687
                }
            }
        }
    }
]