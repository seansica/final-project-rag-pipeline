{
  "top_performers": {
    "ragas_answer_accuracy": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-similarity_score_threshold-0.5",
      "score": 0.34294871794871795,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity_score_threshold"
      }
    },
    "ragas_context_relevance": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k12",
      "score": 0.30128205128205127,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 12,
        "retriever_type": "similarity"
      }
    },
    "ragas_faithfulness": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k12",
      "score": 0.44566035816035804,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 12,
        "retriever_type": "similarity"
      }
    },
    "ragas_response_relevancy": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-similarity_score_threshold-0.5",
      "score": 0.821804604069871,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity_score_threshold"
      }
    },
    "character_length": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-mmr",
      "original_ratio": 0.6147480632634384,
      "closeness_score": 2.5300318785446145,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "mmr"
      }
    },
    "word_count": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-mmr",
      "original_ratio": 0.6202444384078193,
      "closeness_score": 2.5657106621260897,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "mmr"
      }
    },
    "overall": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-mmr",
      "overall_score": 0.6111111111111112,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "mmr"
      }
    }
  },
  "all_metrics": [
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k8",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 8,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3076923076923077,
      "ragas_context_relevance_mean": 0.2980769230769231,
      "ragas_faithfulness_mean": 0.39720695970695974,
      "ragas_response_relevancy_mean": 0.8040478422252482,
      "character_length_output": 349.52564102564105,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5954309619862759,
      "character_length_closeness": 2.4121434750438246,
      "word_count_output": 50.85897435897436,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.598026043132044,
      "word_count_closeness": 2.4273379016540972,
      "ragas_answer_accuracy_rank": 0.5,
      "ragas_context_relevance_rank": 0.6666666666666667,
      "ragas_faithfulness_rank": 0.6666666666666667,
      "ragas_response_relevancy_rank": 0.33333333333333337,
      "character_length_rank": 0.33333333333333337,
      "word_count_rank": 0.5,
      "overall_score": 0.5000000000000001
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k12",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 12,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.30128205128205127,
      "ragas_context_relevance_mean": 0.30128205128205127,
      "ragas_faithfulness_mean": 0.44566035816035804,
      "ragas_response_relevancy_mean": 0.7897732702652596,
      "character_length_output": 360.9102564102564,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6106951591078805,
      "character_length_closeness": 2.5043523082911254,
      "word_count_output": 52.1025641025641,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6059928726049272,
      "word_count_closeness": 2.475203857040161,
      "ragas_answer_accuracy_rank": 0.33333333333333337,
      "ragas_context_relevance_rank": 0.8333333333333334,
      "ragas_faithfulness_rank": 0.8333333333333334,
      "ragas_response_relevancy_rank": 0.16666666666666663,
      "character_length_rank": 0.6666666666666667,
      "word_count_rank": 0.6666666666666667,
      "overall_score": 0.5833333333333334
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-similarity_score_threshold-0.8",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity_score_threshold",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.30128205128205127,
      "ragas_context_relevance_mean": 0.0,
      "ragas_faithfulness_mean": 0.0,
      "ragas_response_relevancy_mean": 0.8175573370813026,
      "character_length_output": 344.2564102564103,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5770455134973493,
      "character_length_closeness": 2.3097116005838587,
      "word_count_output": 48.67948717948718,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5603889950637934,
      "word_count_closeness": 2.2241448474818486,
      "ragas_answer_accuracy_rank": 0.33333333333333337,
      "ragas_context_relevance_rank": 0.0,
      "ragas_faithfulness_rank": 0.0,
      "ragas_response_relevancy_rank": 0.6666666666666667,
      "character_length_rank": 0.16666666666666663,
      "word_count_rank": 0.16666666666666663,
      "overall_score": 0.22222222222222218
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-similarity_score_threshold-0.5",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity_score_threshold",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.34294871794871795,
      "ragas_context_relevance_mean": 0.2403846153846154,
      "ragas_faithfulness_mean": 0.24598087098087099,
      "ragas_response_relevancy_mean": 0.821804604069871,
      "character_length_output": 359.2564102564103,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6050065515672239,
      "character_length_closeness": 2.4691757456071235,
      "word_count_output": 51.53846153846154,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5961623292507979,
      "word_count_closeness": 2.416406409280294,
      "ragas_answer_accuracy_rank": 0.8333333333333334,
      "ragas_context_relevance_rank": 0.33333333333333337,
      "ragas_faithfulness_rank": 0.16666666666666663,
      "ragas_response_relevancy_rank": 0.8333333333333334,
      "character_length_rank": 0.5,
      "word_count_rank": 0.33333333333333337,
      "overall_score": 0.5000000000000001
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-mmr",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "mmr",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3108974358974359,
      "ragas_context_relevance_mean": 0.2948717948717949,
      "ragas_faithfulness_mean": 0.3598827098827099,
      "ragas_response_relevancy_mean": 0.8155726109459079,
      "character_length_output": 364.0,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6147480632634384,
      "character_length_closeness": 2.5300318785446145,
      "word_count_output": 53.743589743589745,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6202444384078193,
      "word_count_closeness": 2.5657106621260897,
      "ragas_answer_accuracy_rank": 0.6666666666666667,
      "ragas_context_relevance_rank": 0.5,
      "ragas_faithfulness_rank": 0.33333333333333337,
      "ragas_response_relevancy_rank": 0.5,
      "character_length_rank": 0.8333333333333334,
      "word_count_rank": 0.8333333333333334,
      "overall_score": 0.6111111111111112
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-multi_query",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "multi_query",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.27884615384615385,
      "ragas_context_relevance_mean": 0.12179487179487179,
      "ragas_faithfulness_mean": 0.38043322418322423,
      "ragas_response_relevancy_mean": 0.7886769011958357,
      "character_length_output": 940.6153846153846,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 1.6183150613979214,
      "character_length_closeness": 1.5915582188577921,
      "word_count_output": 130.96153846153845,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 1.5470450377363687,
      "word_count_closeness": 1.7951869817629853,
      "ragas_answer_accuracy_rank": 0.0,
      "ragas_context_relevance_rank": 0.16666666666666663,
      "ragas_faithfulness_rank": 0.5,
      "ragas_response_relevancy_rank": 0.0,
      "character_length_rank": 0.0,
      "word_count_rank": 0.0,
      "overall_score": 0.1111111111111111
    }
  ]
}