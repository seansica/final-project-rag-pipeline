{
  "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-mmr",
  "config": {
    "rag_type": "cohere",
    "team_type": "engineering",
    "embedding_model": "all-mpnet-base-v2",
    "chunk_size": 2048,
    "chunk_overlap": 50,
    "top_k": 4,
    "retriever_type": "mmr",
    "retriever_kwargs": {
      "k": 4
    }
  },
  "success": true,
  "elapsed_time": 1029.0445158481598,
  "evaluation_type": "ragas",
  "metrics": {
    "feedback": {
      "ragas_answer_accuracy": {
        "mean": 0.34615384615384615,
        "median": 0.25,
        "std": 0.30745807767965627,
        "min": 0.0,
        "max": 1.0,
        "count": 78
      },
      "ragas_context_relevance": {
        "mean": 0.2916666666666667,
        "median": 0.0,
        "std": 0.4402933398408669,
        "min": 0.0,
        "max": 1.0,
        "count": 78
      },
      "ragas_faithfulness": {
        "mean": 0.3317787410696436,
        "median": 0.2111111111111111,
        "std": 0.3393858846267006,
        "min": 0.0,
        "max": 1.0,
        "count": 78
      },
      "ragas_response_relevancy": {
        "mean": 0.819508634550527,
        "median": 0.8860636303969995,
        "std": 0.2234389040319318,
        "min": 0.0,
        "max": 0.9953692798586579,
        "count": 78
      }
    },
    "text_comparison": {
      "character_length": {
        "mean_output": 758.7307692307693,
        "mean_reference": 612.7307692307693,
        "mean_ratio": 1.2435723405920807
      },
      "word_count": {
        "mean_output": 108.8974358974359,
        "mean_reference": 89.91025641025641,
        "mean_ratio": 1.220828154064772
      }
    }
  }
}