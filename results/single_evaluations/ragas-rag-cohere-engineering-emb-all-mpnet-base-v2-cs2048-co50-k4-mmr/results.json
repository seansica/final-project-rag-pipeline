{
  "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4-mmr",
  "config": {
    "rag_type": "cohere",
    "team_type": "engineering",
    "embedding_model": "all-mpnet-base-v2",
    "chunk_size": 2048,
    "chunk_overlap": 50,
    "top_k": 4,
    "retriever_type": "mmr",
    "retriever_kwargs": {
      "k": 4
    }
  },
  "success": true,
  "elapsed_time": 1341.3743240833282,
  "evaluation_type": "ragas",
  "metrics": {
    "feedback": {
      "ragas_answer_accuracy": {
        "mean": 0.3333333333333333,
        "median": 0.375,
        "std": 0.28107708729871417,
        "min": 0.0,
        "max": 1.0,
        "count": 78
      },
      "ragas_context_relevance": {
        "mean": 0.30128205128205127,
        "median": 0.0,
        "std": 0.4511150945211526,
        "min": 0.0,
        "max": 1.0,
        "count": 78
      },
      "ragas_faithfulness": {
        "mean": 0.3184500949816967,
        "median": 0.18823529411764706,
        "std": 0.3172753077953301,
        "min": 0.0,
        "max": 1.0,
        "count": 78
      },
      "ragas_response_relevancy": {
        "mean": 0.8452613096928447,
        "median": 0.8810132009783305,
        "std": 0.15730597188038453,
        "min": 0.0,
        "max": 0.9959709630396713,
        "count": 78
      }
    },
    "text_comparison": {
      "character_length": {
        "mean_output": 752.2307692307693,
        "mean_reference": 612.7307692307693,
        "mean_ratio": 1.2390450781932714
      },
      "word_count": {
        "mean_output": 107.8076923076923,
        "mean_reference": 89.91025641025641,
        "mean_ratio": 1.221516781979125
      }
    }
  }
}