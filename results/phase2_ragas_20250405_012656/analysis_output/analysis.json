{
  "top_performers": {
    "ragas_answer_accuracy": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs512-co50-k4",
      "score": 0.3717948717948718,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_context_relevance": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4",
      "score": 0.2980769230769231,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_faithfulness": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co100-k4",
      "score": 0.3988016613016613,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_response_relevancy": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs256-co0-k4",
      "score": 0.8181730350124152,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 256,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "character_length": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4",
      "original_ratio": 0.6388311795171306,
      "closeness_score": 2.69419182004312,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "word_count": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4",
      "original_ratio": 0.6411771144761215,
      "closeness_score": 2.7113284973615266,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "overall": {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4",
      "overall_score": 0.7777777777777778,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    }
  },
  "all_metrics": [
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs256-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 256,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.30448717948717946,
      "ragas_context_relevance_mean": 0.22435897435897437,
      "ragas_faithfulness_mean": 0.2680046805046805,
      "ragas_response_relevancy_mean": 0.8181730350124152,
      "character_length_output": 352.2564102564103,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6055907706190146,
      "character_length_closeness": 2.4727427747647197,
      "word_count_output": 50.743589743589745,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.603410932823178,
      "word_count_closeness": 2.4594857086137756,
      "ragas_answer_accuracy_rank": 0.41666666666666663,
      "ragas_context_relevance_rank": 0.08333333333333337,
      "ragas_faithfulness_rank": 0.25,
      "ragas_response_relevancy_rank": 0.9166666666666666,
      "character_length_rank": 0.25,
      "word_count_rank": 0.33333333333333337,
      "overall_score": 0.375
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs256-co50-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 256,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2564102564102564,
      "ragas_context_relevance_mean": 0.22435897435897437,
      "ragas_faithfulness_mean": 0.2720848595848596,
      "ragas_response_relevancy_mean": 0.8130318349190397,
      "character_length_output": 356.37179487179486,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6073385528085868,
      "character_length_closeness": 2.4834759001018294,
      "word_count_output": 51.38461538461539,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6006306171825891,
      "word_count_closeness": 2.4427816098939314,
      "ragas_answer_accuracy_rank": 0.0,
      "ragas_context_relevance_rank": 0.08333333333333337,
      "ragas_faithfulness_rank": 0.33333333333333337,
      "ragas_response_relevancy_rank": 0.6666666666666667,
      "character_length_rank": 0.33333333333333337,
      "word_count_rank": 0.25,
      "overall_score": 0.27777777777777785
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs256-co100-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 256,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2692307692307692,
      "ragas_context_relevance_mean": 0.24358974358974358,
      "ragas_faithfulness_mean": 0.23787138787138787,
      "ragas_response_relevancy_mean": 0.7908043775596769,
      "character_length_output": 352.3205128205128,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6012578695659095,
      "character_length_closeness": 2.4465302828900573,
      "word_count_output": 50.833333333333336,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5938178688935494,
      "word_count_closeness": 2.4027941741309915,
      "ragas_answer_accuracy_rank": 0.08333333333333337,
      "ragas_context_relevance_rank": 0.16666666666666663,
      "ragas_faithfulness_rank": 0.0,
      "ragas_response_relevancy_rank": 0.0,
      "character_length_rank": 0.16666666666666663,
      "word_count_rank": 0.08333333333333337,
      "overall_score": 0.08333333333333333
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs512-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.34935897435897434,
      "ragas_context_relevance_mean": 0.2692307692307692,
      "ragas_faithfulness_mean": 0.2643981018981019,
      "ragas_response_relevancy_mean": 0.8135732182158909,
      "character_length_output": 351.20512820512823,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5988750053684898,
      "character_length_closeness": 2.432350290198961,
      "word_count_output": 50.93589743589744,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5971705332850663,
      "word_count_closeness": 2.422307709663851,
      "ragas_answer_accuracy_rank": 0.8333333333333334,
      "ragas_context_relevance_rank": 0.5833333333333333,
      "ragas_faithfulness_rank": 0.16666666666666663,
      "ragas_response_relevancy_rank": 0.75,
      "character_length_rank": 0.0,
      "word_count_rank": 0.16666666666666663,
      "overall_score": 0.4166666666666666
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs512-co50-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3717948717948718,
      "ragas_context_relevance_mean": 0.27884615384615385,
      "ragas_faithfulness_mean": 0.25212148962148956,
      "ragas_response_relevancy_mean": 0.8094276800033705,
      "character_length_output": 374.20512820512823,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6338823414925837,
      "character_length_closeness": 2.65874249023137,
      "word_count_output": 54.56410256410256,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.631355093643319,
      "word_count_closeness": 2.6409968369097947,
      "ragas_answer_accuracy_rank": 0.9166666666666666,
      "ragas_context_relevance_rank": 0.75,
      "ragas_faithfulness_rank": 0.08333333333333337,
      "ragas_response_relevancy_rank": 0.41666666666666663,
      "character_length_rank": 0.8333333333333334,
      "word_count_rank": 0.8333333333333334,
      "overall_score": 0.638888888888889
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs512-co100-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3301282051282051,
      "ragas_context_relevance_mean": 0.26282051282051283,
      "ragas_faithfulness_mean": 0.2990944240944241,
      "ragas_response_relevancy_mean": 0.8150564096884378,
      "character_length_output": 355.43589743589746,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6110874736258557,
      "character_length_closeness": 2.5068152386422917,
      "word_count_output": 51.69230769230769,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6083558695201662,
      "word_count_closeness": 2.4897662485577117,
      "ragas_answer_accuracy_rank": 0.75,
      "ragas_context_relevance_rank": 0.33333333333333337,
      "ragas_faithfulness_rank": 0.41666666666666663,
      "ragas_response_relevancy_rank": 0.8333333333333334,
      "character_length_rank": 0.5,
      "word_count_rank": 0.5,
      "overall_score": 0.5555555555555556
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs1024-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 1024,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28525641025641024,
      "ragas_context_relevance_mean": 0.2692307692307692,
      "ragas_faithfulness_mean": 0.3647588522588523,
      "ragas_response_relevancy_mean": 0.8063768337240677,
      "character_length_output": 351.7307692307692,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6002329720257502,
      "character_length_closeness": 2.4404110915015864,
      "word_count_output": 50.705128205128204,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5934340679583304,
      "word_count_closeness": 2.4005803717524574,
      "ragas_answer_accuracy_rank": 0.16666666666666663,
      "ragas_context_relevance_rank": 0.5833333333333333,
      "ragas_faithfulness_rank": 0.6666666666666667,
      "ragas_response_relevancy_rank": 0.16666666666666663,
      "character_length_rank": 0.08333333333333337,
      "word_count_rank": 0.0,
      "overall_score": 0.27777777777777773
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs1024-co50-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 1024,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3108974358974359,
      "ragas_context_relevance_mean": 0.266025641025641,
      "ragas_faithfulness_mean": 0.318564306064306,
      "ragas_response_relevancy_mean": 0.8108803166671803,
      "character_length_output": 356.02564102564105,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.609726515051575,
      "character_length_closeness": 2.4982918869303807,
      "word_count_output": 51.51282051282051,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6041181977132677,
      "word_count_closeness": 2.463771458503471,
      "ragas_answer_accuracy_rank": 0.5,
      "ragas_context_relevance_rank": 0.41666666666666663,
      "ragas_faithfulness_rank": 0.5,
      "ragas_response_relevancy_rank": 0.5833333333333333,
      "character_length_rank": 0.41666666666666663,
      "word_count_rank": 0.41666666666666663,
      "overall_score": 0.47222222222222215
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs1024-co100-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2948717948717949,
      "ragas_context_relevance_mean": 0.25961538461538464,
      "ragas_faithfulness_mean": 0.33259564509564515,
      "ragas_response_relevancy_mean": 0.8101954474609134,
      "character_length_output": 366.64102564102564,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6186138358748832,
      "character_length_closeness": 2.555021336115305,
      "word_count_output": 53.34615384615385,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6182905341672453,
      "word_count_closeness": 2.552912521207651,
      "ragas_answer_accuracy_rank": 0.25,
      "ragas_context_relevance_rank": 0.25,
      "ragas_faithfulness_rank": 0.5833333333333333,
      "ragas_response_relevancy_rank": 0.5,
      "character_length_rank": 0.6666666666666667,
      "word_count_rank": 0.6666666666666667,
      "overall_score": 0.48611111111111116
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.30448717948717946,
      "ragas_context_relevance_mean": 0.2916666666666667,
      "ragas_faithfulness_mean": 0.3696229696229696,
      "ragas_response_relevancy_mean": 0.7959927423068727,
      "character_length_output": 366.78205128205127,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.614341889355096,
      "character_length_closeness": 2.5274346035016126,
      "word_count_output": 53.14102564102564,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6105585929673258,
      "word_count_closeness": 2.5034960882715906,
      "ragas_answer_accuracy_rank": 0.41666666666666663,
      "ragas_context_relevance_rank": 0.8333333333333334,
      "ragas_faithfulness_rank": 0.75,
      "ragas_response_relevancy_rank": 0.08333333333333337,
      "character_length_rank": 0.5833333333333333,
      "word_count_rank": 0.5833333333333333,
      "overall_score": 0.5416666666666666
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co50-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3301282051282051,
      "ragas_context_relevance_mean": 0.2980769230769231,
      "ragas_faithfulness_mean": 0.37036841363764433,
      "ragas_response_relevancy_mean": 0.8091966099407542,
      "character_length_output": 367.8333333333333,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6388311795171306,
      "character_length_closeness": 2.69419182004312,
      "word_count_output": 53.6025641025641,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6411771144761215,
      "word_count_closeness": 2.7113284973615266,
      "ragas_answer_accuracy_rank": 0.75,
      "ragas_context_relevance_rank": 0.9166666666666666,
      "ragas_faithfulness_rank": 0.8333333333333334,
      "ragas_response_relevancy_rank": 0.33333333333333337,
      "character_length_rank": 0.9166666666666666,
      "word_count_rank": 0.9166666666666666,
      "overall_score": 0.7777777777777778
    },
    {
      "experiment_id": "ragas-rag-cohere-engineering-emb-all-mpnet-base-v2-cs2048-co100-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3269230769230769,
      "ragas_context_relevance_mean": 0.27564102564102566,
      "ragas_faithfulness_mean": 0.3988016613016613,
      "ragas_response_relevancy_mean": 0.8072181868882873,
      "character_length_output": 371.4102564102564,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6214166318299186,
      "character_length_closeness": 2.5734503375921736,
      "word_count_output": 54.12820512820513,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6201688429434887,
      "word_count_closeness": 2.5652131234216267,
      "ragas_answer_accuracy_rank": 0.5833333333333333,
      "ragas_context_relevance_rank": 0.6666666666666667,
      "ragas_faithfulness_rank": 0.9166666666666666,
      "ragas_response_relevancy_rank": 0.25,
      "character_length_rank": 0.75,
      "word_count_rank": 0.75,
      "overall_score": 0.6527777777777778
    }
  ]
}