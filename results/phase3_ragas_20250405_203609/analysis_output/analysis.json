{
  "top_performers": {
    "ragas_answer_accuracy": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k12",
      "score": 0.3076923076923077,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 100,
        "top_k": 12,
        "retriever_type": "similarity"
      }
    },
    "ragas_context_relevance": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k12",
      "score": 0.3269230769230769,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 100,
        "top_k": 12,
        "retriever_type": "similarity"
      }
    },
    "ragas_faithfulness": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k12",
      "score": 0.2835044341623289,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 100,
        "top_k": 12,
        "retriever_type": "similarity"
      }
    },
    "ragas_response_relevancy": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4-similarity_score_threshold-0.8",
      "score": 0.8180650637709594,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "similarity_score_threshold"
      }
    },
    "character_length": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4-multi_query",
      "original_ratio": 1.764222633181501,
      "closeness_score": 1.291618143337809,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "multi_query"
      }
    },
    "word_count": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4-similarity_score_threshold-0.8",
      "original_ratio": 1.7799681167738437,
      "closeness_score": 1.2658738735987307,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "similarity_score_threshold"
      }
    },
    "overall": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4-similarity_score_threshold-0.5",
      "overall_score": 0.47222222222222215,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "similarity_score_threshold"
      }
    }
  },
  "all_metrics": [
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k12",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 12,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3076923076923077,
      "ragas_context_relevance_mean": 0.3269230769230769,
      "ragas_faithfulness_mean": 0.2835044341623289,
      "ragas_response_relevancy_mean": 0.7512097813208134,
      "character_length_output": 405.46153846153845,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.9364298997844331,
      "character_length_closeness": 1.0566022905951813,
      "word_count_output": 60.35897435897436,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 2.0006983449475273,
      "word_count_closeness": 0.9894148981236506,
      "ragas_answer_accuracy_rank": 0.8333333333333334,
      "ragas_context_relevance_rank": 0.8333333333333334,
      "ragas_faithfulness_rank": 0.8333333333333334,
      "ragas_response_relevancy_rank": 0.16666666666666663,
      "character_length_rank": 0.0,
      "word_count_rank": 0.0,
      "overall_score": 0.4444444444444444
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k8",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 8,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2916666666666667,
      "ragas_context_relevance_mean": 0.2980769230769231,
      "ragas_faithfulness_mean": 0.27520869443946366,
      "ragas_response_relevancy_mean": 0.7904632022247989,
      "character_length_output": 393.20512820512823,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8816129186979218,
      "character_length_closeness": 1.1215629327806989,
      "word_count_output": 57.666666666666664,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.9162847305849693,
      "word_count_closeness": 1.079581652359181,
      "ragas_answer_accuracy_rank": 0.5,
      "ragas_context_relevance_rank": 0.6666666666666667,
      "ragas_faithfulness_rank": 0.6666666666666667,
      "ragas_response_relevancy_rank": 0.5,
      "character_length_rank": 0.16666666666666663,
      "word_count_rank": 0.16666666666666663,
      "overall_score": 0.4444444444444444
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4-similarity_score_threshold-0.8",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity_score_threshold",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.23076923076923078,
      "ragas_context_relevance_mean": 0.01282051282051282,
      "ragas_faithfulness_mean": 0.002564102564102564,
      "ragas_response_relevancy_mean": 0.8180650637709594,
      "character_length_output": 360.7692307692308,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.780232073311714,
      "character_length_closeness": 1.2654510412481588,
      "word_count_output": 52.42307692307692,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7799681167738437,
      "word_count_closeness": 1.2658738735987307,
      "ragas_answer_accuracy_rank": 0.0,
      "ragas_context_relevance_rank": 0.0,
      "ragas_faithfulness_rank": 0.0,
      "ragas_response_relevancy_rank": 0.8333333333333334,
      "character_length_rank": 0.6666666666666667,
      "word_count_rank": 0.8333333333333334,
      "overall_score": 0.3888888888888889
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4-similarity_score_threshold-0.5",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity_score_threshold",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2980769230769231,
      "ragas_context_relevance_mean": 0.26282051282051283,
      "ragas_faithfulness_mean": 0.2248556998556999,
      "ragas_response_relevancy_mean": 0.8148115771887152,
      "character_length_output": 368.1923076923077,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7855323302980257,
      "character_length_closeness": 1.2570199373611575,
      "word_count_output": 53.67948717948718,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8103639566928582,
      "word_count_closeness": 1.2189711552312834,
      "ragas_answer_accuracy_rank": 0.6666666666666667,
      "ragas_context_relevance_rank": 0.33333333333333337,
      "ragas_faithfulness_rank": 0.16666666666666663,
      "ragas_response_relevancy_rank": 0.6666666666666667,
      "character_length_rank": 0.5,
      "word_count_rank": 0.5,
      "overall_score": 0.47222222222222215
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4-mmr",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "mmr",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28205128205128205,
      "ragas_context_relevance_mean": 0.2532051282051282,
      "ragas_faithfulness_mean": 0.25406491656491653,
      "ragas_response_relevancy_mean": 0.788180062430711,
      "character_length_output": 365.05128205128204,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8017049111774663,
      "character_length_closeness": 1.231974805412217,
      "word_count_output": 53.282051282051285,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8219583108142656,
      "word_count_closeness": 1.2019833049341935,
      "ragas_answer_accuracy_rank": 0.33333333333333337,
      "ragas_context_relevance_rank": 0.16666666666666663,
      "ragas_faithfulness_rank": 0.5,
      "ragas_response_relevancy_rank": 0.33333333333333337,
      "character_length_rank": 0.33333333333333337,
      "word_count_rank": 0.33333333333333337,
      "overall_score": 0.3333333333333334
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4-multi_query",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "multi_query",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.26,
      "ragas_context_relevance_mean": 0.27564102564102566,
      "ragas_faithfulness_mean": 0.24758852258852257,
      "ragas_response_relevancy_mean": 0.7337987417038031,
      "character_length_output": 365.3076923076923,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.764222633181501,
      "character_length_closeness": 1.291618143337809,
      "word_count_output": 53.47435897435897,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7847955468670107,
      "word_count_closeness": 1.258185207430868,
      "ragas_answer_accuracy_rank": 0.16666666666666663,
      "ragas_context_relevance_rank": 0.5,
      "ragas_faithfulness_rank": 0.33333333333333337,
      "ragas_response_relevancy_rank": 0.0,
      "character_length_rank": 0.8333333333333334,
      "word_count_rank": 0.6666666666666667,
      "overall_score": 0.4166666666666667
    }
  ]
}