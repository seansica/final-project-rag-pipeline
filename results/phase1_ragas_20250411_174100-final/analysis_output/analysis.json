{
  "top_performers": {
    "correctness": {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "score": 0.4230769230769231,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "groundedness": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-dot-v1-cs128-co0-k4",
      "score": 0.2948717948717949,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-dot-v1",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "relevance": {
      "experiment_id": "v1-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "score": 0.9615384615384616,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "retrieval_relevance": {
      "experiment_id": "v1-cohere-engineering-emb-all-MiniLM-L6-v2-cs128-co0-k4",
      "score": 0.7564102564102564,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-MiniLM-L6-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_answer_accuracy": {
      "experiment_id": "v1-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "score": 0.30448717948717946,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_context_relevance": {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "score": 0.21794871794871795,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_faithfulness": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "score": 0.20430402930402927,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_response_relevancy": {
      "experiment_id": "v1-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "score": 0.848300254103529,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "deepeval_faithfulness": {
      "experiment_id": "v1-cohere-marketing-emb-all-mpnet-base-v2-cs128-co0-k4",
      "score": 0.989957264957265,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "deepeval_geval": {
      "experiment_id": "v1-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "score": 0.5119446617219069,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "bertscore_evaluator": {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-dot-v1-cs128-co0-k4",
      "score": 0.8841377145204788,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "multi-qa-mpnet-base-dot-v1",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "character_length": {
      "experiment_id": "v1-cohere-engineering-emb-all-MiniLM-L6-v2-cs128-co0-k4",
      "original_ratio": 0.610209428118558,
      "closeness_score": 2.5013096114146243,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-MiniLM-L6-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "word_count": {
      "experiment_id": "v1-cohere-engineering-emb-all-MiniLM-L6-v2-cs128-co0-k4",
      "original_ratio": 0.6073293270576372,
      "closeness_score": 2.4834190001791794,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "all-MiniLM-L6-v2",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "overall": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "overall_score": 0.6153846153846155,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 128,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    }
  },
  "all_metrics": [
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-dot-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-dot-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.38461538461538464,
      "groundedness_mean": 0.2948717948717949,
      "relevance_mean": 0.8846153846153846,
      "retrieval_relevance_mean": 0.7435897435897436,
      "ragas_answer_accuracy_mean": 0.2916666666666667,
      "ragas_context_relevance_mean": 0.15064102564102563,
      "ragas_faithfulness_mean": 0.18107800319338785,
      "ragas_response_relevancy_mean": 0.8279430778837338,
      "deepeval_faithfulness_mean": 0.9623931623931623,
      "deepeval_geval_mean": 0.49839136569795656,
      "bertscore_evaluator_mean": 0.8801473256869193,
      "character_length_output": 348.43589743589746,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5862887381544246,
      "character_length_closeness": 2.3600977600743054,
      "word_count_output": 50.51282051282051,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5844271215320653,
      "word_count_closeness": 2.349773800435796,
      "correctness_rank": 0.4,
      "groundedness_rank": 0.9,
      "relevance_rank": 0.6,
      "retrieval_relevance_rank": 0.8,
      "ragas_answer_accuracy_rank": 0.8,
      "ragas_context_relevance_rank": 0.5,
      "ragas_faithfulness_rank": 0.8,
      "ragas_response_relevancy_rank": 0.8,
      "deepeval_faithfulness_rank": 0.09999999999999998,
      "deepeval_geval_rank": 0.6,
      "bertscore_evaluator_rank": 0.30000000000000004,
      "character_length_rank": 0.6,
      "word_count_rank": 0.6,
      "overall_score": 0.5999999999999999
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-all-mpnet-base-v2-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.41025641025641024,
      "groundedness_mean": 0.24358974358974358,
      "relevance_mean": 0.9615384615384616,
      "retrieval_relevance_mean": 0.6666666666666666,
      "ragas_answer_accuracy_mean": 0.30448717948717946,
      "ragas_context_relevance_mean": 0.13782051282051283,
      "ragas_faithfulness_mean": 0.15504403004403003,
      "ragas_response_relevancy_mean": 0.848300254103529,
      "deepeval_faithfulness_mean": 0.9544871794871793,
      "deepeval_geval_mean": 0.5119446617219069,
      "bertscore_evaluator_mean": 0.87917410945281,
      "character_length_output": 360.0769230769231,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6062254582544565,
      "character_length_closeness": 2.476629645041352,
      "word_count_output": 52.15384615384615,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6045595866759106,
      "word_count_closeness": 2.466453681322213,
      "correctness_rank": 0.8,
      "groundedness_rank": 0.6,
      "relevance_rank": 0.9,
      "retrieval_relevance_rank": 0.0,
      "ragas_answer_accuracy_rank": 0.9,
      "ragas_context_relevance_rank": 0.4,
      "ragas_faithfulness_rank": 0.30000000000000004,
      "ragas_response_relevancy_rank": 0.9,
      "deepeval_faithfulness_rank": 0.0,
      "deepeval_geval_rank": 0.9,
      "bertscore_evaluator_rank": 0.09999999999999998,
      "character_length_rank": 0.8,
      "word_count_rank": 0.8,
      "overall_score": 0.5692307692307692
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-all-MiniLM-L6-v2-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-MiniLM-L6-v2",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.41025641025641024,
      "groundedness_mean": 0.2692307692307692,
      "relevance_mean": 0.9230769230769231,
      "retrieval_relevance_mean": 0.7564102564102564,
      "ragas_answer_accuracy_mean": 0.28525641025641024,
      "ragas_context_relevance_mean": 0.10256410256410256,
      "ragas_faithfulness_mean": 0.16228123728123728,
      "ragas_response_relevancy_mean": 0.8236231800711402,
      "deepeval_faithfulness_mean": 0.9760683760683759,
      "deepeval_geval_mean": 0.4977115882720513,
      "bertscore_evaluator_mean": 0.8783953663630363,
      "character_length_output": 354.9871794871795,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.610209428118558,
      "character_length_closeness": 2.5013096114146243,
      "word_count_output": 51.294871794871796,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6073293270576372,
      "word_count_closeness": 2.4834190001791794,
      "correctness_rank": 0.8,
      "groundedness_rank": 0.8,
      "relevance_rank": 0.8,
      "retrieval_relevance_rank": 0.9,
      "ragas_answer_accuracy_rank": 0.5,
      "ragas_context_relevance_rank": 0.0,
      "ragas_faithfulness_rank": 0.5,
      "ragas_response_relevancy_rank": 0.6,
      "deepeval_faithfulness_rank": 0.6,
      "deepeval_geval_rank": 0.5,
      "bertscore_evaluator_rank": 0.0,
      "character_length_rank": 0.9,
      "word_count_rank": 0.9,
      "overall_score": 0.6000000000000001
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-all-distilroberta-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "all-distilroberta-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.41025641025641024,
      "groundedness_mean": 0.24358974358974358,
      "relevance_mean": 0.9230769230769231,
      "retrieval_relevance_mean": 0.6794871794871795,
      "ragas_answer_accuracy_mean": 0.2916666666666667,
      "ragas_context_relevance_mean": 0.12179487179487179,
      "ragas_faithfulness_mean": 0.1526098901098901,
      "ragas_response_relevancy_mean": 0.8027552492581451,
      "deepeval_faithfulness_mean": 0.9713675213675214,
      "deepeval_geval_mean": 0.5039254598569932,
      "bertscore_evaluator_mean": 0.8796422565594698,
      "character_length_output": 353.28205128205127,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6019265610626485,
      "character_length_closeness": 2.450539301465104,
      "word_count_output": 51.19230769230769,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.600410768050638,
      "word_count_closeness": 2.441470434270672,
      "correctness_rank": 0.8,
      "groundedness_rank": 0.6,
      "relevance_rank": 0.8,
      "retrieval_relevance_rank": 0.19999999999999996,
      "ragas_answer_accuracy_rank": 0.8,
      "ragas_context_relevance_rank": 0.19999999999999996,
      "ragas_faithfulness_rank": 0.19999999999999996,
      "ragas_response_relevancy_rank": 0.4,
      "deepeval_faithfulness_rank": 0.4,
      "deepeval_geval_rank": 0.8,
      "bertscore_evaluator_rank": 0.19999999999999996,
      "character_length_rank": 0.7,
      "word_count_rank": 0.7,
      "overall_score": 0.5230769230769232
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-dot-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-dot-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.38461538461538464,
      "groundedness_mean": 0.24358974358974358,
      "relevance_mean": 0.8205128205128205,
      "retrieval_relevance_mean": 0.7307692307692307,
      "ragas_answer_accuracy_mean": 0.24358974358974358,
      "ragas_context_relevance_mean": 0.15705128205128205,
      "ragas_faithfulness_mean": 0.1569037444037444,
      "ragas_response_relevancy_mean": 0.7918737991356941,
      "deepeval_faithfulness_mean": 0.9712454212454212,
      "deepeval_geval_mean": 0.4508205392728722,
      "bertscore_evaluator_mean": 0.8841377145204788,
      "character_length_output": 359.2307692307692,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.742211533108004,
      "character_length_closeness": 1.3294132780285597,
      "word_count_output": 52.91025641025641,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7857030498782371,
      "word_count_closeness": 1.2567502413783955,
      "correctness_rank": 0.4,
      "groundedness_rank": 0.6,
      "relevance_rank": 0.4,
      "retrieval_relevance_rank": 0.6,
      "ragas_answer_accuracy_rank": 0.30000000000000004,
      "ragas_context_relevance_rank": 0.6,
      "ragas_faithfulness_rank": 0.4,
      "ragas_response_relevancy_rank": 0.09999999999999998,
      "deepeval_faithfulness_rank": 0.30000000000000004,
      "deepeval_geval_rank": 0.09999999999999998,
      "bertscore_evaluator_rank": 0.9,
      "character_length_rank": 0.0,
      "word_count_rank": 0.0,
      "overall_score": 0.36153846153846153
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.3974358974358974,
      "groundedness_mean": 0.2692307692307692,
      "relevance_mean": 0.8846153846153846,
      "retrieval_relevance_mean": 0.7435897435897436,
      "ragas_answer_accuracy_mean": 0.28846153846153844,
      "ragas_context_relevance_mean": 0.17307692307692307,
      "ragas_faithfulness_mean": 0.20430402930402927,
      "ragas_response_relevancy_mean": 0.8253836178478073,
      "deepeval_faithfulness_mean": 0.9683760683760685,
      "deepeval_geval_mean": 0.503699895940163,
      "bertscore_evaluator_mean": 0.8804433246453603,
      "character_length_output": 336.34615384615387,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.5716816945429657,
      "character_length_closeness": 2.2814470387160775,
      "word_count_output": 48.80769230769231,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5701079213066002,
      "word_count_closeness": 2.273284854253967,
      "correctness_rank": 0.5,
      "groundedness_rank": 0.8,
      "relevance_rank": 0.6,
      "retrieval_relevance_rank": 0.8,
      "ragas_answer_accuracy_rank": 0.6,
      "ragas_context_relevance_rank": 0.8,
      "ragas_faithfulness_rank": 0.9,
      "ragas_response_relevancy_rank": 0.7,
      "deepeval_faithfulness_rank": 0.19999999999999996,
      "deepeval_geval_rank": 0.7,
      "bertscore_evaluator_rank": 0.4,
      "character_length_rank": 0.5,
      "word_count_rank": 0.5,
      "overall_score": 0.6153846153846155
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-all-mpnet-base-v2-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.3333333333333333,
      "groundedness_mean": 0.20512820512820512,
      "relevance_mean": 0.7435897435897436,
      "retrieval_relevance_mean": 0.7307692307692307,
      "ragas_answer_accuracy_mean": 0.24358974358974358,
      "ragas_context_relevance_mean": 0.16346153846153846,
      "ragas_faithfulness_mean": 0.1464896214896215,
      "ragas_response_relevancy_mean": 0.8157353623656842,
      "deepeval_faithfulness_mean": 0.989957264957265,
      "deepeval_geval_mean": 0.4586852479019808,
      "bertscore_evaluator_mean": 0.88203276349948,
      "character_length_output": 345.3205128205128,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.634061095595364,
      "character_length_closeness": 1.5526477330160882,
      "word_count_output": 50.84615384615385,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.6632172514685954,
      "word_count_closeness": 1.4854045968348872,
      "correctness_rank": 0.0,
      "groundedness_rank": 0.19999999999999996,
      "relevance_rank": 0.09999999999999998,
      "retrieval_relevance_rank": 0.6,
      "ragas_answer_accuracy_rank": 0.30000000000000004,
      "ragas_context_relevance_rank": 0.7,
      "ragas_faithfulness_rank": 0.09999999999999998,
      "ragas_response_relevancy_rank": 0.5,
      "deepeval_faithfulness_rank": 0.9,
      "deepeval_geval_rank": 0.19999999999999996,
      "bertscore_evaluator_rank": 0.6,
      "character_length_rank": 0.4,
      "word_count_rank": 0.4,
      "overall_score": 0.38461538461538464
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-all-MiniLM-L6-v2-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-MiniLM-L6-v2",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.38461538461538464,
      "groundedness_mean": 0.19230769230769232,
      "relevance_mean": 0.8205128205128205,
      "retrieval_relevance_mean": 0.7051282051282052,
      "ragas_answer_accuracy_mean": 0.23076923076923078,
      "ragas_context_relevance_mean": 0.11217948717948718,
      "ragas_faithfulness_mean": 0.17008362008362007,
      "ragas_response_relevancy_mean": 0.7888495704144997,
      "deepeval_faithfulness_mean": 0.9834249084249084,
      "deepeval_geval_mean": 0.465018313976471,
      "bertscore_evaluator_mean": 0.8834284146626791,
      "character_length_output": 345.47435897435895,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.6864324834128261,
      "character_length_closeness": 1.4358893702079478,
      "word_count_output": 50.44871794871795,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.703851352191826,
      "word_count_closeness": 1.400851867730693,
      "correctness_rank": 0.4,
      "groundedness_rank": 0.09999999999999998,
      "relevance_rank": 0.4,
      "retrieval_relevance_rank": 0.30000000000000004,
      "ragas_answer_accuracy_rank": 0.0,
      "ragas_context_relevance_rank": 0.09999999999999998,
      "ragas_faithfulness_rank": 0.6,
      "ragas_response_relevancy_rank": 0.0,
      "deepeval_faithfulness_rank": 0.8,
      "deepeval_geval_rank": 0.4,
      "bertscore_evaluator_rank": 0.8,
      "character_length_rank": 0.09999999999999998,
      "word_count_rank": 0.09999999999999998,
      "overall_score": 0.3153846153846154
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-all-distilroberta-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-distilroberta-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.3717948717948718,
      "groundedness_mean": 0.1794871794871795,
      "relevance_mean": 0.717948717948718,
      "retrieval_relevance_mean": 0.6794871794871795,
      "ragas_answer_accuracy_mean": 0.24358974358974358,
      "ragas_context_relevance_mean": 0.125,
      "ragas_faithfulness_mean": 0.17405881155881156,
      "ragas_response_relevancy_mean": 0.7979758160366586,
      "deepeval_faithfulness_mean": 0.9829059829059827,
      "deepeval_geval_mean": 0.44385913641678,
      "bertscore_evaluator_mean": 0.8832259751283206,
      "character_length_output": 348.1666666666667,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.6701604865886253,
      "character_length_closeness": 1.4702412441151114,
      "word_count_output": 51.17948717948718,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.6994390020066577,
      "word_count_closeness": 1.4095644546909414,
      "correctness_rank": 0.09999999999999998,
      "groundedness_rank": 0.0,
      "relevance_rank": 0.0,
      "retrieval_relevance_rank": 0.19999999999999996,
      "ragas_answer_accuracy_rank": 0.30000000000000004,
      "ragas_context_relevance_rank": 0.30000000000000004,
      "ragas_faithfulness_rank": 0.7,
      "ragas_response_relevancy_rank": 0.19999999999999996,
      "deepeval_faithfulness_rank": 0.7,
      "deepeval_geval_rank": 0.0,
      "bertscore_evaluator_rank": 0.7,
      "character_length_rank": 0.19999999999999996,
      "word_count_rank": 0.19999999999999996,
      "overall_score": 0.27692307692307694
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs128-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 128,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "correctness_mean": 0.4230769230769231,
      "groundedness_mean": 0.23076923076923078,
      "relevance_mean": 0.7948717948717948,
      "retrieval_relevance_mean": 0.7307692307692307,
      "ragas_answer_accuracy_mean": 0.2532051282051282,
      "ragas_context_relevance_mean": 0.21794871794871795,
      "ragas_faithfulness_mean": 0.14382376882376882,
      "ragas_response_relevancy_mean": 0.8000909162933899,
      "deepeval_faithfulness_mean": 0.972008547008547,
      "deepeval_geval_mean": 0.4640434272263238,
      "bertscore_evaluator_mean": 0.881421202268356,
      "character_length_output": 353.37179487179486,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.661701631035433,
      "character_length_closeness": 1.4887562480062655,
      "word_count_output": 51.666666666666664,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.6764821233173588,
      "word_count_closeness": 1.4567021718899194,
      "correctness_rank": 0.9,
      "groundedness_rank": 0.30000000000000004,
      "relevance_rank": 0.19999999999999996,
      "retrieval_relevance_rank": 0.6,
      "ragas_answer_accuracy_rank": 0.4,
      "ragas_context_relevance_rank": 0.9,
      "ragas_faithfulness_rank": 0.0,
      "ragas_response_relevancy_rank": 0.30000000000000004,
      "deepeval_faithfulness_rank": 0.5,
      "deepeval_geval_rank": 0.30000000000000004,
      "bertscore_evaluator_rank": 0.5,
      "character_length_rank": 0.30000000000000004,
      "word_count_rank": 0.30000000000000004,
      "overall_score": 0.423076923076923
    }
  ]
}