{
  "top_performers": {
    "ragas_answer_accuracy": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs1024-co0-k4",
      "score": 0.2980769230769231,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 1024,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_context_relevance": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs2048-co0-k4",
      "score": 0.2980769230769231,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_faithfulness": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs2048-co50-k4",
      "score": 0.37045177045177047,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 2048,
        "chunk_overlap": 50,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "ragas_response_relevancy": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co0-k4",
      "score": 0.8270058585425863,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "character_length": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs256-co0-k4",
      "original_ratio": 1.6571510220617325,
      "closeness_score": 1.4989109915617704,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 256,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "word_count": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs256-co0-k4",
      "original_ratio": 1.695294053777469,
      "closeness_score": 1.417848335235668,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 256,
        "chunk_overlap": 0,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    },
    "overall": {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4",
      "overall_score": 0.7083333333333335,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "all-mpnet-base-v2",
        "chunk_size": 512,
        "chunk_overlap": 100,
        "top_k": 4,
        "retriever_type": "similarity"
      }
    }
  },
  "all_metrics": [
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs256-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 256,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.24358974358974358,
      "ragas_context_relevance_mean": 0.24358974358974358,
      "ragas_faithfulness_mean": 0.21071179532717993,
      "ragas_response_relevancy_mean": 0.7876243480309194,
      "character_length_output": 353.3205128205128,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.6571510220617325,
      "character_length_closeness": 1.4989109915617704,
      "word_count_output": 52.32051282051282,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.695294053777469,
      "word_count_closeness": 1.417848335235668,
      "ragas_answer_accuracy_rank": 0.16666666666666663,
      "ragas_context_relevance_rank": 0.08333333333333337,
      "ragas_faithfulness_rank": 0.08333333333333337,
      "ragas_response_relevancy_rank": 0.16666666666666663,
      "character_length_rank": 0.9166666666666666,
      "word_count_rank": 0.9166666666666666,
      "overall_score": 0.38888888888888884
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs256-co50-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 256,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.21474358974358973,
      "ragas_context_relevance_mean": 0.22115384615384615,
      "ragas_faithfulness_mean": 0.21788258038258043,
      "ragas_response_relevancy_mean": 0.8118462251452677,
      "character_length_output": 357.05128205128204,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.6918751015394686,
      "character_length_closeness": 1.4247549140960187,
      "word_count_output": 52.256410256410255,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7139602098504583,
      "word_count_closeness": 1.3812913836888365,
      "ragas_answer_accuracy_rank": 0.0,
      "ragas_context_relevance_rank": 0.0,
      "ragas_faithfulness_rank": 0.16666666666666663,
      "ragas_response_relevancy_rank": 0.75,
      "character_length_rank": 0.8333333333333334,
      "word_count_rank": 0.8333333333333334,
      "overall_score": 0.4305555555555556
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs256-co100-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 256,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.23397435897435898,
      "ragas_context_relevance_mean": 0.25,
      "ragas_faithfulness_mean": 0.19885623635623637,
      "ragas_response_relevancy_mean": 0.8118324210317507,
      "character_length_output": 364.94871794871796,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7354418204996456,
      "character_length_closeness": 1.3414863138879602,
      "word_count_output": 53.34615384615385,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7391864647449928,
      "word_count_closeness": 1.33478118873968,
      "ragas_answer_accuracy_rank": 0.08333333333333337,
      "ragas_context_relevance_rank": 0.16666666666666663,
      "ragas_faithfulness_rank": 0.0,
      "ragas_response_relevancy_rank": 0.6666666666666667,
      "character_length_rank": 0.75,
      "word_count_rank": 0.75,
      "overall_score": 0.40277777777777785
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2564102564102564,
      "ragas_context_relevance_mean": 0.2692307692307692,
      "ragas_faithfulness_mean": 0.22587042587042588,
      "ragas_response_relevancy_mean": 0.8270058585425863,
      "character_length_output": 380.9871794871795,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8189596036197209,
      "character_length_closeness": 1.2063314009915767,
      "word_count_output": 56.166666666666664,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.847261818028363,
      "word_count_closeness": 1.1665047701528617,
      "ragas_answer_accuracy_rank": 0.25,
      "ragas_context_relevance_rank": 0.41666666666666663,
      "ragas_faithfulness_rank": 0.33333333333333337,
      "ragas_response_relevancy_rank": 0.9166666666666666,
      "character_length_rank": 0.08333333333333337,
      "word_count_rank": 0.41666666666666663,
      "overall_score": 0.40277777777777773
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co50-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.27884615384615385,
      "ragas_context_relevance_mean": 0.27564102564102566,
      "ragas_faithfulness_mean": 0.22121489621489623,
      "ragas_response_relevancy_mean": 0.8016615606555926,
      "character_length_output": 367.6794871794872,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.763340456004776,
      "character_length_closeness": 1.2930915384489132,
      "word_count_output": 53.91025641025641,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7893458871141894,
      "word_count_closeness": 1.2510228877391427,
      "ragas_answer_accuracy_rank": 0.5,
      "ragas_context_relevance_rank": 0.6666666666666667,
      "ragas_faithfulness_rank": 0.25,
      "ragas_response_relevancy_rank": 0.5,
      "character_length_rank": 0.5833333333333333,
      "word_count_rank": 0.5833333333333333,
      "overall_score": 0.5138888888888888
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs1024-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 1024,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2980769230769231,
      "ragas_context_relevance_mean": 0.26282051282051283,
      "ragas_faithfulness_mean": 0.2580572205572205,
      "ragas_response_relevancy_mean": 0.8047281755427721,
      "character_length_output": 381.0,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8319031306379168,
      "character_length_closeness": 1.1877851068712524,
      "word_count_output": 56.705128205128204,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8832738268193598,
      "word_count_closeness": 1.1194775554553695,
      "ragas_answer_accuracy_rank": 0.9166666666666666,
      "ragas_context_relevance_rank": 0.33333333333333337,
      "ragas_faithfulness_rank": 0.41666666666666663,
      "ragas_response_relevancy_rank": 0.5833333333333333,
      "character_length_rank": 0.0,
      "word_count_rank": 0.0,
      "overall_score": 0.375
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs1024-co50-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 1024,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.27884615384615385,
      "ragas_context_relevance_mean": 0.27564102564102566,
      "ragas_faithfulness_mean": 0.2671966921966922,
      "ragas_response_relevancy_mean": 0.7943226186021297,
      "character_length_output": 376.7307692307692,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8085010732759867,
      "character_length_closeness": 1.2217454963101977,
      "word_count_output": 55.82051282051282,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8611504592254198,
      "word_count_closeness": 1.1479073326657558,
      "ragas_answer_accuracy_rank": 0.5,
      "ragas_context_relevance_rank": 0.6666666666666667,
      "ragas_faithfulness_rank": 0.5,
      "ragas_response_relevancy_rank": 0.33333333333333337,
      "character_length_rank": 0.33333333333333337,
      "word_count_rank": 0.25,
      "overall_score": 0.4305555555555556
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs1024-co100-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2916666666666667,
      "ragas_context_relevance_mean": 0.26282051282051283,
      "ragas_faithfulness_mean": 0.27752155252155253,
      "ragas_response_relevancy_mean": 0.7544050347601466,
      "character_length_output": 367.44871794871796,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7638051287350396,
      "character_length_closeness": 1.2923150323838346,
      "word_count_output": 54.3974358974359,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8131626764091753,
      "word_count_closeness": 1.2148267027390378,
      "ragas_answer_accuracy_rank": 0.8333333333333334,
      "ragas_context_relevance_rank": 0.33333333333333337,
      "ragas_faithfulness_rank": 0.5833333333333333,
      "ragas_response_relevancy_rank": 0.0,
      "character_length_rank": 0.5,
      "word_count_rank": 0.5,
      "overall_score": 0.4583333333333333
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs2048-co0-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 0,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2724358974358974,
      "ragas_context_relevance_mean": 0.2980769230769231,
      "ragas_faithfulness_mean": 0.3081279831279831,
      "ragas_response_relevancy_mean": 0.8006846435022191,
      "character_length_output": 387.4230769230769,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8165623224341152,
      "character_length_closeness": 1.2098301275760237,
      "word_count_output": 57.256410256410255,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8672132261374235,
      "word_count_closeness": 1.139973691918937,
      "ragas_answer_accuracy_rank": 0.33333333333333337,
      "ragas_context_relevance_rank": 0.9166666666666666,
      "ragas_faithfulness_rank": 0.75,
      "ragas_response_relevancy_rank": 0.41666666666666663,
      "character_length_rank": 0.16666666666666663,
      "word_count_rank": 0.16666666666666663,
      "overall_score": 0.45833333333333326
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs2048-co50-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 50,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28846153846153844,
      "ragas_context_relevance_mean": 0.2980769230769231,
      "ragas_faithfulness_mean": 0.37045177045177047,
      "ragas_response_relevancy_mean": 0.7926424968160379,
      "character_length_output": 381.56410256410254,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.807711318541376,
      "character_length_closeness": 1.2229254717713685,
      "word_count_output": 56.794871794871796,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8672524782950926,
      "word_count_closeness": 1.1399226844516446,
      "ragas_answer_accuracy_rank": 0.75,
      "ragas_context_relevance_rank": 0.9166666666666666,
      "ragas_faithfulness_rank": 0.9166666666666666,
      "ragas_response_relevancy_rank": 0.25,
      "character_length_rank": 0.41666666666666663,
      "word_count_rank": 0.08333333333333337,
      "overall_score": 0.5555555555555555
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs2048-co100-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 2048,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28525641025641024,
      "ragas_context_relevance_mean": 0.28525641025641024,
      "ragas_faithfulness_mean": 0.3234145484145484,
      "ragas_response_relevancy_mean": 0.7748253993016403,
      "character_length_output": 378.7564102564103,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8088931364835013,
      "character_length_closeness": 1.2211605586221048,
      "word_count_output": 55.85897435897436,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8603495149096325,
      "word_count_closeness": 1.1489637012135625,
      "ragas_answer_accuracy_rank": 0.5833333333333333,
      "ragas_context_relevance_rank": 0.75,
      "ragas_faithfulness_rank": 0.8333333333333334,
      "ragas_response_relevancy_rank": 0.08333333333333337,
      "character_length_rank": 0.25,
      "word_count_rank": 0.33333333333333337,
      "overall_score": 0.47222222222222227
    },
    {
      "experiment_id": "ragas-rag-cohere-marketing-emb-all-mpnet-base-v2-cs512-co100-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "all-mpnet-base-v2",
      "chunk_size": 512,
      "chunk_overlap": 100,
      "top_k": 4,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28846153846153844,
      "ragas_context_relevance_mean": 0.27564102564102566,
      "ragas_faithfulness_mean": 0.2791412291412291,
      "ragas_response_relevancy_mean": 0.8147330755336871,
      "character_length_output": 359.53846153846155,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7517416156295813,
      "character_length_closeness": 1.312781105143504,
      "word_count_output": 52.76923076923077,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7747432308480973,
      "word_count_closeness": 1.2743021674991293,
      "ragas_answer_accuracy_rank": 0.75,
      "ragas_context_relevance_rank": 0.6666666666666667,
      "ragas_faithfulness_rank": 0.6666666666666667,
      "ragas_response_relevancy_rank": 0.8333333333333334,
      "character_length_rank": 0.6666666666666667,
      "word_count_rank": 0.6666666666666667,
      "overall_score": 0.7083333333333335
    }
  ]
}