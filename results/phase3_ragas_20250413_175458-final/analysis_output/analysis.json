{
  "top_performers": {
    "ragas_answer_accuracy": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-multi_query-k4",
      "score": 0.3433333333333333,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": NaN,
        "retriever_type": "multi_query"
      }
    },
    "ragas_context_relevance": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k12",
      "score": 0.3333333333333333,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 12.0,
        "retriever_type": "similarity"
      }
    },
    "ragas_faithfulness": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k12",
      "score": 0.42159137159137156,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 12.0,
        "retriever_type": "similarity"
      }
    },
    "ragas_response_relevancy": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-similarity_score_threshold-0.5",
      "score": 0.8234187887035429,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 4.0,
        "retriever_type": "similarity_score_threshold"
      }
    },
    "deepeval_faithfulness": {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k12",
      "score": 0.9855616605616606,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 12.0,
        "retriever_type": "similarity"
      }
    },
    "deepeval_geval": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k12",
      "score": 0.5429126861510242,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 12.0,
        "retriever_type": "similarity"
      }
    },
    "bertscore_evaluator": {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-similarity_score_threshold-0.5",
      "score": 0.8847572803497314,
      "config": {
        "rag_type": "cohere",
        "team_type": "marketing",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 4.0,
        "retriever_type": "similarity_score_threshold"
      }
    },
    "character_length": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k12",
      "original_ratio": 0.6291966483001055,
      "closeness_score": 2.626027306577084,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 12.0,
        "retriever_type": "similarity"
      }
    },
    "word_count": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k12",
      "original_ratio": 0.6285158568864121,
      "closeness_score": 2.621340933959206,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 12.0,
        "retriever_type": "similarity"
      }
    },
    "overall": {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k8",
      "overall_score": 0.7222222222222222,
      "config": {
        "rag_type": "cohere",
        "team_type": "engineering",
        "embedding_model": "multi-qa-mpnet-base-cos-v1",
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "top_k": 8.0,
        "retriever_type": "similarity"
      }
    }
  },
  "all_metrics": [
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k12",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 12.0,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.30448717948717946,
      "ragas_context_relevance_mean": 0.3333333333333333,
      "ragas_faithfulness_mean": 0.42159137159137156,
      "ragas_response_relevancy_mean": 0.7774797914290859,
      "deepeval_faithfulness_mean": 0.966758241758242,
      "deepeval_geval_mean": 0.5429126861510242,
      "bertscore_evaluator_mean": 0.8793980823113368,
      "character_length_output": 371.14102564102564,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6291966483001055,
      "character_length_closeness": 2.626027306577084,
      "word_count_output": 54.1025641025641,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6285158568864121,
      "word_count_closeness": 2.621340933959206,
      "ragas_answer_accuracy_rank": 0.6666666666666667,
      "ragas_context_relevance_rank": 0.9166666666666666,
      "ragas_faithfulness_rank": 0.9166666666666666,
      "ragas_response_relevancy_rank": 0.16666666666666663,
      "deepeval_faithfulness_rank": 0.5833333333333333,
      "deepeval_geval_rank": 0.9166666666666666,
      "bertscore_evaluator_rank": 0.16666666666666663,
      "character_length_rank": 0.9166666666666666,
      "word_count_rank": 0.9166666666666666,
      "overall_score": 0.6851851851851853
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k8",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 8.0,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.34294871794871795,
      "ragas_context_relevance_mean": 0.3301282051282051,
      "ragas_faithfulness_mean": 0.3601648351648351,
      "ragas_response_relevancy_mean": 0.8084110866903598,
      "deepeval_faithfulness_mean": 0.9659951159951158,
      "deepeval_geval_mean": 0.5223857505360914,
      "bertscore_evaluator_mean": 0.8814543974705231,
      "character_length_output": 361.94871794871796,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6155732899696467,
      "character_length_closeness": 2.535325257062445,
      "word_count_output": 52.43589743589744,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6119209399993857,
      "word_count_closeness": 2.5120638096323296,
      "ragas_answer_accuracy_rank": 0.8333333333333334,
      "ragas_context_relevance_rank": 0.8333333333333334,
      "ragas_faithfulness_rank": 0.8333333333333334,
      "ragas_response_relevancy_rank": 0.6666666666666667,
      "deepeval_faithfulness_rank": 0.5,
      "deepeval_geval_rank": 0.75,
      "bertscore_evaluator_rank": 0.5833333333333333,
      "character_length_rank": 0.75,
      "word_count_rank": 0.75,
      "overall_score": 0.7222222222222222
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k8",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 8.0,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2948717948717949,
      "ragas_context_relevance_mean": 0.32371794871794873,
      "ragas_faithfulness_mean": 0.3243945868945869,
      "ragas_response_relevancy_mean": 0.7775641980845763,
      "deepeval_faithfulness_mean": 0.974114774114774,
      "deepeval_geval_mean": 0.477138021234528,
      "bertscore_evaluator_mean": 0.8836650030735211,
      "character_length_output": 389.6923076923077,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8851175514106706,
      "character_length_closeness": 1.1171717037879982,
      "word_count_output": 57.01282051282051,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.9254526036754964,
      "word_count_closeness": 1.0690012471726411,
      "ragas_answer_accuracy_rank": 0.5,
      "ragas_context_relevance_rank": 0.6666666666666667,
      "ragas_faithfulness_rank": 0.6666666666666667,
      "ragas_response_relevancy_rank": 0.25,
      "deepeval_faithfulness_rank": 0.8333333333333334,
      "deepeval_geval_rank": 0.5,
      "bertscore_evaluator_rank": 0.75,
      "character_length_rank": 0.08333333333333337,
      "word_count_rank": 0.08333333333333337,
      "overall_score": 0.48148148148148145
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-k12",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 12.0,
      "retriever_type": "similarity",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.2980769230769231,
      "ragas_context_relevance_mean": 0.3301282051282051,
      "ragas_faithfulness_mean": 0.3242997742997743,
      "ragas_response_relevancy_mean": 0.8037266016860204,
      "deepeval_faithfulness_mean": 0.9855616605616606,
      "deepeval_geval_mean": 0.48627432590738945,
      "bertscore_evaluator_mean": 0.8817464961455419,
      "character_length_output": 403.7692307692308,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.9483590624093712,
      "character_length_closeness": 1.0434502465974924,
      "word_count_output": 59.73076923076923,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.9900351914899324,
      "word_count_closeness": 0.999964809748465,
      "ragas_answer_accuracy_rank": 0.5833333333333333,
      "ragas_context_relevance_rank": 0.8333333333333334,
      "ragas_faithfulness_rank": 0.5833333333333333,
      "ragas_response_relevancy_rank": 0.5833333333333333,
      "deepeval_faithfulness_rank": 0.9166666666666666,
      "deepeval_geval_rank": 0.5833333333333333,
      "bertscore_evaluator_rank": 0.6666666666666667,
      "character_length_rank": 0.0,
      "word_count_rank": 0.0,
      "overall_score": 0.5277777777777778
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-similarity_score_threshold-0.8",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 4.0,
      "retriever_type": "similarity_score_threshold",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28846153846153844,
      "ragas_context_relevance_mean": 0.0,
      "ragas_faithfulness_mean": 0.0,
      "ragas_response_relevancy_mean": 0.8129651952537842,
      "deepeval_faithfulness_mean": 0.0,
      "deepeval_geval_mean": 0.0,
      "bertscore_evaluator_mean": 0.8801231185595194,
      "character_length_output": 357.70512820512823,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6096962079037146,
      "character_length_closeness": 2.4981027403294473,
      "word_count_output": 51.282051282051285,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.5985789896947492,
      "word_count_closeness": 2.4306002244709313,
      "ragas_answer_accuracy_rank": 0.33333333333333337,
      "ragas_context_relevance_rank": 0.08333333333333337,
      "ragas_faithfulness_rank": 0.08333333333333337,
      "ragas_response_relevancy_rank": 0.8333333333333334,
      "deepeval_faithfulness_rank": 0.08333333333333337,
      "deepeval_geval_rank": 0.08333333333333337,
      "bertscore_evaluator_rank": 0.41666666666666663,
      "character_length_rank": 0.5833333333333333,
      "word_count_rank": 0.5,
      "overall_score": 0.3333333333333333
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-similarity_score_threshold-0.5",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 4.0,
      "retriever_type": "similarity_score_threshold",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28205128205128205,
      "ragas_context_relevance_mean": 0.23397435897435898,
      "ragas_faithfulness_mean": 0.24638278388278392,
      "ragas_response_relevancy_mean": 0.8234187887035429,
      "deepeval_faithfulness_mean": 0.4951770451770452,
      "deepeval_geval_mean": 0.31856301182982677,
      "bertscore_evaluator_mean": 0.8796622638518994,
      "character_length_output": 357.4230769230769,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6112234674261137,
      "character_length_closeness": 2.5076701318042516,
      "word_count_output": 51.62820512820513,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6056310572212752,
      "word_count_closeness": 2.472989129996591,
      "ragas_answer_accuracy_rank": 0.16666666666666663,
      "ragas_context_relevance_rank": 0.25,
      "ragas_faithfulness_rank": 0.25,
      "ragas_response_relevancy_rank": 0.9166666666666666,
      "deepeval_faithfulness_rank": 0.16666666666666663,
      "deepeval_geval_rank": 0.25,
      "bertscore_evaluator_rank": 0.33333333333333337,
      "character_length_rank": 0.6666666666666667,
      "word_count_rank": 0.6666666666666667,
      "overall_score": 0.40740740740740744
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-similarity_score_threshold-0.8",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 4.0,
      "retriever_type": "similarity_score_threshold",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.23076923076923078,
      "ragas_context_relevance_mean": 0.0,
      "ragas_faithfulness_mean": 0.0,
      "ragas_response_relevancy_mean": 0.7960196782752958,
      "deepeval_faithfulness_mean": 0.0,
      "deepeval_geval_mean": 0.0,
      "bertscore_evaluator_mean": 0.8811919528704423,
      "character_length_output": 366.44871794871796,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.8031248930053727,
      "character_length_closeness": 1.2298233747388085,
      "word_count_output": 52.97435897435897,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7885757944983873,
      "word_count_closeness": 1.2522292898047755,
      "ragas_answer_accuracy_rank": 0.0,
      "ragas_context_relevance_rank": 0.08333333333333337,
      "ragas_faithfulness_rank": 0.08333333333333337,
      "ragas_response_relevancy_rank": 0.41666666666666663,
      "deepeval_faithfulness_rank": 0.08333333333333337,
      "deepeval_geval_rank": 0.08333333333333337,
      "bertscore_evaluator_rank": 0.5,
      "character_length_rank": 0.25,
      "word_count_rank": 0.33333333333333337,
      "overall_score": 0.20370370370370372
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-similarity_score_threshold-0.5",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": 4.0,
      "retriever_type": "similarity_score_threshold",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.27884615384615385,
      "ragas_context_relevance_mean": 0.23076923076923078,
      "ragas_faithfulness_mean": 0.17178469678469677,
      "ragas_response_relevancy_mean": 0.8028114699090557,
      "deepeval_faithfulness_mean": 0.5241452991452992,
      "deepeval_geval_mean": 0.30790192686910284,
      "bertscore_evaluator_mean": 0.8847572803497314,
      "character_length_output": 359.96153846153845,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7424404745370867,
      "character_length_closeness": 1.329008783871197,
      "word_count_output": 52.30769230769231,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.7513276621004383,
      "word_count_closeness": 1.3134948981639325,
      "ragas_answer_accuracy_rank": 0.08333333333333337,
      "ragas_context_relevance_rank": 0.16666666666666663,
      "ragas_faithfulness_rank": 0.16666666666666663,
      "ragas_response_relevancy_rank": 0.5,
      "deepeval_faithfulness_rank": 0.25,
      "deepeval_geval_rank": 0.16666666666666663,
      "bertscore_evaluator_rank": 0.9166666666666666,
      "character_length_rank": 0.41666666666666663,
      "word_count_rank": 0.41666666666666663,
      "overall_score": 0.3425925925925925
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-mmr-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": NaN,
      "retriever_type": "mmr",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.32051282051282054,
      "ragas_context_relevance_mean": 0.30128205128205127,
      "ragas_faithfulness_mean": 0.3099765974765975,
      "ragas_response_relevancy_mean": 0.8111909334529191,
      "deepeval_faithfulness_mean": 0.9710012210012212,
      "deepeval_geval_mean": 0.535442020217009,
      "bertscore_evaluator_mean": 0.8795020266985282,
      "character_length_output": 370.05128205128204,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6250382524761778,
      "character_length_closeness": 2.5976606933864725,
      "word_count_output": 53.41025641025641,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6189980151435799,
      "word_count_closeness": 2.5575317740834747,
      "ragas_answer_accuracy_rank": 0.75,
      "ragas_context_relevance_rank": 0.5833333333333333,
      "ragas_faithfulness_rank": 0.5,
      "ragas_response_relevancy_rank": 0.75,
      "deepeval_faithfulness_rank": 0.75,
      "deepeval_geval_rank": 0.8333333333333334,
      "bertscore_evaluator_rank": 0.25,
      "character_length_rank": 0.8333333333333334,
      "word_count_rank": 0.8333333333333334,
      "overall_score": 0.6759259259259258
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-mmr-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": NaN,
      "retriever_type": "mmr",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.28525641025641024,
      "ragas_context_relevance_mean": 0.2916666666666667,
      "ragas_faithfulness_mean": 0.29035409035409043,
      "ragas_response_relevancy_mean": 0.7900353318717489,
      "deepeval_faithfulness_mean": 0.9679487179487182,
      "deepeval_geval_mean": 0.48946585498080164,
      "bertscore_evaluator_mean": 0.8838380177815756,
      "character_length_output": 377.5897435897436,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.811093164104609,
      "character_length_closeness": 1.217888595005521,
      "word_count_output": 55.333333333333336,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.8309468817411272,
      "word_count_closeness": 1.1891357488948213,
      "ragas_answer_accuracy_rank": 0.25,
      "ragas_context_relevance_rank": 0.5,
      "ragas_faithfulness_rank": 0.41666666666666663,
      "ragas_response_relevancy_rank": 0.33333333333333337,
      "deepeval_faithfulness_rank": 0.6666666666666667,
      "deepeval_geval_rank": 0.6666666666666667,
      "bertscore_evaluator_rank": 0.8333333333333334,
      "character_length_rank": 0.16666666666666663,
      "word_count_rank": 0.16666666666666663,
      "overall_score": 0.44444444444444453
    },
    {
      "experiment_id": "v1-cohere-engineering-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-multi_query-k4",
      "rag_type": "cohere",
      "team_type": "engineering",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": NaN,
      "retriever_type": "multi_query",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.3433333333333333,
      "ragas_context_relevance_mean": 0.27884615384615385,
      "ragas_faithfulness_mean": 0.3246947496947497,
      "ragas_response_relevancy_mean": 0.7651758972972452,
      "deepeval_faithfulness_mean": 0.9450854700854701,
      "deepeval_geval_mean": 0.4075820107488103,
      "bertscore_evaluator_mean": 0.8451399428722186,
      "character_length_output": 360.53846153846155,
      "character_length_reference": 612.7307692307693,
      "character_length_ratio": 0.6078225704273543,
      "character_length_closeness": 2.4864647453304416,
      "word_count_output": 51.794871794871796,
      "word_count_reference": 89.91025641025641,
      "word_count_ratio": 0.6005022504031771,
      "word_count_closeness": 2.4420158620763233,
      "ragas_answer_accuracy_rank": 0.9166666666666666,
      "ragas_context_relevance_rank": 0.33333333333333337,
      "ragas_faithfulness_rank": 0.75,
      "ragas_response_relevancy_rank": 0.08333333333333337,
      "deepeval_faithfulness_rank": 0.33333333333333337,
      "deepeval_geval_rank": 0.41666666666666663,
      "bertscore_evaluator_rank": 0.0,
      "character_length_rank": 0.5,
      "word_count_rank": 0.5833333333333333,
      "overall_score": 0.43518518518518523
    },
    {
      "experiment_id": "v1-cohere-marketing-emb-multi-qa-mpnet-base-cos-v1-cs1024-co100-multi_query-k4",
      "rag_type": "cohere",
      "team_type": "marketing",
      "embedding_model": "multi-qa-mpnet-base-cos-v1",
      "chunk_size": 1024,
      "chunk_overlap": 100,
      "top_k": NaN,
      "retriever_type": "multi_query",
      "evaluation_type": "ragas",
      "ragas_answer_accuracy_mean": 0.29,
      "ragas_context_relevance_mean": 0.28205128205128205,
      "ragas_faithfulness_mean": 0.27357226107226107,
      "ragas_response_relevancy_mean": 0.7485904120324848,
      "deepeval_faithfulness_mean": 0.9476495726495727,
      "deepeval_geval_mean": 0.37030661094116596,
      "bertscore_evaluator_mean": 0.8476215409926879,
      "character_length_output": 370.06410256410254,
      "character_length_reference": 252.96153846153845,
      "character_length_ratio": 1.7601425175357617,
      "character_length_closeness": 1.2984609695355054,
      "word_count_output": 54.93589743589744,
      "word_count_reference": 36.94871794871795,
      "word_count_ratio": 1.790708764103179,
      "word_count_closeness": 1.2488935363659146,
      "ragas_answer_accuracy_rank": 0.41666666666666663,
      "ragas_context_relevance_rank": 0.41666666666666663,
      "ragas_faithfulness_rank": 0.33333333333333337,
      "ragas_response_relevancy_rank": 0.0,
      "deepeval_faithfulness_rank": 0.41666666666666663,
      "deepeval_geval_rank": 0.33333333333333337,
      "bertscore_evaluator_rank": 0.08333333333333337,
      "character_length_rank": 0.33333333333333337,
      "word_count_rank": 0.25,
      "overall_score": 0.28703703703703703
    }
  ]
}