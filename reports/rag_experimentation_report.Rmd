---
title: "Optimizing Retrieval-Augmented Generation: A Systematic Evaluation of Embedding Models and Retrieval Parameters"
author: "Sean Sica"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    number_sections: true
    fig_caption: true
    keep_tex: true
    citation_package: natbib
    latex_engine: xelatex
  html_document:
    toc: true
    toc_depth: 3
    theme: paper
header-includes:
  - \usepackage{amsmath}
  - \usepackage{booktabs}
  - \usepackage{microtype}
  - \usepackage{hyperref}
bibliography: references.bib
abstract: |
  In this paper, I present a systematic evaluation of Retrieval-Augmented Generation (RAG) systems across various configurations of embedding models, retrieval strategies, and generation parameters. I conducted multi-phased experiments to identify optimal combinations for domain-specific question answering tasks. My results demonstrate that [key finding 1], [key finding 2], and [key finding 3]. I provide a comparative analysis of performance metrics including faithfulness, context relevance, and answer accuracy, offering insights for practitioners implementing RAG systems. My findings contribute to the growing body of knowledge on effectively designing and optimizing RAG pipelines for production applications.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                     fig.width = 7, fig.height = 5, fig.align = "center",
                     out.width = "80%")
library(ggplot2)
library(dplyr)
library(readr)
library(knitr)
library(kableExtra)
```

# Executive Summary

This research systematically evaluated Retrieval-Augmented Generation (RAG) systems across multiple configurations to identify optimal parameters for domain-specific question answering.
Through a phased experimental approach, I found that embedding model selection has less impact than query template design and retrieval parameters.
The multi-qa-mpnet-base-cos-v1 embedding model with chunk size of 1024, chunk overlap of 100, and a similarity-based retriever with top-k=8 consistently delivered the best performance.
Engineering-focused query templates consistently outperformed marketing templates regardless of other parameters.
Larger chunk sizes and higher overlap improved context relevance, while similarity-based retrieval with appropriate top-k values yielded more faithful and accurate responses than threshold-based approaches.
These findings offer practical guidelines for RAG system implementation in production environments.

# Introduction

Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language model (LLM) responses with relevant external knowledge.
By combining the strengths of retrieval systems with generative capabilities, RAG addresses hallucination issues and improves factual accuracy in model outputs.
However, systematically evaluating the impact of various configuration choices on RAG performance is still very1 challenging.
This project implemented a RAG system designed to answer domain-specific questions across engineering and marketing contexts.
The system ingests technical documentation, processes it into retrievable chunks, and leverages state-of-the-art embedding models and retrieval strategies to provide accurate, contextually relevant answers.
One of the main features of this implementation is its ability to customize responses for different audiences through specialized query templates.
The inputs consist of domain-specific documentation split into chunks and stored in a vector database. 
The system can answer factual questions about technical specifications, processes, and concepts contained within the corpus.

# Key Findings

- Template Engineering Matters Most: The engineering vs. marketing query template had a more significant impact on performance than embedding model selection, with engineering templates consistently outperforming marketing ones across all configurations. Notably, the engineering template consistently outperformed the marketing template, which I attribute to the nature of the RAG corpus and evaluation corpus being fundamentally biased towards technical content (i.e., the marketing template has the added challenge of needing to summarize/translate technical material for a non-technical audience, whereas this is not as much a requirement for the engineering audience). 

- Chunking Parameters: A chunk size of 1024 tokens with 100 token overlap provided the best balance between context preservation and retrieval precision, with context relevance showing a strong correlation ($R^2 \approx 0.70$) to these parameters.

- Retriever Strategy: Similarity-based retrieval with top-k=8 consistently outperformed other strategies, while high similarity thresholds (0.8+) often retrieved nothing and collapsed performance, demonstrating that retriever method selection significantly impacts evaluation metrics ($R^2 \approx 0.4-0.6$).

-Embedding Model Performance: While multi-qa-mpnet-base-cos-v1 was technically the best performing embedding model for both engineering and marketing contexts, differences between embedding models were statistically insignificant. This could likely be overcome with a larger evaluation QA corpus, as well as repeated iterations of experiments to increase the draw pool for the random distribution. LangSmith specifically provides an argument for evaluating (and normalizing) metric outcomes against multiple runs, but this drives up the cost linearly.

- Evaluation Metric Sensitivity: Different evaluation metrics showed varying sensitivities to parameter changes, with DeepEval faithfulness being extremely sensitive to retriever method changes, making it an effective signal for optimizing RAG systems.



# Methodology

## Technical Approach
The implementation followed a phased approach to systematically explore different aspects of the RAG pipeline:

### Phase 1: Embedding Model Selection
I tested five embedding models (all-mpnet-base-v2, all-MiniLM-L6-v2, all-distilroberta-v1, multi-qa-mpnet-base-cos-v1, multi-qa-mpnet-base-dot-v1) while keeping other parameters constant.
This phase established the foundation for subsequent experiments by identifying the most effective semantic representation for the corpus.

This embedding-first approach was deliberate as it determines how documents are represented in vector space, fundamentally constraining all downstream retrieval performance.
Since switching embedding models requires re-embedding the entire corpus (making it computationally expensive), identifying the best model early allowed me to focus subsequent phases on parameter optimization. (Though, in reality, this last point is a bit of a principled stance given how small the RAG corpus actually was).

### Phase 2: Chunking Parameter Optimization
Using the best-performing embedding model from Phase 1, I explored variations in chunk size (256, 512, 1024, 2048) and chunk overlap (0, 50, 100) to determine the optimal document segmentation strategy.
These parameters directly influence what information is captured in each vector, affecting both retrieval precision and the context provided to the LLM.

### Phase 3: Retrieval Strategy Refinement
With optimized embeddings and chunking in place, I experimented with advanced retrieval techniques including similarity-based retrieval with various top-k values (4, 8, 12), Maximum Marginal Relevance (MMR), and similarity score thresholding at different levels (0.5, 0.8).
These strategies operate on the already-embedded documents and don't require reprocessing the corpus.

For audience customization (i.e., engineering vs marketing), I implemented two distinct query templates:

- Engineering template: Emphasizes technical language with precise terminology, focused on accuracy and comprehensiveness
- Marketing template: Emphasizes conversational language with simplified terminology, focused on clarity and accessibility

The template selection adjusts how the system frames questions to the retrieval component, leading to different context selection and ultimately tailored responses for different audience needs.


# Testing and Evaluation

I evaluated system performance using 13 distinct metrics across three categories:

- Simple Metrics (Binary LLM-as-judge):

  - Groundedness: Is the RAG response factually consistent with the provided context?
  - Retrieval Relevance: Are the retrieved documents pertinent to the question?
  - Relevance: Is the answer relevant to the question?
  - Correctness: Is the answer in agreement with the reference ground truth?

These metrics were implemented manually, by me, following a tutorial from LangChain's documentation.

- RAGAS Metrics (Normalized LLM-as-judge):

  - Faithfulness: Equivalent to Groundedness (above) but with gradation
  - Context Relevance: Equivalent to Retrieved Relevance (above).
  - Response Relevance: Equivalent to Relevance (above).
  - Answer Accuracy: Equivalent to Correctness (above), but with gradation.

- Additional Metrics:

  - DeepEval G-Eval: Framework using LLM-as-judge with chain-of-thought
  - DeepEval Faithfulness: Measures hallucination in RAG systems
  - BERTScore: Semantic similarity between response and reference
  - Word count and character count: Heuristic length metrics

Testing was conducted on a set of 78 domain-specific questions with reference answers. Each experimental configuration processed all questions, and results were analyzed for statistical significance using ANOVA and regression analysis to determine which differences were meaningful rather than due to chance.

I intentionally used multiple overlapping metrics to cross-validate findings. For example, both simple groundedness and RAGAS faithfulness measure the same concept but with different methodologies, providing complementary perspectives on performance. However, the Simple Metrics can largely be ignored, as they were far less robust in their evaluation strategy than their RAGAS counterparts. For example, Correctness uses an LLM-as-a-judge to produce a binary score as to whether the produced output is in agreement with the reference answer. RAGAS Answer Accuracy does the same thing, but does two independent passes through the judge, requesting each to produce a score from 0 to 2, then normalizes both, resulting in (intuitively) a more accurate score, as the former is binary, whereas the former counts in units of 0.25, resulting in more stable means across the full evaluation set. The key purpose of the simple approach was to see if comparable results could be achieved using fewer judge calls, thus resulting in financial savings. This was not the case, and I abandoned the simple metrics after phase 1.



# Results and Findings

## Proof of Concept Functionality
The RAG system successfully demonstrated the ability to answer domain-specific questions with high factual accuracy. Key performance characteristics include:

- Embedding Model Performance:
  Despite testing various models, statistical analysis revealed minimal significant differences between embedding models (only about 5% of comparisons showed statistical significance). Multi-qa-mpnet-base-cos-v1 performed marginally better, but the difference was within statistical noise, suggesting embedding model selection is less critical than other parameters.

- Chunking Parameter Impact:
  The chunk size and overlap had a substantial impact on performance metrics:
  - Chunk size of 1024 with overlap of 100 consistently produced the best results
  - Ragas Context Relevance showed a strong correlation to chunk parameters ($R^2 \approx 0.70$)
  - Ragas Faithfulness (Hallucination for RAG) showed a moderate correlation ($R%2 \approx 0.29$)
  - Larger chunks with more overlap improved context relevance by keeping related information together

- Retrieval Strategy Effects:
  Retrieval method selection demonstrated the most pronounced impact on performance:
    - Similarity with top-k=8 was the overall best performer
    - MMR provided more diverse but sometimes less precise results
    - Similarity score thresholding at 0.8 usually retrieved nothing, causing performance collapse
    - DeepEval faithfulness was extremely sensitive to retriever changes

- Team Type Influence:
  The engineering template consistently outperformed the marketing template across all configurations, with the largest effect size (d=0.70) observed for relevance metrics. This suggests that how questions are framed significantly impacts retrieval quality.

- System Performance:
  The Cohere implementation used approximately 1330 MB of memory with response times averaging 2-5 seconds. Non-LLM components (vector retrieval) completed in about 0.25 seconds, indicating that most latency came from the LLM generation phase rather than retrieval operations.


# Lessons Learned

Technical Insights:

1. Parameter Interaction: Chunk size and retrieval strategy interact synergistically - larger chunks benefit from higher top-k values to maintain diversity of information.
2. Metric Reliability: DeepEval's G-Eval showed strong correlation with human judgment, making it a reliable indicator of system quality.
3. Statistical Significance: Many observed differences between configurations were within statistical noise, highlighting the importance of rigorous evaluation rather than relying on raw scores.

Non-Technical Insights:

1. Template Design Impact: How questions are formulated dramatically affects retrieval quality, with engineering/technical phrasings yielding better performance.
2. Evaluation Trade-offs: Different metrics emphasized different aspects of performance, revealing the multi-faceted nature of "good" RAG systems.
3. Audience Adaptation: The system could effectively tailor responses to different audiences, though engineering templates consistently retrieved better context regardless of audience.


# Challenges and Limitations

Several limitations emerged during experimentation:

- Metric Inconsistency: Different evaluation metrics sometimes yielded contradictory results, making it difficult to determine a single "best" configuration. For example, DeepEval faithfulness consistently scored systems highly (95th percentile) while RAGAS faithfulness was harsh.
- LLM Evaluation Subjectivity: Using LLMs-as-judges introduced potential biases and inconsistencies in evaluation, as they may favor certain response patterns or writing styles.
- Statistical Power: While conducting experiments with 78 questions per configuration, many observed differences were not statistically significant, suggesting either more test questions or more pronounced parameter adjustments would be needed for definitive conclusions.
- Latent Knowledge Effects: It was difficult to determine whether factual accuracy came from the retrieval component or from the LLM's pre-existing knowledge, presenting challenges for attribution of performance gains.
- Resource Constraints: The full factorial design across all parameters would have required thousands of experimental runs, necessitating a phased approach that may have missed optimal combinations across phase boundaries.
- Price: These evaluations sometimes require 3 or more calls to an LLM judge, racking up potentially hundreds of dollars of API calls. I spent over $300 on calls to gpt4o for this project. Storing them in LangSmith for evaluation cost money. Renting a VM with access to a T4 cost money. Making calls to Cohere cost money. Overall, these evaluations are very expensive to run if you lack the requisite hardware to self-host the LLM judges used to produce the evaluation scores.
- Underpowered hardware: Running Mistral on a T4 GPU yielded exceptionally poor performance results. A typical pipeline call with self-hosted Mistral took about 24 seconds, while an equivalent Cohere call took only 2-3 seconds. This is in spite of the inherit disadvantage Cohere has with its implicit network barrier. I attribute this to the T4 GPU being insufficiently capable from a computing perspective to host Mistral and conduct such exhaustive evaluations.

With additional time and resources, several promising directions could be explored:

- Hybrid Retrieval Strategies: Combining MMR for diversity with similarity-based filtering could potentially yield better results than either approach alone.
- Parameter Fine-Tuning: More granular exploration of chunk sizes between 512-2048 and overlap percentages could identify more precise optimal values.
- Prompt Engineering Optimization: Systematic evaluation of different prompt structures for both retrieval and generation could further enhance performance.
- Cross-Domain Testing: Evaluating the best configurations on different document types and domains would test the generalizability of findings.
- LLM Comparison: Comparing performance across different foundation models (e.g., Mistral, Claude, GPT-4) would help isolate the effects of the retrieval component versus the LLM capabilities.


# Summary & Recommendations

Based on the experimental results, I recommend implementing a production RAG system with the following configuration:

  - Embedding Model: multi-qa-mpnet-base-cos-v1
  - Chunk Size: 1024 tokens
  - Chunk Overlap: 100 tokens
  - Retrieval Strategy: Similarity-based with top-k=8
  - Query Template: Engineering-focused template, even for non-technical users

This configuration consistently delivered the best overall performance across metrics and demonstrated robust behavior across different question types. The engineering template's superior performance suggests that technical precision in question formulation benefits retrieval quality, even when the final response will be adapted for non-technical audiences.

The system's modest resource requirements (1330 MB memory, 2-5 second response time) make it viable for production deployment with minimal infrastructure. 
The fact that vector retrieval comprises only a small portion of the overall latency (0.25 seconds) suggests that optimization efforts should focus on LLM inference rather than retrieval operations.

For future development, establishing a continuous evaluation pipeline would enable ongoing refinement as document collections grow and query patterns evolve. 
The significant impact of template design also suggests that A/B testing different query formulations could yield substantial performance improvements with minimal implementation costs.