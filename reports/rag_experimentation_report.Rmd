---
title: "Optimizing Retrieval-Augmented Generation: A Systematic Evaluation of Embedding Models and Retrieval Parameters"
author: "Sean Sica"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    number_sections: true
    fig_caption: true
    keep_tex: true
    citation_package: natbib
    latex_engine: xelatex
  html_document:
    toc: true
    toc_depth: 3
    theme: paper
header-includes:
  - \usepackage{amsmath}
  - \usepackage{booktabs}
  - \usepackage{microtype}
  - \usepackage{hyperref}
bibliography: references.bib
abstract: |
  In this paper, I present a systematic evaluation of Retrieval-Augmented Generation (RAG) systems across various configurations of embedding models, retrieval strategies, and generation parameters. I conducted multi-phased experiments to identify optimal combinations for domain-specific question answering tasks. My results demonstrate that [key finding 1], [key finding 2], and [key finding 3]. I provide a comparative analysis of performance metrics including faithfulness, context relevance, and answer accuracy, offering insights for practitioners implementing RAG systems. My findings contribute to the growing body of knowledge on effectively designing and optimizing RAG pipelines for production applications.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                     fig.width = 7, fig.height = 5, fig.align = "center",
                     out.width = "80%")
library(ggplot2)
library(dplyr)
library(readr)
library(knitr)
library(kableExtra)
```

# Executive Summary

This research systematically evaluated Retrieval-Augmented Generation (RAG) systems across multiple configurations to identify optimal parameters for domain-specific question answering.
Through a phased experimental approach, I found that embedding model selection has less impact than query template design and retrieval parameters.
The multi-qa-mpnet-base-cos-v1 embedding model with chunk size of 1024, chunk overlap of 100, and a similarity-based retriever with top-k=8 consistently delivered the best performance.
Engineering-focused query templates consistently outperformed marketing templates regardless of other parameters.
Larger chunk sizes and higher overlap improved context relevance, while similarity-based retrieval with appropriate top-k values yielded more faithful and accurate responses than threshold-based approaches.
These findings offer practical guidelines for RAG system implementation in production environments.

# Introduction

Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language model (LLM) responses with relevant external knowledge.
By combining the strengths of retrieval systems with generative capabilities, RAG addresses hallucination issues and improves factual accuracy in model outputs.
However, systematically evaluating the impact of various configuration choices on RAG performance is still very challenging.
This project implemented a RAG system designed to answer domain-specific questions across engineering and marketing contexts.
The system ingests technical documentation, processes it into retrievable chunks, and leverages state-of-the-art embedding models and retrieval strategies to provide accurate, contextually relevant answers.
One of the main features of this implementation is its ability to customize responses for different audiences through specialized query templates.
The inputs consist of domain-specific documentation split into chunks and stored in a vector database. 
The system can answer factual questions about technical specifications, processes, and concepts contained within the corpus.

# Key Findings

- Template Engineering Matters Most: The engineering vs. marketing query template had a more significant impact on performance than embedding model selection, with engineering templates consistently outperforming marketing ones across all configurations. Notably, the engineering template consistently outperformed the marketing template, which I attribute to the nature of the RAG corpus and evaluation corpus being fundamentally biased towards technical content (i.e., the marketing template has the added challenge of needing to summarize/translate technical material for a non-technical audience, whereas this is not as much a requirement for the engineering audience). 

- Chunking Parameters: A chunk size of 1024 tokens with 100 token overlap provided the best balance between context preservation and retrieval precision, with context relevance showing a strong correlation ($R^2 \approx 0.70$) to these parameters.

- Retriever Strategy: Similarity-based retrieval with top-k=8 consistently outperformed other strategies, while high similarity thresholds (0.8+) often retrieved nothing and collapsed performance, demonstrating that retriever method selection significantly impacts evaluation metrics ($R^2 \approx 0.4-0.6$).

- Embedding Model Performance: While multi-qa-mpnet-base-cos-v1 was technically the best performing embedding model for both engineering and marketing contexts, differences between embedding models were statistically insignificant. This could likely be overcome with a larger evaluation QA corpus, as well as repeated iterations of experiments to increase the draw pool for the random distribution. LangSmith specifically provides an argument for evaluating (and normalizing) metric outcomes against multiple runs, but this drives up the cost linearly.

- Evaluation Metric Sensitivity: Different evaluation metrics showed varying sensitivities to parameter changes, with DeepEval faithfulness being extremely sensitive to retriever method changes, making it an effective signal for optimizing RAG systems.



# Methodology

## Technical Approach
The implementation followed a phased approach to systematically explore different aspects of the RAG pipeline:

### Phase 1: Embedding Model Selection
I tested five embedding models (all-mpnet-base-v2, all-MiniLM-L6-v2, all-distilroberta-v1, multi-qa-mpnet-base-cos-v1, multi-qa-mpnet-base-dot-v1) while keeping other parameters constant. This phase established the foundation for subsequent experiments by identifying the most effective semantic representation for the corpus.

This embedding-first approach was deliberate as it determines how documents are represented in vector space, fundamentally constraining all downstream retrieval performance. Since switching embedding models requires re-embedding the entire corpus (making it computationally expensive), identifying the best model early allowed me to focus subsequent phases on parameter optimization. (Though, in reality, this last point is a bit of a principled stance given how small the RAG corpus actually was).

### Phase 2: Chunking Parameter Optimization
Using the best-performing embedding model from Phase 1, I explored variations in chunk size (256, 512, 1024, 2048) and chunk overlap (0, 50, 100) to determine the optimal document segmentation strategy. These parameters directly influence what information is captured in each vector, affecting both retrieval precision and the context provided to the LLM.

### Phase 3: Retrieval Strategy Refinement
With optimized embeddings and chunking in place, I experimented with advanced retrieval techniques including similarity-based retrieval with various top-k values (4, 8, 12), Maximum Marginal Relevance (MMR), and similarity score thresholding at different levels (0.5, 0.8).
These strategies operate on the already-embedded documents and don't require reprocessing the corpus.

For audience customization (i.e., engineering vs marketing), I implemented two a query template for each team:

- Engineering template: Emphasizes technical language with precise terminology, focused on accuracy and comprehensiveness
- Marketing template: Emphasizes conversational language with simplified terminology, focused on clarity and accessibility

The template selection adjusts how the system frames questions to the retrieval component, leading to different context selection and ultimately tailored responses for different audience needs.

In addition, I used LangSmith to log all results. Here was the general process:

Run the `run_phased_evaluations.py` script to orchestrate to evaluate all 78 questions of the validation set against any of the specified evaluation metrics.

The phased evaluations script logs all runs and traces to LangSmith to make it easy to browse the results. Each question has a trace which breaks down each sub-step of the pipeline, including how long it took to retrieve documents from the vector store, which documents were retrieved, and what the final templated input was before running it through the RAG system's LLM. 

The phased evaluation script also logs all runs to file, recording them in JSON files for local analysis.

Next, I would analyze the phased evaluation results using another script, `analyze_top_performers.py`, which would identify the top performing RAG system configuration based on an unweighted, normalized aggregate of all relevant evaluation metrics. In the future, I would like to implement a weighting mechanism to give more preference to evaluation metrics that the researcher values more.

Lastly, I would run a statistical analysis on the results using another script, `analyze_statistical_significance.py`, to ensure that the findings were significantly signifiant, and to better understand the effect size; additionally, I ran regressions to identify whether any of the independent variables were collinear with any of the evaluation metrics.

## Testing and Evaluation

I evaluated system performance using 13 distinct metrics across three categories:

- Simple Metrics (Binary LLM-as-judge):

  - Groundedness: Is the RAG response factually consistent with the provided context?
  - Retrieval Relevance: Are the retrieved documents pertinent to the question?
  - Relevance: Is the answer relevant to the question?
  - Correctness: Is the answer in agreement with the reference ground truth?

These metrics were implemented manually, by me, following a tutorial from LangChain's documentation.

- RAGAS Metrics (Normalized LLM-as-judge):

  - Faithfulness: Equivalent to Groundedness (above).
  - Context Relevance: Equivalent to Retrieved Relevance (above).
  - Response Relevance: Equivalent to Relevance (above).
  - Answer Accuracy: Equivalent to Correctness (above).

- Additional Metrics:

  - DeepEval G-Eval: Framework using LLM-as-judge with chain-of-thought. The authors of DeepEval claim that G-Eval most versatile metric they have to offer, claiming that it is capable of evaluating almost any use case with human-like accuracy.
  - DeepEval Faithfulness: Measures hallucination in RAG systems. Although similar to the Hallucination metric, the DeepEval Faithfulness metric is more concerned with contradictions between the actual output and retrieval context in RAG pipelines, rather than hallucination in the actual LLM itself.
  - BERTScore: Semantic similarity between response and reference
  - Word count and character count: Heuristic length metrics comparing hte actual output to the reference output

Testing was conducted on a set of 78 domain-specific questions with reference answers. Each experimental configuration processed all questions, with results analyzed for statistical significance using ANOVA and regression analysis to determine which differences were meaningful rather than due to chance. For all LLM-as-judge evaluations (10 of the 13 metrics), I used GPT-4o as the judge model.

I intentionally used overlapping metrics to cross-validate findings. The Simple Metrics and RAGAS Metrics measure similar concepts but differ in methodology. The Simple Metrics use a single inference request per question-answer pair to produce binary scores, while RAGAS metrics typically involve at least two inference calls. For example, while Simple Correctness produces a binary score about agreement with the reference answer, RAGAS Answer Accuracy performs two independent judge passes with scores from 0 to 2, then normalizes them to produce increments of 0.25. This results in more stable means across the full evaluation set.

The primary purpose of implementing Simple Metrics was to determine if comparable results could be achieved with fewer judge calls, potentially reducing costs. After Phase 1, I abandoned this approach as the RAGAS metrics proved more robust in their evaluation strategy, despite their higher computational expense.

# Results and Findings

## Proof of Concept Functionality
Overall, the RAG system successfully demonstrated the ability to answer domain-specific questions with high factual accuracy.

Regarding embedding model performance, despite testing various models, statistical analysis revealed minimal significant differences between them, with only about 5% of comparisons showing statistical significance. The multi-qa-mpnet-base-cos-v1 model performed marginally better, but the difference was within statistical noise, suggesting that embedding model selection is less critical than other parameters in the overall RAG pipeline performance. 

The chunk size and overlap parameters had a substantial impact on performance metrics. A chunk size of 1024 tokens with 100 token overlap consistently produced the best results across evaluations. Ragas Context Relevance showed a strong correlation ($R^2 \approx 0.70$) to these parameters, while Ragas Faithfulness (which measures hallucination in RAG systems) showed a moderate correlation ($R^2 \approx 0.29$). The data suggested that larger chunks with more overlap improved context relevance by keeping related information together, which contributed to better overall system performance.

Retrieval method selection demonstrated the most pronounced impact on performance among all tested parameters. Similarity-based retrieval with top-k=8 consistently outperformed other strategies in most metrics. Maximum Marginal Relevance (MMR) provided more diverse but sometimes less precise results, which could be beneficial depending on the specific use case. Notably, similarity score thresholding at 0.8 often retrieved nothing, causing performance collapse in those configurations. The evaluation also revealed that DeepEval faithfulness was extremely sensitive to retriever method changes, making it a particularly useful signal for optimizing RAG retrieval strategies.

The query template type showed significant influence on system performance. The engineering template consistently outperformed the marketing template across all configurations, with the largest effect size (d=0.70) observed for relevance metrics. This suggests that how questions are framed significantly impacts retrieval quality, with technically precise formulations yielding better results even when the final response might need to be adapted for non-technical audiences.

In terms of system performance metrics, the Cohere implementation used approximately 1330 MB of memory with response times averaging 2-5 seconds. The non-LLM components (specifically vector retrieval) completed in about 0.25 seconds, indicating that most latency came from the LLM generation phase rather than retrieval operations. This insight suggests that optimization efforts for production systems should focus primarily on generation efficiency rather than retrieval speed.

# Lessons Learned

Technical Insights:

1. Parameter Interaction: Chunk size and retrieval strategy interact synergistically - larger chunks benefit from higher top-k values to maintain diversity of information.
2. Metric Reliability: DeepEval's G-Eval showed strong correlation with human judgment, making it a reliable indicator of system quality.
3. Statistical Significance: Many observed differences between configurations were within statistical noise, highlighting the importance of rigorous evaluation rather than relying on raw scores.

Non-Technical Insights:

1. Template Design Impact: How questions are formulated dramatically affects retrieval quality, with engineering/technical phrasings yielding better performance.
2. Evaluation Trade-offs: Different metrics emphasized different aspects of performance, revealing the multi-faceted nature of "good" RAG systems.
3. Audience Adaptation: The system could effectively tailor responses to different audiences, though engineering templates consistently retrieved better context regardless of audience.


# Challenges and Limitations

Several limitations emerged during experimentation:

- Metric Inconsistency: Different evaluation metrics sometimes yielded contradictory results, making it difficult to determine a single "best" configuration. For example, DeepEval faithfulness consistently scored systems highly (95th percentile) while RAGAS faithfulness was harsh.
- LLM Evaluation Subjectivity: Using LLMs-as-judges introduced potential biases and inconsistencies in evaluation, as they may favor certain response patterns or writing styles.
- Statistical Power: While conducting experiments with 78 questions per configuration, many observed differences were not statistically significant, suggesting either more test questions or more pronounced parameter adjustments would be needed for definitive conclusions.
- Latent Knowledge Effects: It was difficult to determine whether factual accuracy came from the retrieval component or from the LLM's pre-existing knowledge, presenting challenges for attribution of performance gains.
- Resource Constraints: The full factorial design across all parameters would have required thousands of experimental runs, necessitating a phased approach that may have missed optimal combinations across phase boundaries.
- Price: These evaluations sometimes require 3 or more calls to an LLM judge, racking up potentially hundreds of dollars of API calls. I spent over $300 on calls to gpt4o for this project. Storing them in LangSmith for evaluation cost money. Renting a VM with access to a T4 cost money. Making calls to Cohere cost money. Overall, these evaluations are very expensive to run if you lack the requisite hardware to self-host the LLM judges used to produce the evaluation scores.
- Underpowered hardware: Running Mistral on a T4 GPU yielded exceptionally poor performance results. A typical pipeline call with self-hosted Mistral took about 24 seconds, while an equivalent Cohere call took only 2-3 seconds. This is in spite of the inherit disadvantage Cohere has with its implicit network barrier. I attribute this to the T4 GPU being insufficiently capable from a computing perspective to host Mistral and conduct such exhaustive evaluations.

With additional time and resources, several promising directions could be explored:

- Hybrid Retrieval Strategies: Combining MMR for diversity with similarity-based filtering could potentially yield better results than either approach alone.
- Parameter Fine-Tuning: More granular exploration of chunk sizes between 512-2048 and overlap percentages could identify more precise optimal values.
- Prompt Engineering Optimization: Systematic evaluation of different prompt structures for both retrieval and generation could further enhance performance.
- Cross-Domain Testing: Evaluating the best configurations on different document types and domains would test the generalizability of findings.
- LLM Comparison: Comparing performance across different foundation models (e.g., Mistral, Claude, GPT-4) would help isolate the effects of the retrieval component versus the LLM capabilities.


# Summary & Recommendations

Based on the experimental results, I recommend implementing a production RAG system with the following configuration:

  - Embedding Model: multi-qa-mpnet-base-cos-v1
  - Chunk Size: 1024 tokens
  - Chunk Overlap: 100 tokens
  - Retrieval Strategy: Similarity-based with top-k=8
  - Query Template: Engineering-focused template, even for non-technical users

This configuration consistently delivered the best overall performance across metrics and demonstrated robust behavior across different question types. The engineering template's superior performance suggests that technical precision in question formulation benefits retrieval quality, even when the final response will be adapted for non-technical audiences.

The system's modest resource requirements (1330 MB memory, 2-5 second response time) make it viable for production deployment with minimal infrastructure. 
The fact that vector retrieval comprises only a small portion of the overall latency (0.25 seconds) suggests that optimization efforts should focus on LLM inference rather than retrieval operations.

For future development, establishing a continuous evaluation pipeline would enable ongoing refinement as document collections grow and query patterns evolve. 
The significant impact of template design also suggests that A/B testing different query formulations could yield substantial performance improvements with minimal implementation costs.