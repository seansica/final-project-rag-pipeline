---
title: "W267 Final Project: Optimizing RAG Through Systematic Evaluations"
author: "Sean Sica"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: false
    number_sections: true
    fig_caption: true
    citation_package: biblatex
    latex_engine: xelatex
bibliography: references.bib
biblio-style: apa
---

# Executive Summary

I systematically evaluated RAG systems across multiple configurations to identify optimal parameters for domain-specific question answering. Through a phased experimental approach, I found that embedding model selection has less impact than query template design and retrieval parameters. The multi-qa-mpnet-base-cos-v1 embedding model with chunk size of 1024, chunk overlap of 100, and a similarity-based retriever with top-k=8 consistently delivered the best performance. Engineering-focused query templates consistently outperformed marketing templates regardless of other parameters. Larger chunk sizes and higher overlap improved context relevance, while similarity-based retrieval with appropriate top-k values yielded more faithful and accurate responses than threshold-based approaches.

All code and experimental results can be found on GitHub [here](https://github.com/seansica/final-project-rag-pipeline/tree/main).

# Introduction

Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language model (LLM) responses with relevant external knowledge. By combining the strengths of retrieval systems with generative capabilities, RAG addresses hallucination issues and improves factual accuracy in model outputs. However, systematically evaluating the impact of various configuration choices on RAG performance is still very challenging. This project implemented a RAG system designed to answer domain-specific questions across engineering and marketing contexts. The system ingests technical documentation, processes it into retrievable chunks, and leverages state-of-the-art embedding models and retrieval strategies to provide accurate, contextually relevant answers.

One of the main features of this implementation is its ability to customize responses for different audiences through specialized query templates. The inputs consist of domain-specific documentation split into chunks and stored in a vector database. The system can answer factual questions about technical specifications, processes, and concepts contained within the corpus.

# Key Findings

- Template Engineering Matters Most: The engineering vs. marketing query template had a more significant impact on performance than embedding model selection, with engineering templates consistently outperforming marketing ones across all configurations. Notably, the engineering template consistently outperformed the marketing template, which I attribute to the nature of the RAG corpus and evaluation corpus being fundamentally biased towards technical content (i.e., the marketing template has the added challenge of needing to summarize/translate technical material for a non-technical audience, whereas this is not as much a requirement for the engineering audience). 

- Chunking Parameters: A chunk size of 1024 tokens with 100 token overlap provided the best balance between context preservation and retrieval precision, with context relevance showing a strong correlation ($R^2 \approx 0.70$) to these parameters.

- Retriever Strategy: Similarity-based retrieval with top-k=8 consistently outperformed other strategies, while high similarity thresholds (0.8+) often retrieved nothing and collapsed performance, demonstrating that retriever method selection significantly impacts evaluation metrics ($R^2 \approx 0.4-0.6$).

- Embedding Model Performance: While multi-qa-mpnet-base-cos-v1 was technically the best performing embedding model for both engineering and marketing contexts, differences between embedding models were statistically insignificant. This could likely be overcome with a larger evaluation QA corpus, as well as repeated iterations of experiments to increase the draw pool for the random distribution. LangSmith specifically provides an argument for evaluating (and normalizing) metric outcomes against multiple runs, but this drives up the cost linearly.

- Evaluation Metric Sensitivity: Different evaluation metrics showed varying sensitivities to parameter changes, with DeepEval faithfulness being extremely sensitive to retriever method changes, making it an effective signal for optimizing RAG systems.


# Methodology

## Technical Approach

I followed a three-phase approach to optimize the RAG system:

1. **Embedding Model Selection:**  Five embedding models (all-mpnet-base-v2, all-MiniLM-L6-v2, all-distilroberta-v1, multi-qa-mpnet-base-cos-v1, and multi-qa-mpnet-base-dot-v1) were evaluated under constant conditions. This phase established the best semantic representation for the corpus while minimizing the need for costly re-embedding.

2. **Chunking Parameter Optimization:**  After selecting the best embedding model, I experimented with different chunk sizes (256, 512, 1024, 2048 tokens) and overlaps (0, 50, 100 tokens). This phase identified a 1024-token chunk with 100-token overlap as optimal for balancing context preservation and retrieval precision.

3. **Retrieval and Generation Refinement:**  Advanced retrieval strategies, including similarity-based retrieval with various top-k values (4, 8, 12), Maximum Marginal Relevance (MMR) for enhanced diversity, and similarity score thresholding, were tested. Simultaneously, distinct query templates were tailored for engineering versus marketing audiences. The engineering-focused template consistently delivered superior results.  
   In evaluating the RAG generator, comparisons between Cohere[@cohere2024basicrag] and Mistral models revealed that Cohere not only produced higher-quality outputs but also achieved significantly faster response times, making it the preferred choice despite its network call overhead.

All experiments were executed on an Amazon EC2 instance (g4dn.xlarge) featuring one NVIDIA T4 GPU, 4 vCPUs, and 16 GB of memory.

## Evaluation Process

I designed the evaluation process to ensure reproducibility and enable in-depth analysis. Initially, the `run_phased_evaluations.py` script processes the evaluation questions and logs all details, including details about each sub-step of the pipeline, to LangSmith. To enable offline analysis, the results are also saved locally.

After data logging, the evaluation metrics for each configuration are aggregated into an unweighted, normalized composite score using the `analyze_top_performers.py` script. This approach facilitates a clear ranking of the various RAG system configurations to help identify the best performing configuration. Future iterations of the study may explore incorporating weighting mechanisms to prioritize the most critical metrics.

The final step involves confirming statistical significance using the `analyze_statistical_significance.py` script, which conducted both ANOVA and regression analyses to uncover correlations between configuration choices and performance outcomes. A comparative analysis between the Cohere and Mistral models further demonstrated that, despite potential network latency, Cohere delivered notably higher quality outputs and faster responses—Mistral often required upwards of 25 seconds per request due to challenges like 4-bit quantization. Consequently, Cohere was selected as the sole generator for the RAG pipeline.

## Testing and Evaluation

System performance was measured using 13 distinct metrics that fall into three main groups. The first group comprises Simple Metrics, which use a binary LLM-as-judge approach. These include measures of Groundedness (whether the RAG response is factually consistent with the provided context), Retrieval Relevance (if the retrieved documents are pertinent to the question), Relevance (whether the answer is aligned with the question), and Correctness (if the answer agrees with the reference ground truth). These metrics were implemented manually, following guidance from a tutorial from LangChain's documentation[@langchain2024evaluaterag].

The second group consists of the RAGAS[@es2023ragasautomatedevaluationretrieval] Metrics[@ragas2024availablemetrics], which adopt a normalized LLM-as-judge strategy. This set includes [Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/), which corresponds to Groundedness; [Context Relevance](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/nvidia_metrics/#context-relevance), equivalent to Retrieval Relevance; [Response Relevance](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/answer_relevance/), which mirrors Relevance; and [Answer Accuracy](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/nvidia_metrics/), analogous to Correctness. Unlike the binary scoring of the Simple Metrics, RAGAS metrics generally require at least two inference calls per evaluation. For instance, where Simple Correctness offers a binary score, RAGAS Answer Accuracy involves independent judgments that are normalized into 0.25-increment scores, resulting in more stable mean values across the evaluation set.

Additional metrics further complemented these evaluations. This group includes the [DeepEval G-Eval](https://www.deepeval.com/docs/metrics-llm-evals) framework, which leverages chain-of-thought reasoning to assess almost any use case with human-like accuracy, and [DeepEval Faithfulness](https://www.deepeval.com/docs/metrics-faithfulness), which focuses on measuring contradictions between the generated output and the retrieval context rather than hallucinations within the LLM itself (DeepEval suggests that this metric measures hallucination in RAG systems). Moreover, [BERTScore](https://github.com/Tiiiger/bert_score) was used to determine semantic similarity between the response and the reference, while word count and character count served as heuristic measures to compare the lengths of the outputs with the reference answers.

Testing was carried out on 78 domain-specific questions with established reference answers. Every experimental configuration processed all these questions, with statistical significance determined through ANOVA tests (reporting F-values and p-values), pairwise t-tests, and regression analyses to uncover correlations between configuration choices and performance outcomes. For the LLM-as-judge evaluations (which constituted 10 of the 13 metrics) I used GPT-4o as the judge model.

Metric selection evolved throughout the research process. I initially developed the Simple Metrics based on the fundamental relationships among the input question, retrieved contexts, output answer, and reference answer. However, subsequent research revealed that employing the normalized RAGAS metrics offered a more robust evaluation framework. To further validate these methods, I incorporated the DeepEval framework, particularly its G-Eval metric, which consistently aligned with my human judgment.

# Results and Findings

## Proof of Concept Functionality

The RAG system successfully demonstrated the ability to answer domain-specific questions with high factual accuracy. Although several embedding models were evaluated, statistical analysis revealed that differences among them were minimal—only about 5% of pairwise comparisons showed statistical significance. The multi-qa-mpnet-base-cos-v1 model performed marginally better; however, the variation in evaluation scores was so slight that the choice of embedding model appears less critical than other configuration parameters.

In contrast, chunking parameters had a substantial impact on performance metrics. Experiments consistently showed that a chunk size of 1024 tokens combined with a 100-token overlap yielded the best results. This combination significantly improved context relevance, as indicated by statistically significant p-values in metrics such as ragas_faithfulness, DeepEval Faithfulness, G-Eval, and BERTScore[@zhang2020bertscoreevaluatingtextgeneration]. Larger chunks with moderate overlap effectively preserved related information, thereby enhancing overall system performance.

Retrieval method selection emerged as the most influential parameter. Similarity-based retrieval with a top-k value of 8 consistently outperformed alternative strategies. In comparison, methods like Maximum Marginal Relevance produced a more diverse yet sometimes less precise set of results, while similarity score thresholding at 0.8 often retrieved no data at all, leading to a collapse in performance.

Additionally, the configuration of the generator template had a significant impact, with the engineering-oriented template outperforming the marketing version across all experiments. A sizable effect (reflected in an effect size of d=0.70 for relevance metrics) suggests that technical precision in question formulation directly improves retrieval quality, even if the final output is later adjusted for a non-technical audience.

Finally, performance metrics for system resource usage further validated the setup. The Cohere implementation consumed approximately 1.3 GB of memory and produced responses within 2–5 seconds, while vector retrieval operations took roughly 0.25 seconds. Thus, the overall latency is primarily attributable to the language model generation phase.


# Lessons Learned

Technical Insights:

1. Parameter Interaction: Chunk size and retrieval strategy are collinear: larger chunks benefit from higher top-k values to maintain diversity of information. Interestingly, a chunk overlap of 100 was consistently observed to be the "sweet spot" across all tested chunk sizes, yielding the highest results within its vertical of same (chunk) size experiments.
2. Metric Reliability: DeepEval's G-Eval showed strong correlation with my own human judgment, making it a reliable indicator of system quality. It also conveniently provides a supplemental natural language explanation for the produced score, and I found that those reasons were always grounded and well reasoned.
3. Statistical Significance: Most observed differences between configurations were within statistical noise. This was to be expected given the relatively small size of the validation set (78 questions). This could possibly have been overcome by running each evaluation more than once, but the existing experimental setup was already cost prohibitive for most. Overall, this finding just highlights the importance of doing statistical evaluations rather than relying on raw scores, because typical RAG evaluations run the risk of misleading interpreters into making false causal relationships between the evalation metric and some variable of interest.
4. Overall, the majority of the RAG optimization I experimented with were not worth the squeeze. Despite some independent variables showing colinearity with the evalation metrics and having statistical significance, most of the potential improvement margins were quite narrow. We might infer that the system is approaching it's theoretical maximum output, qualitatively speaking, given the project constraints. This is certainly possible given the overall coherence and accuracy of the results from a human evaluation perspective. But we can't know for sure without expanding the scope of the project.

Non-Technical Insights:

1. Template Design Impact: How questions are formulated dramatically affects retrieval quality, with engineering/technical phrasings yielding better performance.
2. Evaluation Trade-offs: Different metrics emphasized different aspects of performance, revealing the multi-faceted nature of "good" RAG systems.
3. Audience Adaptation: The system could effectively tailor responses to different audiences, though engineering templates consistently retrieved better context regardless of audience.


# Challenges and Limitations

The experimental process revealed several key challenges. Different evaluation frameworks sometimes produced conflicting results. DeepEval’s faithfulness metric tended to be more lenient than RAGAS assessments. In addition, using LLMs as evaluators introduces subjective biases that can affect score objectivity. Notably, many performance differences did not reach statistical significance, suggesting that either the sample size was too small or the parameter adjustments were too subtle to yield definitive outcomes.

Resource constraints further limited the study. A phased approach was necessary, potentially missing some optimal parameter combinations, and the financial burden from API calls and hardware limitations (such as using an NVIDIA T4 GPU) constrained experimentation. For instance, self-hosted models like Mistral required around 24 seconds per request compared to Cohere’s 2–3 seconds. Future work should explore hybrid retrieval strategies, finer parameter tuning, improved prompt engineering, and broader cross-domain testing with additional models.

# Summary & Recommendations

Based on the experimental results, I recommend implementing a production RAG system with the following configuration:

  - Embedding Model: multi-qa-mpnet-base-cos-v1
  - Chunk Size: 1024 tokens
  - Chunk Overlap: 100 tokens
  - Retrieval Strategy: Similarity-based with top-k=8
  - Query Template: Engineering-focused template, even for non-technical users

This configuration consistently delivered the best overall performance across metrics and demonstrated robust behavior across different question types. The engineering template's superior performance suggests that technical precision in question formulation benefits retrieval quality, even when the final response will be adapted for non-technical audiences.

The system's modest resource requirements (1.3 GB MB memory, 2-5 second response time) make it viable for research environments with minimal infrastructure. The fact that vector retrieval comprises only a small portion of the overall latency (0.25 seconds) suggests that optimization efforts should focus on LLM inference rather than retrieval operations.

For future development, establishing a continuous evaluation pipeline would enable ongoing refinement as document collections grow and query patterns evolve. The significant impact of template design also suggests that A/B testing different query formulations could yield substantial performance improvements with minimal implementation costs.