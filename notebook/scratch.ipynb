{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B-MGa4BKbEin"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip -q install git+https://github.com/huggingface/transformers\n",
        "# !pip install -q datasets loralib sentencepiece\n",
        "# !pip -q install bitsandbytes accelerate\n",
        "# !pip -q install langchain\n",
        "# !pip install einops\n",
        "# !pip install faiss-gpu\n",
        "# !pip install langchain_community\n",
        "# !pip install --upgrade --quiet chromadb bs4 qdrant-client\n",
        "# !pip install langchainhub\n",
        "# !pip install -U langchain-huggingface\n",
        "# !pip install -U langchain-cohere\n",
        "# !pip install --upgrade --quiet  wikipedia\n",
        "# !pip install --upgrade --quiet  arxiv\n",
        "# !pip install --upgrade --quiet  pymupdf\n",
        "\n",
        "# !pip install xmltodict\n",
        "\n",
        "# !pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8sodyjNlrhIZ",
        "outputId": "707f8eeb-698d-4b1e-fb9a-4cf954fd7a55"
      },
      "outputs": [],
      "source": [
        "# import importlib\n",
        "# from subprocess import run\n",
        "\n",
        "# def install_if_missing(package, import_name=None, extra_args=\"\"):\n",
        "#     try:\n",
        "#         importlib.import_module(import_name or package)\n",
        "#         print(f\"{package} is already installed. Skipping installation.\")\n",
        "#     except ImportError:\n",
        "#         print(f\"{package} not found. Installing now.\")\n",
        "#         run(f\"pip install {extra_args} {package}\", shell=True)\n",
        "\n",
        "# install_if_missing(\"git+https://github.com/huggingface/transformers\", \"transformers\", \"-q\")\n",
        "# install_if_missing(\"datasets\", extra_args=\"-q\")\n",
        "# install_if_missing(\"loralib\", extra_args=\"-q\")\n",
        "# install_if_missing(\"sentencepiece\", extra_args=\"-q\")\n",
        "# install_if_missing(\"bitsandbytes\", extra_args=\"-q\")\n",
        "# install_if_missing(\"accelerate\", extra_args=\"-q\")\n",
        "# install_if_missing(\"langchain\", extra_args=\"-q\")\n",
        "# install_if_missing(\"einops\")\n",
        "# install_if_missing(\"faiss-gpu\")\n",
        "# install_if_missing(\"langchain_community\")\n",
        "# install_if_missing(\"chromadb\", extra_args=\"--upgrade --quiet\")\n",
        "# install_if_missing(\"bs4\", extra_args=\"--upgrade --quiet\")\n",
        "# install_if_missing(\"qdrant-client\", \"qdrant_client\", extra_args=\"--upgrade --quiet\")\n",
        "# install_if_missing(\"langchainhub\")\n",
        "# install_if_missing(\"langchain-huggingface\", \"langchain_huggingface\", extra_args=\"-U\")\n",
        "# install_if_missing(\"langchain-cohere\", \"langchain_cohere\", extra_args=\"-U\")\n",
        "# install_if_missing(\"wikipedia\", extra_args=\"--upgrade --quiet\")\n",
        "# install_if_missing(\"arxiv\", extra_args=\"--upgrade --quiet\")\n",
        "# install_if_missing(\"pymupdf\", \"fitz\", extra_args=\"--upgrade --quiet\")\n",
        "# install_if_missing(\"xmltodict\")\n",
        "# install_if_missing(\"cohere\")\n",
        "# install_if_missing(\"loguru\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-a9CoFY0cdJk"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip -q install loguru"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hydrate Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND67290jbHNg",
        "outputId": "06453a9f-71e6-4d65-e4cb-b75e1d832a71"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import bs4\n",
        "import enum\n",
        "from loguru import logger\n",
        "from typing import List, Tuple, Any, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# from langchain_community.embeddings import HuggingFaceEmbeddings # deprecated\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_community.document_loaders import (\n",
        "    PyMuPDFLoader,\n",
        "    WikipediaLoader,\n",
        "    WebBaseLoader,\n",
        ")\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "class SupportedEmbeddingModels(enum.Enum):\n",
        "    MpNetBaseV2 = \"all-mpnet-base-v2\"\n",
        "    MiniLmL6V2 = \"all-MiniLM-L6-v2\"\n",
        "    DistilRobertaV1 = \"all-distilroberta-v1\"\n",
        "    MultiQaMpNetBasedCosV1 = \"multi-qa-mpnet-base-cos-v1\"\n",
        "    MultiQaMpNetBasedDotV1 = \"multi-qa-mpnet-base-dot-v1\"\n",
        "\n",
        "\n",
        "class SourceType(enum.Enum):\n",
        "    ARXIV = \"arxiv\"\n",
        "    WIKIPEDIA = \"wikipedia\"\n",
        "    WEBSITE = \"website\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataSource:\n",
        "    identifier: str\n",
        "    source_type: SourceType\n",
        "\n",
        "    # Optional metadata that can be added to all documents from this source\n",
        "    additional_metadata: Optional[Dict[str, Any]] = None\n",
        "\n",
        "\n",
        "def get_embedding_model(model: str) -> HuggingFaceEmbeddings:\n",
        "    \"\"\"\n",
        "    Creates and returns a HuggingFaceEmbeddings model for text embeddings.\n",
        "\n",
        "    Args:\n",
        "        model (str): The name of the Hugging Face model to use for embeddings.\n",
        "                    Should be one of the values in SupportedEmbeddingModels.\n",
        "\n",
        "    Returns:\n",
        "        HuggingFaceEmbeddings: The initialized embedding model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the model name is empty, invalid, or not supported.\n",
        "        ImportError: If the required dependencies are not installed.\n",
        "    \"\"\"\n",
        "    if not model or not isinstance(model, str):\n",
        "        raise ValueError(\"Model name must be a non-empty string\")\n",
        "\n",
        "    # Check if model is supported\n",
        "    supported_models = [m.value for m in SupportedEmbeddingModels]\n",
        "    if model not in supported_models:\n",
        "        raise ValueError(\n",
        "            f\"Unsupported model: {model}. Must be one of: {supported_models}\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        return HuggingFaceEmbeddings(model_name=model)\n",
        "    except ImportError as e:\n",
        "        raise ImportError(f\"Missing dependencies for HuggingFaceEmbeddings: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to load embedding model '{model}': {str(e)}\")\n",
        "\n",
        "\n",
        "def initialize_vector_db(\n",
        "    embedding_model_name: str = SupportedEmbeddingModels.MultiQaMpNetBasedCosV1,\n",
        "    collection_name: str = \"rag267\",\n",
        "    in_memory: bool = True,\n",
        "    force_recreate: bool = True,\n",
        ") -> Qdrant:\n",
        "    \"\"\"\n",
        "    Initialize a Qdrant vector database with specified embedding model.\n",
        "\n",
        "    Args:\n",
        "        embedding_model_name: Name of the Hugging Face embedding model to use\n",
        "        collection_name: Name for the Qdrant collection\n",
        "        in_memory: Whether to use in-memory storage (True) or disk storage (False)\n",
        "        force_recreate: Whether to recreate the collection if it exists\n",
        "\n",
        "    Returns:\n",
        "        Initialized Qdrant vectorstore object\n",
        "    \"\"\"\n",
        "    # Initialize embeddings\n",
        "    embeddings = get_embedding_model(embedding_model_name)\n",
        "\n",
        "    # Location for Qdrant storage\n",
        "    db_location = (\n",
        "        \":memory:\" if in_memory else \"http://localhost:6333\"\n",
        "    )  # TODO make this more robust\n",
        "\n",
        "    # Create an empty Qdrant vectorstore\n",
        "    vectorstore = Qdrant.from_documents(\n",
        "        documents=[\n",
        "            Document(page_content=\"Initialization document\", metadata={})\n",
        "        ],  # Just to initialize\n",
        "        embedding=embeddings,\n",
        "        location=db_location,\n",
        "        collection_name=collection_name,\n",
        "        force_recreate=force_recreate,\n",
        "    )\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def load_arxiv_documents(\n",
        "    arxiv_id: str, doc_id: int, additional_metadata: Optional[Dict[str, Any]] = None\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from an ArXiv paper.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: ArXiv ID of the paper\n",
        "        doc_id: Document ID to assign\n",
        "        additional_metadata: Additional metadata to add to documents\n",
        "\n",
        "    Returns:\n",
        "        List of Document objects\n",
        "    \"\"\"\n",
        "    # Construct URL using the arXiv unique identifier\n",
        "    arx_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
        "\n",
        "    try:\n",
        "        # Extract pages from the document\n",
        "        arx_loader = PyMuPDFLoader(arx_url)\n",
        "        arx_pages = arx_loader.load()\n",
        "\n",
        "        # Add metadata to each page\n",
        "        for page_num in range(len(arx_pages)):\n",
        "            page = arx_pages[page_num]\n",
        "            page.metadata[\"page_num\"] = page_num\n",
        "            page.metadata[\"doc_num\"] = doc_id\n",
        "            page.metadata[\"doc_source\"] = \"ArXiv\"\n",
        "            page.metadata[\"source_id\"] = arxiv_id\n",
        "\n",
        "            # Add any additional metadata\n",
        "            if additional_metadata:\n",
        "                for key, value in additional_metadata.items():\n",
        "                    page.metadata[key] = value\n",
        "\n",
        "        return arx_pages\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error loading ArXiv document {arxiv_id}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def load_wikipedia_documents(\n",
        "    query: str,\n",
        "    doc_id: int,\n",
        "    max_docs: int = 4,\n",
        "    additional_metadata: Optional[Dict[str, Any]] = None,\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from Wikipedia.\n",
        "\n",
        "    Args:\n",
        "        query: Search query for Wikipedia\n",
        "        doc_id: Document ID to assign\n",
        "        max_docs: Maximum number of documents to load\n",
        "        additional_metadata: Additional metadata to add to documents\n",
        "\n",
        "    Returns:\n",
        "        List of Document objects\n",
        "    \"\"\"\n",
        "    try:\n",
        "        wiki_docs = WikipediaLoader(query=query, load_max_docs=max_docs).load()\n",
        "\n",
        "        # Add metadata to each document\n",
        "        for doc in wiki_docs:\n",
        "            doc.metadata[\"doc_num\"] = doc_id\n",
        "            doc.metadata[\"doc_source\"] = \"Wikipedia\"\n",
        "            doc.metadata[\"source_id\"] = query\n",
        "\n",
        "            # Add any additional metadata\n",
        "            if additional_metadata:\n",
        "                for key, value in additional_metadata.items():\n",
        "                    doc.metadata[key] = value\n",
        "\n",
        "        return wiki_docs\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error loading Wikipedia documents for query {query}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def load_website_documents(\n",
        "    url: str, doc_id: int, additional_metadata: Optional[Dict[str, Any]] = None\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents from a website.\n",
        "\n",
        "    Args:\n",
        "        url: URL of the website\n",
        "        doc_id: Document ID to assign\n",
        "        additional_metadata: Additional metadata to add to documents\n",
        "\n",
        "    Returns:\n",
        "        List of Document objects\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Set up the web loader with BeautifulSoup settings to extract relevant content\n",
        "        web_loader = WebBaseLoader(\n",
        "            web_paths=(url,),\n",
        "            bs_kwargs=dict(\n",
        "                parse_only=bs4.SoupStrainer(\n",
        "                    class_=(\n",
        "                        \"post-content\",\n",
        "                        \"post-title\",\n",
        "                        \"post-header\",\n",
        "                        \"article\",\n",
        "                        \"content\",\n",
        "                        \"main\",\n",
        "                    )\n",
        "                )\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        web_documents = web_loader.load()\n",
        "\n",
        "        # Add metadata to each document\n",
        "        for doc in web_documents:\n",
        "            doc.metadata[\"doc_num\"] = doc_id\n",
        "            doc.metadata[\"doc_source\"] = \"Website\"\n",
        "            doc.metadata[\"source_id\"] = url\n",
        "\n",
        "            # Add any additional metadata\n",
        "            if additional_metadata:\n",
        "                for key, value in additional_metadata.items():\n",
        "                    doc.metadata[key] = value\n",
        "\n",
        "        return web_documents\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error loading website document from {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def split_and_add_documents(\n",
        "    vectorstore: Qdrant,\n",
        "    documents: List[Document],\n",
        "    chunk_size: int = 128,\n",
        "    chunk_overlap: int = 0,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Split documents into chunks and add them to the vectorstore.\n",
        "\n",
        "    Args:\n",
        "        vectorstore: Qdrant vectorstore object\n",
        "        documents: List of Document objects\n",
        "        chunk_size: Size of each chunk\n",
        "        chunk_overlap: Overlap between chunks\n",
        "    \"\"\"\n",
        "    if not documents:\n",
        "        return\n",
        "\n",
        "    # Create text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    # Split documents\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Add split_id to metadata\n",
        "    for idx, split in enumerate(splits):\n",
        "        split.metadata[\"split_id\"] = idx\n",
        "\n",
        "    # Add to vectorstore\n",
        "    vectorstore.add_documents(documents=splits)\n",
        "\n",
        "    logger.info(f\"Added {len(splits)} chunks to the vectorstore\")\n",
        "\n",
        "\n",
        "def hydrate_vector_db(\n",
        "    vectorstore: Qdrant,\n",
        "    data_sources: List[DataSource],\n",
        "    chunk_size: int = 128,\n",
        "    chunk_overlap: int = 0,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Hydrate the vector database with documents from various sources.\n",
        "\n",
        "    Args:\n",
        "        vectorstore: Qdrant vectorstore object\n",
        "        data_sources: List of DataSource objects\n",
        "        chunk_size: Size of each chunk\n",
        "        chunk_overlap: Overlap between chunks\n",
        "    \"\"\"\n",
        "    doc_id = 1\n",
        "\n",
        "    for data_source in data_sources:\n",
        "        logger.info(\n",
        "            f\"Processing source: {data_source.identifier} ({data_source.source_type.value})\"\n",
        "        )\n",
        "\n",
        "        documents = []\n",
        "\n",
        "        # Route to the appropriate loading function based on source type\n",
        "        if data_source.source_type == SourceType.ARXIV:\n",
        "            documents = load_arxiv_documents(\n",
        "                data_source.identifier, doc_id, data_source.additional_metadata\n",
        "            )\n",
        "        elif data_source.source_type == SourceType.WIKIPEDIA:\n",
        "            documents = load_wikipedia_documents(\n",
        "                data_source.identifier,\n",
        "                doc_id,\n",
        "                additional_metadata=data_source.additional_metadata,\n",
        "            )\n",
        "        elif data_source.source_type == SourceType.WEBSITE:\n",
        "            documents = load_website_documents(\n",
        "                data_source.identifier, doc_id, data_source.additional_metadata\n",
        "            )\n",
        "        else:\n",
        "            logger.warning(f\"Unknown source type: {data_source.source_type}\")\n",
        "            continue\n",
        "\n",
        "        # Split and add documents to vectorstore\n",
        "        split_and_add_documents(vectorstore, documents, chunk_size, chunk_overlap)\n",
        "\n",
        "        doc_id += 1\n",
        "\n",
        "    logger.info(f\"Finished hydrating vector database with {doc_id - 1} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VectorDatabaseManager:\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_model_name: str = \"multi-qa-mpnet-base-dot-v1\",\n",
        "        collection_name: str = \"myrag\",\n",
        "        chunk_size: int = 128,\n",
        "        chunk_overlap: int = 0,\n",
        "        in_memory: bool = True,\n",
        "        force_recreate: bool = True,\n",
        "    ):\n",
        "        self.embedding_model_name = embedding_model_name\n",
        "        self.collection_name = collection_name\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.in_memory = in_memory\n",
        "        self.force_recreate = force_recreate\n",
        "        \n",
        "        # *** VECTORSTORE ***\n",
        "        self.vectorstore = initialize_vector_db(\n",
        "            embedding_model_name=embedding_model_name,\n",
        "            collection_name=collection_name,\n",
        "            in_memory=in_memory,\n",
        "            force_recreate=force_recreate,\n",
        "        )\n",
        "\n",
        "    def hydrate(self, data_sources: List[DataSource]) -> None:\n",
        "        hydrate_vector_db(\n",
        "            self.vectorstore,\n",
        "            data_sources,\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap,\n",
        "        )\n",
        "\n",
        "    def get_config(self) -> dict:\n",
        "        return {\n",
        "            'embedding_model': self.embedding_model_name,\n",
        "            'collection_name': self.collection_name,\n",
        "            'in_memory': self.in_memory,\n",
        "            'force_recreate': self.force_recreate,\n",
        "            'chunking_strategy': {\n",
        "                'chunk_size': self.chunk_size,\n",
        "                'chunk_overlap': self.chunk_overlap,\n",
        "                'splitter_type': \"RecursiveCharacterTextSplitter\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "class VectorDatabaseRunner:\n",
        "    def __init__(self):\n",
        "        self.instances = {}\n",
        "\n",
        "    def add_instance(self, name: str, manager: VectorDatabaseManager):\n",
        "        self.instances[name] = manager\n",
        "\n",
        "    def get_instance(self, name: str) -> VectorDatabaseManager:\n",
        "        return self.instances.get(name)\n",
        "\n",
        "    def list_instances(self) -> List[str]:\n",
        "        return list(self.instances.keys())\n",
        "\n",
        "# Example usage:\n",
        "# vdb_runner = VectorDatabaseRunner()\n",
        "\n",
        "# # Create and add multiple instances\n",
        "# vdb_runner.add_instance(\n",
        "#     \"default\",\n",
        "#     VectorDatabaseManager(),\n",
        "# )\n",
        "\n",
        "# vdb_runner.add_instance(\n",
        "#     \"custom_chunk\",\n",
        "#     VectorDatabaseManager(chunk_size=256, chunk_overlap=50),\n",
        "# )\n",
        "\n",
        "# print(vdb_runner)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "vdm = VectorDatabaseManager(\n",
        "    embedding_model_name=\"multi-qa-mpnet-base-dot-v1\",\n",
        "    collection_name=\"myrag\",\n",
        "    chunk_size=128,\n",
        "    chunk_overlap=0,\n",
        "    in_memory=True,\n",
        "    force_recreate=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'embedding_model': 'multi-qa-mpnet-base-dot-v1',\n",
              " 'collection_name': 'myrag',\n",
              " 'in_memory': True,\n",
              " 'force_recreate': True,\n",
              " 'chunking_strategy': {'chunk_size': 128,\n",
              "  'chunk_overlap': 0,\n",
              "  'splitter_type': 'RecursiveCharacterTextSplitter'}}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vdm.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "vdr = VectorDatabaseRunner()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['default']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vdr.add_instance('default', vdm)\n",
        "\n",
        "vdr.list_instances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data sources\n",
        "data_sources = [\n",
        "    # ArXiv papers\n",
        "    DataSource(identifier=\"2005.11401\", source_type=SourceType.ARXIV),\n",
        "    DataSource(identifier=\"2104.07567\", source_type=SourceType.ARXIV),\n",
        "    # Wikipedia articles\n",
        "    DataSource(\n",
        "        identifier=\"Generative Artificial Intelligence\",\n",
        "        source_type=SourceType.WIKIPEDIA,\n",
        "    ),\n",
        "    DataSource(\n",
        "        identifier=\"Large Language Models\",\n",
        "        source_type=SourceType.WIKIPEDIA,\n",
        "        additional_metadata={\"category\": \"AI Models\"},\n",
        "    ),\n",
        "    # Websites\n",
        "    DataSource(\n",
        "        identifier=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "        source_type=SourceType.WEBSITE,\n",
        "    ),\n",
        "    DataSource(\n",
        "        identifier=\"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n",
        "        source_type=SourceType.WEBSITE,\n",
        "        additional_metadata={\"author\": \"Lilian Weng\"},\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746,
          "referenced_widgets": [
            "f61bb98c3b3348dab028a665ae8f0c38",
            "387bd51ec7e645e88e74c80f6f12e78a",
            "5b231cf463c844fcb520e1055acde69c",
            "360688c187f5463cb9bb8dd2157cefa2",
            "6b681faa8ddf48d2ab966be163ad3e40",
            "c181dc7cab144be08b4cabe38879aefe",
            "0a93364cd1034f87a1ad939510bd13c5",
            "bacc640b4edc42069130e49a98048829",
            "8fc2ed9b65664d5e95d3acf7eaef0159",
            "27678faf333f408788e3269b37bcec0f",
            "9355d75a5e364e88a021ef8077bfc4b0",
            "edad1644d5804c61bc16163b9d6f0cc1",
            "bd394eb35cb9403184914c64a68b91d6",
            "54a79bce342e4a72901998f90992f3e9",
            "b5796395032c4ab79b79bd21b4a1b2d0",
            "53436395dff1406ab0fe30b82f1a8aaa",
            "341b7510fa9b433392bcffad7114eeee",
            "56bd1b6b90e24a7cba6bd503e6656939",
            "a02e6283104a4a6786e4b65321b956b8",
            "be3a3ddde3584583a0fe60aecfc63563",
            "98b498a56adc4198a56dd83e8ee0102c",
            "5268c3134818448eaa0731511b2cd7dc",
            "b051a369955c4ab5adcbe5363b3b53ab",
            "d93d30d9cfac47a0ab6bf1581780d8cc",
            "8d6cce2936ab4e6aa33cca2c3687ca20",
            "6212177143bf4c2eb9e66934efb49d11",
            "224b7fab89744239a8dd747e5be89aaa",
            "731220e2bdcf46139f4370eac93ce5ae",
            "f6251266f701456a97b8cab8472d0027",
            "797fd7c0ae104bc4a9f5ca7a46b5f562",
            "ffcaf0176810401d8baf5a709408be58",
            "3e48cbd2e3f4456cbc8b2d2cf8c2994a",
            "2c16ea58087d44dda0a307934e50912b",
            "45e9c00c10b447c88df63f18efb52632",
            "cf20ba7837ed4d6bbc47d49e442ad45f",
            "e4dc7967c2ae4620974eda53f724be7d",
            "4fc52afe944d41b69c8bdc3a84effa54",
            "1d730dff53b141a48fe12b13d8440257",
            "e9d63e9475274a4d80ccfad50caa2130",
            "012e63ef86894290a11080a8b22bb903",
            "f18f9b575f934fe9a5ba75e98bce11c7",
            "e312eac78a374af88fc156e5a9e87f7a",
            "3dbadc5dd0f44239bd049ed07b6a669b",
            "9528790f946949e596eb5c8d3cf601f8",
            "ea33c75804854721b3856293c85da72b",
            "1639f2d1172746c2b4c32f033ce6625f",
            "47e5ffc30ef04e4a8df75850b1ba1f39",
            "7688794547ec427989ee05f8f86823f8",
            "a043c64ce355492988a41a4b893ecd8f",
            "16e0fa8f37dd46d6811f2c160bf05547",
            "08c4c2618862443494abc48f8557f955",
            "bc694ff92d324d6084504808aad50956",
            "e4f2b72c2bbd49f0b03d1b92540d2030",
            "f898238cd8a448838d626679bf4114c7",
            "29e4769caac44fd0acbcbeaa19fed14a",
            "2788ddc0936e4e14b17108eae5861e9d",
            "14896be7b47d41bfac1dd5c932beff0a",
            "31d69c318150444eabca6bf57abe1ec8",
            "b4634cba00df4a91a0cc06ba70bb07c6",
            "f1c451ffac75420092bd6e6488cca4c8",
            "ee83dd1d169940c18301c0bb9958098e",
            "6f826325488e4933a4df3a87b9a31e51",
            "ba1659f4589f46089ba640165dc4d181",
            "6f09141374824d3ea7569c3982f8ae50",
            "16a1008d9c134f798e2d23bbb3477b7e",
            "c0584e8a421f40de9f4d01cc70f18123",
            "c5640836ba76486dbcd322797c65aa56",
            "8085f68b3f9e4651a95b7362053d0802",
            "c17de30438604cb1871664935ee20e7d",
            "35892a2030304c99a3a32f676633482b",
            "147f9868c769437c987a965d1100026c",
            "b8511aea1b0d4374a82f0595061f46c7",
            "866bed08b3a749b7b052e6ef03d4599d",
            "f1c82268cf3f44349b552c1a21a53a0b",
            "c4dbb25b16c247648c54f5c3a55460af",
            "c8d61a6fade84aeeb2554bda794b08f4",
            "488080359cf041d18894f7d40988a21d",
            "d7f8bce676ed47c6bfa79a7c469102b4",
            "e3427b9fd72e49b0b31c5b693aa265f0",
            "86511ea338cf4b728c26c59c77355999",
            "be7bc2efd17943449852bee5661c02b3",
            "5ad4674dd3914c90a4c63c8466f008fd",
            "71dd30b8f023404684ef8439f603b675",
            "31747ba9223f4d61806a7a34216330b2",
            "e95a45d783834274998e5226994e3f0b",
            "a06059d0740540959ef4b732875487ab",
            "a4245fd8f28d47d5bad13aa2a5748a50",
            "b0e73b5521554aff8b6cb738d7b6d57b",
            "5c52155aa938499cb7df5c766e050d96",
            "b10b17d92ab14134999d5913084aaad2",
            "95eb41cb46a74e62879827bfe2c1bb4c",
            "f1242bf788f34a548e16f58cdee2d04a",
            "69b8a35bfada4eb9a37f47ce4865c296",
            "20fa5573029f4fd7a9f78ca7bb67309f",
            "989460806f7c423f96741bc259a4e29b",
            "be20bc4f71e744fda9c25788f244b8a5",
            "ece26bd7586142a3be5db5ab84dfb039",
            "b51e3615d19a4beb93eb8a7713db108b",
            "53d0ff8830704b6ebde0ee6b97a61027",
            "1fb65402b1a646ddb85b7fecb9655677",
            "575341da64d5420081312aab56010045",
            "c528f458fa124c67a4193dacb604816e",
            "cf65282b0fa34e0ab410ce0de848cca3",
            "0f2eb670a6bf4423ab6c6642cbca8d3b",
            "e080fb08f4c541c49dd85d2e76790fa2",
            "af5ef67c23a44bc68c3b95b30c3f055e",
            "cc7c681203ba4c2a86a8b3124c7c4715",
            "d79ecc22b0204a14841c9214a8da732e",
            "816c91be94db435c92b6f88b46776de6",
            "b5a654cc87d24eafb6996aacb1869e3e",
            "6b691b1d777f4d348223bb09afc0643a",
            "100da52bb40f4e56b7ab446419347669",
            "36579715748f42eab854917f3484ee08",
            "c4f5f399f1384aeea27f03017a2ab6dc",
            "5d9fdb26d12f40d2bd428b7f86a9b45b",
            "a6245ccaefe141e883b96e871524a876",
            "ece2931f44e349c39794259a530d05e3",
            "40e76af80291407a951555fd57e99149",
            "1dbdc9d94b4e4186b204c845eddb0beb",
            "626b1c5f885a4a16b16cb99572dad3b7",
            "74745948e0e24210a0fc928052c2c299"
          ]
        },
        "id": "yhXeuJt2bwOf",
        "outputId": "e0415d08-3a44-48d6-9365-3a4ca9d573ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-03-23 15:33:23.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhydrate_vector_db\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mProcessing source: 2005.11401 (arxiv)\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:25.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_and_add_documents\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mAdded 731 chunks to the vectorstore\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:25.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhydrate_vector_db\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mProcessing source: 2104.07567 (arxiv)\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:28.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_and_add_documents\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mAdded 838 chunks to the vectorstore\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:28.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhydrate_vector_db\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mProcessing source: Generative Artificial Intelligence (wikipedia)\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:29.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_and_add_documents\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mAdded 157 chunks to the vectorstore\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:29.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhydrate_vector_db\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mProcessing source: Large Language Models (wikipedia)\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:31.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_and_add_documents\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mAdded 129 chunks to the vectorstore\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:31.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhydrate_vector_db\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mProcessing source: https://lilianweng.github.io/posts/2023-06-23-agent/ (website)\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:32.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_and_add_documents\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mAdded 448 chunks to the vectorstore\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:32.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhydrate_vector_db\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mProcessing source: https://lilianweng.github.io/posts/2020-10-29-odqa/ (website)\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:34.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_and_add_documents\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mAdded 505 chunks to the vectorstore\u001b[0m\n",
            "\u001b[32m2025-03-23 15:33:34.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mhydrate_vector_db\u001b[0m:\u001b[36m330\u001b[0m - \u001b[1mFinished hydrating vector database with 6 documents\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "\n",
        "# Initialize vector database\n",
        "# vectorstore = initialize_vector_db(\n",
        "#     embedding_model_name=\"multi-qa-mpnet-base-dot-v1\",\n",
        "#     collection_name=\"myrag\",\n",
        "#     in_memory=True,\n",
        "#     force_recreate=True,\n",
        "# )\n",
        "\n",
        "# Hydrate vector database\n",
        "# hydrate_vector_db(\n",
        "#     vectorstore=vectorstore,\n",
        "#     data_sources=data_sources,\n",
        "#     chunk_size=128,\n",
        "#     chunk_overlap=0,\n",
        "# )\n",
        "\n",
        "vdm.hydrate(data_sources=data_sources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZkEYSAyGdTgL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, List, Optional, Any, Callable\n",
        "import enum\n",
        "from dotenv import load_dotenv\n",
        "from loguru import logger\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "\n",
        "class SupportedGeneratorModels(enum.Enum):\n",
        "    MistralInstructV2 = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "class ModelType(enum.Enum):\n",
        "    Mistral = \"mistral\"\n",
        "    Cohere = \"cohere\"\n",
        "\n",
        "class Team(enum.Enum):\n",
        "    Engineering = \"engineering\"\n",
        "    Marketing = \"marketing\"\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_db_manager: VectorDatabaseManager,\n",
        "        engineering_template_path: str = \"templates/engineering_template.txt\",\n",
        "        marketing_template_path: str = \"templates/marketing_template.txt\",\n",
        "        cohere_api_key: Optional[str] = None,\n",
        "        use_cohere: bool = True,\n",
        "        use_mistral: bool = False,\n",
        "        mistral_model_name: SupportedGeneratorModels = SupportedGeneratorModels.MistralInstructV2,\n",
        "        top_k: int = 4,\n",
        "    ):\n",
        "\n",
        "        if use_cohere == True and use_mistral == True:\n",
        "            raise ValueError(\"use_cohere and use_mistral cannot both be True. Choose one LLM.\")\n",
        "\n",
        "        # load_dotenv()\n",
        "\n",
        "        self.config = {\n",
        "            'llm': {},\n",
        "            'top_k': top_k,\n",
        "            'engineering_template_path': engineering_template_path,\n",
        "            'marketing_template_path': marketing_template_path,\n",
        "        }\n",
        "\n",
        "        # self.hf_token = os.getenv('HF_TOKEN')\n",
        "\n",
        "        self.vdm = vector_db_manager\n",
        "        # self.vectorstore = vdm.vectorstore\n",
        "        self.retriever = vdm.vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "        if use_mistral:\n",
        "            self.llm = self._init_mistral(mistral_model_name)\n",
        "\n",
        "        if use_cohere:\n",
        "            if not cohere_api_key:\n",
        "                raise ValueError(\"Cohere API key is required when use_cohere=True\")\n",
        "            self.llm = self._init_cohere(cohere_api_key)\n",
        "\n",
        "        self.engineering_template = self._load_template(engineering_template_path)\n",
        "        self.marketing_template = self._load_template(marketing_template_path)\n",
        "\n",
        "        self.engineering_prompt = ChatPromptTemplate.from_template(\n",
        "            self.engineering_template\n",
        "        )\n",
        "        self.marketing_prompt = ChatPromptTemplate.from_template(\n",
        "            self.marketing_template\n",
        "        )\n",
        "\n",
        "        self.output_parser = StrOutputParser()\n",
        "\n",
        "    def get_config(self) -> dict:\n",
        "        summary = {\n",
        "            **self.config,\n",
        "            'vectorstore': self.vdm.get_config()\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "    def _load_template(self, template_path: str) -> str:\n",
        "        try:\n",
        "            with open(template_path, \"r\") as f:\n",
        "                template = f.read()\n",
        "            return template\n",
        "        except FileNotFoundError as e:\n",
        "            logger.exception(e)\n",
        "            logger.warning(\n",
        "                f\"Template file {template_path} not found. Using default template.\"\n",
        "            )\n",
        "            return e\n",
        "\n",
        "    def _init_mistral(self, model_name: SupportedGeneratorModels):\n",
        "        logger.info(f\"Initializing Mistral model: {model_name}\")\n",
        "\n",
        "        quant_cfg = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float32,\n",
        "            quantization_config=quant_cfg,\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=1000,\n",
        "            temperature=0.6,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2,\n",
        "        )\n",
        "\n",
        "        pipe.model.config.pad_token_id = pipe.model.config.eos_token_id\n",
        "\n",
        "        self.config['llm'] = {\n",
        "            'family': ModelType.Mistral,\n",
        "            'model_name': model_name,\n",
        "            'quantization': {'load_in_4bit': True},\n",
        "            'tokenizer': tokenizer.name_or_path,\n",
        "            'temperature': 0.6,\n",
        "            'top_p': 0.95,\n",
        "            'max_new_tokens': 1000,\n",
        "            'repetition_penalty': 1.2,\n",
        "        }\n",
        "\n",
        "        return HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    def _init_cohere(self, api_key: str):\n",
        "        logger.info(\"Initializing Cohere model\")\n",
        "        model_name = \"command-r\"\n",
        "        self.config['llm'] = {\n",
        "            'family': ModelType.Cohere,\n",
        "            'model_name': model_name,\n",
        "            'api_key_provided': bool(api_key)\n",
        "        }\n",
        "        return ChatCohere(cohere_api_key=api_key, model=model_name)\n",
        "\n",
        "    def format_docs(self, docs):\n",
        "        \"\"\"Format a list of documents into a string.\"\"\"\n",
        "        return \"\\n\\n\".join(\n",
        "            f\"Document {i + 1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)\n",
        "        )\n",
        "\n",
        "    def query_vectorstore(self, query: str) -> List[Any]:\n",
        "        \"\"\"Get retrieved documents for a query.\"\"\"\n",
        "        return self.retriever.invoke(query)\n",
        "\n",
        "    def get_retrieval_metadata(self, query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get metadata about the retrieved documents.\"\"\"\n",
        "        docs = self.query_vectorstore(query)\n",
        "        return [doc.metadata for doc in docs]\n",
        "\n",
        "    def invoke(self, team: Team, query: str) -> str:\n",
        "        \"\"\"Generate an answer for the engineering team.\"\"\"\n",
        "\n",
        "        if not isinstance(team, Team):\n",
        "            raise ValueError(f\"Invalid team: {team}\")\n",
        "\n",
        "        if not self.llm:\n",
        "            raise ValueError(\"LLM not initialized\")\n",
        "\n",
        "        prompt = self.engineering_prompt if team == Team.Engineering else self.marketing_prompt\n",
        "\n",
        "        # Create chain\n",
        "        chain = (\n",
        "            {\n",
        "                \"context\": self.retriever | self.format_docs,\n",
        "                \"question\": RunnablePassthrough(),\n",
        "            }\n",
        "            | prompt\n",
        "            | self.llm\n",
        "            | self.output_parser\n",
        "        )\n",
        "\n",
        "        # Run chain\n",
        "        return chain.invoke(query)\n",
        "\n",
        "    def generate_responses(self, query: str) -> Dict[str, str]:\n",
        "        \"\"\"Generate responses for both engineering and marketing teams.\"\"\"\n",
        "        engineering_response = self.invoke(Team.Engineering, query)\n",
        "        marketing_response = self.invoke(Team.Marketing, query)\n",
        "\n",
        "        return {\n",
        "            \"engineering\": engineering_response,\n",
        "            \"marketing\": marketing_response\n",
        "        }\n",
        "\n",
        "    def get_document_sources(self, query: str) -> List[str]:\n",
        "        docs = self.query_vectorstore(query)\n",
        "        sources = []\n",
        "\n",
        "        for doc in docs:\n",
        "            source_info = \"\"\n",
        "            if \"doc_source\" in doc.metadata:\n",
        "                source_info += doc.metadata[\"doc_source\"]\n",
        "            if \"source_id\" in doc.metadata:\n",
        "                source_info += f\": {doc.metadata['source_id']}\"\n",
        "            sources.append(source_info)\n",
        "\n",
        "        return sources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize and test the RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0m_ttO3Oc93_"
      },
      "outputs": [],
      "source": [
        "# Uncomment for Google Colab:\n",
        "# from google.colab import userdata\n",
        "# cohere_api_key = userdata.get(\"COHERE_API_KEY\")\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "cohere_api_key = os.getenv('COHERE_API_KEY_PROD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bblSPMLjdbgo"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "engineering_template = Path.cwd() / \"../templates/engineering_template.txt\"\n",
        "marketing_template = Path.cwd() / \"../templates/marketing_template.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "# login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-03-23 15:45:24.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_init_cohere\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mInitializing Cohere model\u001b[0m\n",
            "\u001b[32m2025-03-23 15:45:24.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_init_mistral\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mInitializing Mistral model: mistralai/Mistral-7B-Instruct-v0.2\u001b[0m\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c9edf25504241f3bf5a436e0a2bda86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "rag_system_cohere = RAGSystem(\n",
        "    vector_db_manager=vdm,\n",
        "    engineering_template_path=engineering_template,\n",
        "    marketing_template_path=marketing_template,\n",
        "    cohere_api_key=cohere_api_key,\n",
        "    use_mistral=False,\n",
        "    use_cohere=True,\n",
        "    mistral_model_name=SupportedGeneratorModels.MistralInstructV2.value,\n",
        "    top_k=4,\n",
        ")\n",
        "rag_system_mistral = RAGSystem(\n",
        "    vector_db_manager=vdm,\n",
        "    engineering_template_path=engineering_template,\n",
        "    marketing_template_path=marketing_template,\n",
        "    cohere_api_key=cohere_api_key,\n",
        "    use_mistral=True,\n",
        "    use_cohere=False,\n",
        "    mistral_model_name=SupportedGeneratorModels.MistralInstructV2.value,\n",
        "    top_k=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5blUUNcGd88f",
        "outputId": "b1828c87-cefe-4e30-9fc7-8eaee3c6e808"
      },
      "outputs": [],
      "source": [
        "rag_system = rag_system_mistral\n",
        "# rag_system = rag_system_cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vbzWQ_Qbes9u"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "answer_research = rag_system.invoke(Team.Engineering, \"What defines a large language model in the context of natural language processing tasks?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Nuk7IUtdAQCQ"
      },
      "outputs": [],
      "source": [
        "answer_marketing = rag_system.invoke(Team.Marketing, \"What defines a large language model in the context of natural language processing tasks?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-nN0F1yq_7Lu"
      },
      "outputs": [],
      "source": [
        "gold_answer_research = \"A large language model in the context of natural language processing tasks is characterized by its ability to achieve general-purpose language generation and other NLP tasks through self-supervised and semi-supervised training on large datasets. These models typically utilize feedforward neural networks and transformers, surpassing earlier models like recurrent neural networks and word n-gram language models. They are often pre-trained on vast amounts of text data from the internet and can be fine-tuned for specific downstream tasks such as summarization, machine reading comprehension, and natural language to SQL translation.\"\n",
        "\n",
        "gold_answer_marketing = \"A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dchFb5O__5Y",
        "outputId": "47a888cd-fe6e-4f0d-f450-61c2f018dff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of research answer: 3969\n",
            "Length of research gold answer: 633\n"
          ]
        }
      ],
      "source": [
        "print(f\"Length of research answer: {len(answer_research)}\")\n",
        "print(f\"Length of research gold answer: {len(gold_answer_research)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61COuog8AfTK",
        "outputId": "6c0ededa-7fea-4c8e-b65c-0bf9f222f522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Legnth of marketing answer: 611\n",
            "Length of marketing gold answer: 363\n"
          ]
        }
      ],
      "source": [
        "print(f\"Legnth of marketing answer: {len(answer_marketing)}\")\n",
        "print(f\"Length of marketing gold answer: {len(gold_answer_marketing)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh0_M3bfAoGV",
        "outputId": "f2e5c85b-ba84-40a9-e283-10f272e8fbab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Research answer cosine similarity: [[0.84549987]]\n",
            "Marketing answer cosine similarity: [[0.89585698]]\n"
          ]
        }
      ],
      "source": [
        "# cosine similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "base_embeddings = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")\n",
        "answer_research_embedding = base_embeddings.embed_query(answer_research)\n",
        "answer_marketing_embedding = base_embeddings.embed_query(answer_marketing)\n",
        "\n",
        "gold_answer_research_embedding = base_embeddings.embed_query(gold_answer_research)\n",
        "gold_answer_marketing_embedding = base_embeddings.embed_query(gold_answer_marketing)\n",
        "\n",
        "print(f\"Research answer cosine similarity: {cosine_similarity([answer_research_embedding], [gold_answer_research_embedding])}\")\n",
        "print(f\"Marketing answer cosine similarity: {cosine_similarity([answer_marketing_embedding], [gold_answer_marketing_embedding])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TshREWpvBURi",
        "outputId": "71d511b4-aa8d-4308-e13d-737f9b7345e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# BLEU\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def compute_bleu(reference: str, candidate: str):\n",
        "    reference_tokens: list[str] = word_tokenize(reference)\n",
        "    candidate_tokens: list[str] = word_tokenize(candidate)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = sentence_bleu(reference_tokens, candidate_tokens)\n",
        "\n",
        "    # For example, using only unigrams and bigrams with equal weights\n",
        "    # weights = (0.5, 0.5, 0, 0)\n",
        "    # bleu_score = sentence_bleu(reference_tokens, candidate_tokens, weights=weights)\n",
        "\n",
        "    return bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F29HzkIUB6Yv",
        "outputId": "17b93902-c342-4962-f5cd-79ffdf546eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Research answer BLEU score: 6.190746313491463e-232\n",
            "Marketing answer BLEU score: 8.551484609999234e-232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "bleu_score_research = compute_bleu(gold_answer_research, answer_research)\n",
        "bleu_score_marketing = compute_bleu(gold_answer_marketing, answer_marketing)\n",
        "\n",
        "print(f\"Research answer BLEU score: {bleu_score_research}\")\n",
        "print(f\"Marketing answer BLEU score: {bleu_score_marketing}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Full Pass With Basic Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"../data/validation_question_answers.json\", \"r\") as f:\n",
        "    validation_question_answers = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Questions:  47%|████▋     | 37/78 [04:15<04:48,  7.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error invoking Cohere API: status_code: 504, body: stream timeout, attempt 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Questions: 100%|██████████| 78/78 [14:21<00:00, 11.04s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test questions answered and evaluated in 861.10 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load sentence embedding model for cosine similarity\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Store results with evaluation metrics\n",
        "answers_with_metrics = []\n",
        "\n",
        "for key, data in tqdm(validation_question_answers.items(), desc=\"Processing Questions\"):\n",
        "    question = data[\"question\"]\n",
        "\n",
        "    # Invoke RAG system with error handling and retry logic\n",
        "    def robust_invoke(team, question, retries=3, delay=10):\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                return rag_system.invoke(team, question)\n",
        "            except Exception as e:\n",
        "                print(f\"Error invoking Cohere API: {e}, attempt {attempt + 1}/{retries}\")\n",
        "                if attempt < retries - 1:\n",
        "                    time.sleep(delay)\n",
        "                else:\n",
        "                    return \"[API Error: No response]\"\n",
        "\n",
        "    answer_research = robust_invoke(Team.Engineering, question)\n",
        "    answer_marketing = robust_invoke(Team.Marketing, question)\n",
        "\n",
        "    # Gold answers\n",
        "    gold_answer_research = data[\"gold_answer_research\"]\n",
        "    gold_answer_marketing = data[\"gold_answer_marketing\"]\n",
        "\n",
        "    # Calculate BLEU\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_research = sentence_bleu(\n",
        "        [gold_answer_research.split()], answer_research.split(), smoothing_function=smoothie\n",
        "    )\n",
        "    bleu_marketing = sentence_bleu(\n",
        "        [gold_answer_marketing.split()], answer_marketing.split(), smoothing_function=smoothie\n",
        "    )\n",
        "\n",
        "    # Calculate ROUGE\n",
        "    rouge_research = rouge.score(gold_answer_research, answer_research)\n",
        "    rouge_marketing = rouge.score(gold_answer_marketing, answer_marketing)\n",
        "\n",
        "    # Cosine similarity of embeddings\n",
        "    embed_gold_research = embedding_model.encode(gold_answer_research)\n",
        "    embed_answer_research = embedding_model.encode(answer_research)\n",
        "    cosine_research = util.cos_sim(embed_gold_research, embed_answer_research).item()\n",
        "\n",
        "    embed_gold_marketing = embedding_model.encode(gold_answer_marketing)\n",
        "    embed_answer_marketing = embedding_model.encode(answer_marketing)\n",
        "    cosine_marketing = util.cos_sim(embed_gold_marketing, embed_answer_marketing).item()\n",
        "\n",
        "    # Length difference\n",
        "    len_diff_research = abs(len(answer_research.split()) - len(gold_answer_research.split()))\n",
        "    len_diff_marketing = abs(len(answer_marketing.split()) - len(gold_answer_marketing.split()))\n",
        "\n",
        "    answers_with_metrics.append({\n",
        "        \"question\": question,\n",
        "        \"answer_research\": answer_research,\n",
        "        \"answer_marketing\": answer_marketing,\n",
        "        \"metrics_research\": {\n",
        "            \"bleu\": bleu_research,\n",
        "            \"rouge\": rouge_research,\n",
        "            \"cosine_similarity\": cosine_research,\n",
        "            \"length_difference\": len_diff_research\n",
        "        },\n",
        "        \"metrics_marketing\": {\n",
        "            \"bleu\": bleu_marketing,\n",
        "            \"rouge\": rouge_marketing,\n",
        "            \"cosine_similarity\": cosine_marketing,\n",
        "            \"length_difference\": len_diff_marketing\n",
        "        }\n",
        "    })\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Display total elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Test questions answered and evaluated in {elapsed_time:.2f} seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'llm': {'family': <ModelType.Mistral: 'mistral'>,\n",
              "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
              "  'quantization': {'load_in_4bit': True},\n",
              "  'tokenizer': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
              "  'temperature': 0.6,\n",
              "  'top_p': 0.95,\n",
              "  'max_new_tokens': 1000,\n",
              "  'repetition_penalty': 1.2},\n",
              " 'top_k': 4,\n",
              " 'engineering_template_path': PosixPath('/home/ubuntu/w267-final-project-rag-pipeline/notebook/../templates/engineering_template.txt'),\n",
              " 'marketing_template_path': PosixPath('/home/ubuntu/w267-final-project-rag-pipeline/notebook/../templates/marketing_template.txt'),\n",
              " 'vectorstore': {'embedding_model': 'multi-qa-mpnet-base-dot-v1',\n",
              "  'collection_name': 'myrag',\n",
              "  'in_memory': True,\n",
              "  'force_recreate': True,\n",
              "  'chunking_strategy': {'chunk_size': 128,\n",
              "   'chunk_overlap': 0,\n",
              "   'splitter_type': 'RecursiveCharacterTextSplitter'}}}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_system.get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM-Driven Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# These should already be loaded from the dotenv file\n",
        "# If not, set them here:\n",
        "\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR LANGSMITH API KEY\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 {'question': 'What defines a large language model in the context of natural language processing tasks?', 'gold_answer_research': 'A large language model in the context of natural language processing tasks is characterized by its ability to achieve general-purpose language generation and other NLP tasks through self-supervised and semi-supervised training on large datasets. These models typically utilize feedforward neural networks and transformers, surpassing earlier models like recurrent neural networks and word n-gram language models. They are often pre-trained on vast amounts of text data from the internet and can be fine-tuned for specific downstream tasks such as summarization, machine reading comprehension, and natural language to SQL translation.', 'gold_answer_marketing': 'A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.'}\n",
            "1 {'question': 'How do large language models like GPT-3 become capable of text generation?', 'gold_answer_research': \"Large language models like GPT-3 become capable of text generation through a process of pre-training on vast amounts of unlabelled text data, where they learn statistical relationships and patterns in language. This pre-training process involves training the model to predict the next word in a sequence, allowing it to understand and generate human-like text. Additionally, fine-tuning on specific tasks or datasets further enhances the model's ability to generate text that is contextually relevant and coherent. By combining these pre-training and fine-tuning techniques, GPT-3 can generate novel and diverse text outputs across a wide range of natural language processing tasks.\", 'gold_answer_marketing': 'Large language models like GPT-3 become capable of text generation by being pre-trained on large data sets of unlabelled text, learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. This allows them to generate novel human-like content based on the patterns and relationships they have learned.'}\n",
            "2 {'question': 'What are some of the architectures used in building artificial neural networks for LLMs?', 'gold_answer_research': 'Some common architectures used in building artificial neural networks for LLMs include decoder-only transformer-based architectures, recurrent neural network variants, and Mamba (a state space model). MRKL (Modular Reasoning, Knowledge and Language) is another neuro-symbolic architecture that utilizes neural and symbolic expert modules. Additionally, the unified architecture proposed by Ronan Collobert and Jason Weston combines deep neural networks with multitask learning for natural language processing.', 'gold_answer_marketing': 'The architectures used in building artificial neural networks for LLMs include decoder-only transformer-based architecture, recurrent neural network variants, and Mamba (a state space model).'}\n",
            "3 {'question': 'Can you name some notable large language models and their respective creators or companies?', 'gold_answer_research': 'Some notable large language models include Mistral 7B developed by a team of engineers including Albert Q. Jiang and Guillaume Lample, as well as Chinchilla developed by the research team at DeepMind. Additionally, The Pile dataset was created by Leo Gao, Stella Biderman, and others, while the Realm model was developed by Kelvin Guu and his team. These models have been recognized for their superior performance and efficiency in language generation and natural language processing tasks.', 'gold_answer_marketing': 'Some notable large language models include Mistral 7B by a team of researchers, Chinchilla by DeepMind, and GPT-3 by OpenAI.'}\n",
            "7 {'question': \"What licensing terms are associated with source-available models like Mistral AI's language models?\", 'gold_answer_research': \"Source-available models like Mistral AI's language models, including Mistral 7B, are released under the Apache 2.0 license. This license allows for more permissive use and modification of the models, providing flexibility for users to adapt the models to their specific needs. Additionally, the release of Mistral 7B includes a reference implementation for easy deployment on various cloud platforms and integration with tools like the vLLM inference server and Hugging Face for streamlined usage. This licensing approach aims to facilitate widespread adoption and adaptation of the models for different tasks and applications.\", 'gold_answer_marketing': \"Source-available models like Mistral AI's language models are released under the Apache 2.0 license.\"}\n",
            "8 {'question': 'What are the main applications of language models?', 'gold_answer_research': 'Language models have a wide range of applications, including speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval. Large language models, which are currently the most advanced form, are created using larger datasets, often sourced from the public internet, and feedforward neural networks. These models can be used for tasks such as text generation, question-answering, creative writing, dialogue, and classification.', 'gold_answer_marketing': 'The main applications of language models include speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.'}\n",
            "9 {'question': 'Who proposed the first significant statistical language model?', 'gold_answer_research': \"The first significant statistical language model was proposed in 1980. This model was a probabilistic model of natural language. IBM conducted 'Shannon-style' experiments during the 1980s to identify potential sources for language modeling improvement by observing and analyzing the performance of human subjects in predicting or correcting text.\", 'gold_answer_marketing': 'The first significant statistical language model was proposed in 1980.'}\n",
            "11 {'question': 'Which components have allowed large language models to surpass their predecessors?', 'gold_answer_research': 'Large language models have surpassed their predecessors due to their use of larger datasets, feedforward neural networks, and transformers. These components have enabled these models to outperform recurrent neural network-based models and traditional statistical models like word n-gram language models. The combination of these elements has allowed for the creation of more adaptable, efficient, and high-performing language models that can be applied to a wide range of tasks in the field of natural language processing.', 'gold_answer_marketing': 'The combination of larger datasets, feedforward neural networks, and transformers.'}\n",
            "12 {'question': 'What is a common strategy used by language models to address the curse of dimensionality?', 'gold_answer_research': 'One common strategy used by language models to address the curse of dimensionality is the use of continuous space embeddings produced by recurrent neural network-based models. These embeddings represent words as non-linear combinations of weights, which helps alleviate the data sparsity issue caused by the exponential increase in possible word sequences with the size of the vocabulary. This approach allows language models to efficiently capture the relationships between words in a lower-dimensional space, reducing the impact of the curse of dimensionality.', 'gold_answer_marketing': 'Continuous space embeddings produced in recurrent neural network-based language models are a common strategy used to address the curse of dimensionality.'}\n",
            "13 {'question': 'Why might large language models not be considered plausible cognitive models?', 'gold_answer_research': 'Large language models may not be considered plausible cognitive models because they sometimes learn patterns that humans do not learn, and fail to learn patterns that humans typically do learn. This discrepancy in learning capabilities between large language models, particularly recurrent neural networks, and humans raises questions about the true cognitive abilities of these models. Additionally, the potential for misuse of these models, such as generating misinformation or abusive content, highlights the need for caution when deploying them in various applications. Furthermore, the pre-defined context window size in LLMs and the potential for hallucination of data also contribute to the concerns about their cognitive modeling capabilities.', 'gold_answer_marketing': 'Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically learn, making them not considered plausible cognitive models.'}\n",
            "16 {'question': 'What is the purpose of the constitution in training AI systems?', 'gold_answer_research': 'The purpose of the constitution in training AI systems, such as in the Constitutional AI approach, is to provide a set of guiding principles for the AI model to follow during the supervised learning phase. This constitution helps the model generate responses to prompts, self-critique these responses, and revise them accordingly. Additionally, the constitution serves as a framework for the reinforcement learning phase, where the AI evaluates responses based on these principles, ultimately training the AI to be harmless and helpful without extensive human feedback.', 'gold_answer_marketing': 'The purpose of the constitution in training AI systems is to guide the model to generate responses that align with a set of guiding principles, ensuring that the AI is harmless and helpful without extensive human feedback.'}\n",
            "17 {'question': 'What is the meaning of the term alignment tax in the context of AI development?', 'gold_answer_research': 'In the context of AI development, the term alignment tax refers to the additional cost incurred in ensuring that an AI system is aligned with human intent. This cost arises from the need to mitigate performance degradations introduced by fine-tuning the model to achieve alignment. A high alignment tax could deter the adoption of alignment techniques, as it may impact usability and performance. Therefore, there is a growing need for alignment techniques that have a low alignment tax to encourage the development of highly capable AI systems aligned with human intent.', 'gold_answer_marketing': \"The term 'alignment tax' in the context of AI development refers to the additional cost incurred to ensure that an AI system is aligned with human intent and ethical considerations. This cost can arise from mitigating performance degradations introduced by fine-tuning the AI model.\"}\n",
            "18 {'question': 'How does the release of successive models in a language model series typically improve functionality?', 'gold_answer_research': 'The release of successive models in a language model series typically improves functionality by incorporating larger datasets, allowing for more comprehensive training and better understanding of syntax, semantics, and ontology in human language corpora. Additionally, newer models often address inaccuracies and biases present in earlier versions, leading to more accurate and reliable results. Furthermore, advancements in technology and research techniques contribute to the development of more efficient and high-performing language models, enabling them to be used in a wider range of tasks effectively.', 'gold_answer_marketing': 'The release of successive models in a language model series typically improves functionality by enhancing adaptability, performance, and efficiency through advancements in training methods and model architecture.'}\n",
            "19 {'question': 'What is the significant enhancement in Claude 2.1 compared to its previous version?', 'gold_answer_research': 'The significant enhancement in Claude 2.1 compared to its previous version Claude 2 is the expanded context window, which has been increased from 100,000 tokens to 200,000 tokens. This allows Claude 2.1 to process and analyze larger amounts of text, enabling it to provide more comprehensive summaries and assistance with tasks. Additionally, Claude 2.1 has improved its performance in handling complex queries and requests, showcasing advancements in its capabilities for natural language processing and understanding.', 'gold_answer_marketing': 'The significant enhancement in Claude 2.1 compared to its previous version is the expansion of the context window from 9,000 tokens to 100,000 tokens, as well as the ability to upload PDFs and other documents for reading, summarizing, and task assistance.'}\n",
            "20 {'question': 'In what way can a language model demonstrate meta-cognitive reasoning capabilities?', 'gold_answer_research': 'A language model can demonstrate meta-cognitive reasoning capabilities by realizing it is being artificially tested during needle-in-a-haystack evaluations, as shown by Claude 3. This ability allows the model to understand the context of the evaluation and adjust its responses accordingly. Additionally, fine-tuning a pre-trained model to answer questions without external context, like in the study by Roberts et al. (2020), can also showcase the meta-cognitive reasoning abilities of the language model. This fine-tuning process forces the model to rely on internalized knowledge acquired during pre-training, demonstrating its capacity for self-awareness and adaptive reasoning.', 'gold_answer_marketing': 'A language model can demonstrate meta-cognitive reasoning capabilities by realizing it is being artificially tested during evaluations such as needle in a haystack tasks.'}\n",
            "22 {'question': \"How can a language model's ability to analyze images expand its range of applications?\", 'gold_answer_research': \"One way a language model's ability to analyze images can expand its range of applications is by enabling it to perform tasks that require both textual and visual information, such as image captioning or visual question answering. By incorporating image features into the model's input, it can generate more contextually relevant and accurate responses. This integration of image analysis can also enhance the model's performance in tasks like content generation, recommendation systems, and sentiment analysis, where visual cues play a significant role in understanding and interpreting the data. Additionally, combining language and image processing capabilities can lead to more sophisticated and versatile AI systems that can handle a wider range of real-world applications effectively.\", 'gold_answer_marketing': 'By incorporating image analysis capabilities, a language model can be used for tasks such as image captioning, visual question answering, and text-to-image generation, expanding its range of applications beyond just text-based tasks.'}\n",
            "23 {'question': 'What are some ethical considerations that come into play when refining the performance of language models?', 'gold_answer_research': 'Some ethical considerations when refining the performance of language models include preventing biased outputs, ensuring privacy of data, avoiding generation of misinformation, and not promoting harmful activities. It is important to evaluate model outputs based on criteria such as not generating abusive or offensive language, not providing bad advice or promoting illegal activities, and not causing harm to individuals or the environment. Trade-offs may need to be made between these criteria depending on the task at hand. Additionally, alignment techniques should be used as part of a broader safety ecosystem to address safety issues associated with large language models.', 'gold_answer_marketing': 'Ethical considerations when refining language models include avoiding biased outputs, protecting privacy, preventing misinformation, and ensuring outputs do not cause harm or promote illegal activity. Trade-offs may need to be made in evaluating model outputs based on these criteria.'}\n",
            "24 {'question': 'Who developed the language model family known as Chinchilla?', 'gold_answer_research': \"The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\", 'gold_answer_marketing': 'The research team at DeepMind developed the language model family known as Chinchilla.'}\n",
            "25 {'question': 'What benchmark did Chinchilla achieve an average accuracy of 67.5% on?', 'gold_answer_research': 'Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).', 'gold_answer_marketing': 'Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).'}\n",
            "27 {'question': 'What is the significance of the accuracy percentage achieved by Chinchilla on the MMLU benchmark?', 'gold_answer_research': \"The significance of the accuracy percentage achieved by Chinchilla on the MMLU benchmark is that it is 67.5%, which is 7% higher than Gopher's performance. Chinchilla's performance on the benchmark is a positive indicator of its effectiveness in developing an effective training paradigm for large autoregressive language models with limited compute resources. The higher accuracy suggests that Chinchilla may have better performance on downstream tasks compared to Gopher. Additionally, Chinchilla was still in the testing phase as of January 12, 2023, indicating ongoing development and potential for further improvement in accuracy.\", 'gold_answer_marketing': \"The significance of the accuracy percentage achieved by Chinchilla on the MMLU benchmark is 67.5%, which is 7% higher than Gopher's performance.\"}\n",
            "28 {'question': 'Why is Chinchilla considered more efficient in terms of computing power for inference and fine-tuning?', 'gold_answer_research': \"Chinchilla is considered more efficient in terms of computing power for inference and fine-tuning because it has 70B parameters and four times as much data, allowing for high performance with limited compute resources. Additionally, Chinchilla's training paradigm recommends doubling the number of training tokens for every model size doubling, leading to better results on downstream tasks. Moreover, Chinchilla's smooth and differentiable model architecture contributes to its efficiency in training and inference processes.\", 'gold_answer_marketing': 'Chinchilla is considered more efficient in terms of computing power for inference and fine-tuning because it has 70B parameters and four times as much data, which allows for high performance with limited compute resources.'}\n",
            "30 {'question': 'What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?', 'gold_answer_research': 'The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.', 'gold_answer_marketing': 'The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.'}\n",
            "33 {'question': 'What assumptions must be met in order for the reparameterization of reward functions to be applied within the context of Plackett-Luce and Bradley-Terry models?', 'gold_answer_research': 'In order for the reparameterization of reward functions to be applied within the context of Plackett-Luce and Bradley-Terry models, the assumptions must include the existence of a reward function r(x, y) that induces an optimal model \\\\u03c0r(y | x) as specified by Equation 4. Additionally, the reward function must be from an equivalence class of reward functions that differ only in an input-specific component. It is also important to ensure that the value distribution, representing human utility, is affected by input-specific changes to maximize preference. Finally, the assumptions should allow for the cancellation of the normalization constant Z(x) in the derived expressions.', 'gold_answer_marketing': 'The assumptions that must be met for the reparameterization of reward functions to be applied within the context of Plackett-Luce and Bradley-Terry models are that the reward classes consistent with these models can be represented by the reparameterization formula r(x, y) = \\\\u03b2 log(\\\\u03c0(y|x) / \\\\u03c0ref(y|x)).'}\n",
            "34 {'question': 'What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?', 'gold_answer_research': 'One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.', 'gold_answer_marketing': 'Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.'}\n",
            "35 {'question': 'How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?', 'gold_answer_research': \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\", 'gold_answer_marketing': 'The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.'}\n",
            "36 {'question': 'What approaches or methods are suggested for improving the alignment of language models with human preferences?', 'gold_answer_research': 'One approach suggested for improving the alignment of language models with human preferences is Direct Preference Optimization (DPO), which optimizes language models to adhere to human preferences without explicit reward modeling or reinforcement learning. This algorithm implicitly optimizes the same objective as existing RLHF algorithms, making it simple to implement and straightforward to train. Additionally, incorporating pretraining data into RLHF fine-tuning can help mitigate the alignment tax and improve the alignment of language models with human preferences.', 'gold_answer_marketing': 'Direct Preference Optimization (DPO) is a suggested approach for improving the alignment of language models with human preferences. It optimizes the same objective as existing reinforcement learning with a KL-divergence constraint but is simpler to implement and train.'}\n",
            "38 {'question': 'What methods have been explored to improve the alignment of language models with user preferences or desired outputs?', 'gold_answer_research': 'Several methods have been explored to improve the alignment of language models with user preferences or desired outputs. These include fine-tuning language models from human preferences, using reinforcement learning from human feedback, and minimizing f-divergence to align language models with preferences. Additionally, incorporating pretraining data into reinforcement learning from human feedback (RLHF) has shown promise in mitigating alignment issues. It is important to consider factors such as the underlying model, training data, fine-tuning data, and alignment methods when working towards better alignment with user intentions.', 'gold_answer_marketing': 'Methods explored to improve the alignment of language models with user preferences or desired outputs include RLHF, DPO, and f-divergence minimization.'}\n",
            "39 {'question': 'How does the uniqueness of a reparameterized reward function within an equivalence class impact the selection of optimal policies in constrained reinforcement learning problems?', 'gold_answer_research': 'The uniqueness of a reparameterized reward function within an equivalence class ensures that different reward functions from the same class will induce the same optimal policy in constrained reinforcement learning problems. This means that despite the under-specification issue, the optimal policy remains consistent across equivalent reward functions, allowing for more stable and reliable policy learning. By reparameterizing the reward function, the selection of optimal policies becomes more straightforward and consistent, leading to better convergence and performance in constrained reinforcement learning scenarios.', 'gold_answer_marketing': 'The uniqueness of a reparameterized reward function within an equivalence class ensures that different reward functions from the same class will lead to the selection of the same optimal policy in constrained reinforcement learning problems.'}\n",
            "41 {'question': \"Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\", 'gold_answer_research': \"When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\", 'gold_answer_marketing': 'Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.'}\n",
            "43 {'question': 'How does the evaluation process determine the level of alignment in the models being tested?', 'gold_answer_research': 'The evaluation process determines the level of alignment in the models by comparing the system-level judgments made by the language model (GPT-4) with those made by human annotators, using metrics such as Kendall Tau and Spearman rank correlation. At the example level, the agreement between the model and human annotators is measured using Fleiss \\\\u03ba. These metrics provide insights into the alignment of the language model with human intentions and indicate the reliability of model-based evaluation as an alternative to human evaluation.', 'gold_answer_marketing': 'The evaluation process determines the level of alignment in the models being tested by comparing system-level judgments by the language model and human annotators, as well as examining agreement at the example level.'}\n",
            "44 {'question': 'What approaches can be used to improve the performance of an AI model on various datasets, and how do they compare in terms of effectiveness?', 'gold_answer_research': \"To improve the performance of an AI model on various datasets, you can consider combining your method with ways to filter the pretraining data or training the initial pretrained models using human data. Additionally, you could explore methods that improve models' truthfulness, such as WebGPT. Comparing the effectiveness of these approaches, it may be beneficial to experiment with different adaptation approaches in the low-data regime and evaluate their performance on tasks like MNLI-n. Fine-tuning models using different training datasets and varying the number of fine-tuning steps can also impact performance on public NLP datasets.\", 'gold_answer_marketing': 'Approaches such as fine-tuning with different datasets, filtering pretraining data, and combining methods to improve model truthfulness can be used to enhance the performance of an AI model on various datasets. However, the effectiveness of these approaches may vary depending on the specific task and dataset.'}\n",
            "46 {'question': 'What are some common strategies for addressing the ethical and social risks associated with the deployment of language models, according to recent research findings?', 'gold_answer_research': 'Recent research findings suggest that common strategies for addressing the ethical and social risks associated with the deployment of language models include implementing alignment techniques to ensure that the models are helpful, honest, and harmless. These alignment techniques involve fine-tuning approaches such as reinforcement learning from human feedback to train the models to follow a broad class of written instructions. Additionally, it is important to consider the potential harms of language models in real-world applications and to evaluate how their outputs are used, especially in safety-critical situations. Furthermore, regulating access to large language models and incorporating diverse values-targeted datasets can help mitigate the risks of biased outputs, private data leaks, and misinformation generation.', 'gold_answer_marketing': 'Recent research findings suggest that common strategies for addressing ethical and social risks associated with the deployment of language models include mitigating biases, protecting private data, preventing the generation of misinformation, and ensuring alignment with human intentions to be helpful, honest, and harmless. These strategies aim to minimize the potential harms that language models may cause in various domains, such as medical diagnoses, employment, housing, and law enforcement.'}\n",
            "47 {'question': 'What categories are the listed companies classified into, and what are some examples of tasks that can be performed in relation to text extraction, generation, rewriting, and chat services based on the given content?', 'gold_answer_research': \"The listed companies are classified as follows: Apple falls under the category of Technology, Facebook is categorized as Social Media, and Fedex's category is not specified. Some examples of tasks that can be performed in relation to text extraction include salient span masking, named entity identification, and adding an empty null document. For text generation, tasks can include natural language generation, speech recognition, and machine translation. For rewriting, tasks can involve summarization, paraphrasing, and producing rap lyrics based on a given article. Chat services can include customer assistance, complaint resolution, and information retrieval.\", 'gold_answer_marketing': 'Apple is classified into the Technology category, Facebook is classified into the Social Media category, and Fedex is not classified. Tasks that can be performed in relation to text extraction include salient span masking and adding an empty null document. Tasks related to text generation include speech recognition, machine translation, and natural language generation. For rewriting, tasks can include summarization and rewriting rap lyrics. Chat services can involve customer assistance and complaints.'}\n",
            "48 {'question': 'What criteria were used to select labelers to ensure they can effectively detect and rate sensitive content?', 'gold_answer_research': 'The criteria used to select labelers included agreement on sensitive speech flagging, performance on a screening test measuring the ability to detect and respond to sensitive content, comparison of outputs, and a demonstration score of 6/7. Labelers were chosen subjectively based on these criteria, with soft cutoffs at 75% agreement on sensitive speech flagging and comparisons. Additionally, labelers were trained on the project and provided with detailed instructions for each task to ensure consistency and accuracy in their ratings.', 'gold_answer_marketing': 'The criteria used to select labelers included agreement on sensitive speech flagging, performance on a screening test measuring sensitivity to different demographic groups, and ability to identify potentially harmful outputs. Soft cutoffs were set at 75% agreement on sensitive speech flagging and comparisons, and a 6/7 demonstration score.'}\n",
            "50 {'question': 'How do the evaluation metrics used in a model contribute to the overall quality and reliability of the generated responses?', 'gold_answer_research': \"The evaluation metrics used in a model, such as percentage of true responses and informative responses, play a crucial role in assessing the quality and reliability of the generated responses. By comparing the model's outputs to ground-truth summaries and baselines, these metrics help determine the performance of the model in generating accurate and relevant information. Additionally, subjective evaluations by labelers on a Likert scale further contribute to understanding the overall quality of the responses. Future work should focus on mitigating subjective preferences and biases in evaluation systems to ensure more reliable and consistent results.\", 'gold_answer_marketing': \"The evaluation metrics used in a model help determine how often the model's outputs are preferred to a baseline policy, as well as the overall quality of each response. This contributes to assessing the performance and reliability of the generated responses in comparison to other models and benchmarks.\"}\n",
            "51 {'question': 'How do the datasets encourage the application of common-sense reasoning and entailment recognition in natural language processing tasks?', 'gold_answer_research': 'The datasets mentioned in the context focus on evaluating natural language models on tasks that require common-sense reasoning and entailment recognition. By incorporating prompts that require understanding of context and relevant information, the models are challenged to go beyond surface-level processing. This encourages the development of models that can infer implicit information, draw logical conclusions, and make accurate predictions based on contextual clues. Through these datasets, researchers aim to improve the ability of natural language processing systems to perform more complex and nuanced tasks that mimic human-like reasoning.', 'gold_answer_marketing': 'The datasets encourage common-sense reasoning and entailment recognition by evaluating model performance on tasks like question answering, reading comprehension, and summarization, which require understanding context and relationships between different pieces of information in natural language.'}\n",
            "52 {'question': 'What metrics are used to evaluate the quality of translations and summaries in the dataset examples provided?', 'gold_answer_research': 'Translations in the dataset examples are evaluated using the BLEU metric, while summaries are judged based on their ROUGE-L scores with respect to a set of reference summaries. Additionally, the evaluation metric for the summaries is the f1 score from the sample to the target completion. These metrics help assess the quality and accuracy of the translations and summaries in the datasets.', 'gold_answer_marketing': 'Translations are evaluated using the BLEU metric, while summaries are judged via their ROUGE-L scores with respect to a set of reference summaries. The evaluation metric for translations is the f1 score from the sample to the target completion.'}\n",
            "53 {'question': 'What are the implications of adding updates on the pretraining data during the fine-tuning phase of model development based on the observed performance of the models?', 'gold_answer_research': \"Adding updates on the pretraining data during the fine-tuning phase of model development can help mitigate performance regressions observed in the models. This approach allows for reducing performance regressions on specific datasets without compromising labeler preference scores. Additionally, it enables the models to generalize to the preferences of 'held-out' labelers that did not provide training data, improving overall model performance. This method can be particularly useful for addressing performance issues and enhancing model robustness during the fine-tuning process.\", 'gold_answer_marketing': 'Adding updates on the pretraining data during the fine-tuning phase can help mitigate performance regressions on certain tasks without compromising labeler preference scores. This approach can improve the generalization of models to new data and tasks.'}\n",
            "54 {'question': 'How do the capabilities of different AI models compare in terms of following explicit constraints and minimizing hallucinations, as evidenced by metadata ratings?', 'gold_answer_research': \"Based on the metadata ratings, it appears that the capabilities of different AI models vary in terms of following explicit constraints and minimizing hallucinations. The results show that extending LLaMA 7B and 13B models with a longer context window size leads to significant reductions in perplexity, indicating improved performance in minimizing hallucinations. However, it is important to note that the specific term 'AI hallucination' may anthropomorphize computers and the concept of hallucinations in AI is associated with unjustified responses or beliefs. Further research and analysis are needed to fully understand and compare the capabilities of different AI models in this context.\", 'gold_answer_marketing': 'The capabilities of different AI models in terms of following explicit constraints and minimizing hallucinations can be compared based on metadata ratings.'}\n",
            "55 {'question': 'How do human likert scores compare when evaluating PPO with different initial models based on the pretraining fraction?', 'gold_answer_research': 'Based on the data provided, it appears that human likert scores for PPO with different initial models do not show significant sensitivity to the pretraining fraction choice, as indicated by the performance seeming not sensitive to the particular choice of 0%, 0.1%, or 0.5% pretraining data mix. Additionally, the likert scores seem to be consistent across different pretraining fractions. Further investigation into the impact of pretraining fraction on likert scores may be necessary to determine if there are any subtle differences in performance.', 'gold_answer_marketing': 'Human likert scores for PPO with different initial models show that there is not a significant difference in performance based on the pretraining fraction used.'}\n",
            "59 {'question': 'What methodologies can be employed to test the reliability and accuracy of AI-generated responses to prompts with varying levels of obscurity?', 'gold_answer_research': \"To test the reliability and accuracy of AI-generated responses to prompts with varying levels of obscurity, methodologies such as measuring truthfulness by comparing the model's actual output to its believed correct output can be employed. Additionally, sensitivity speech flagging can be used to identify and label prompts or completions that may elicit strong negative feelings. Agreement on rankings of prompts submitted to the API can also help assess the model's performance on novel questions. Finally, human evaluation through random sampling of prompts and generated outputs can provide valuable insights into the quality of model responses.\", 'gold_answer_marketing': 'Use automated and human evaluations, measure truthfulness, analyze model responses against user intentions, and conduct closed-book QA testing with various prompts.'}\n",
            "60 {'question': 'How does the subjective nature of human preferences influence the evaluation of chatbot task performance, and what are potential methods for addressing this challenge?', 'gold_answer_research': 'The subjective nature of human preferences can introduce variability in the evaluation of chatbot task performance, as different human annotators may have differing opinions on what constitutes a preferred response. This can lead to disagreements among evaluators, as seen in the case of comparing generations from different chatbot systems. The text suggests that drawing from disciplines such as Human-Computer Interaction and Psychology may offer insights into mitigating these challenges posed by subjective preferences. Additionally, the proposed Direct Preference Optimization (DPO) algorithm aims to optimize language models based on human preferences without the need for explicit reward modeling or reinforcement learning, offering a potential method to address this challenge.', 'gold_answer_marketing': 'The subjective nature of human preferences can impact the evaluation of chatbot task performance by introducing disagreements among evaluators. Potential methods for addressing this challenge mentioned in the text include investigating approaches from disciplines like Human-Computer Interaction and Psychology to mitigate subjective preferences.'}\n",
            "61 {'question': 'What are some potential societal impacts of the widespread use of the QLORA finetuning method for Language Learning Models (LLMs)?', 'gold_answer_research': 'The authors suggest that the QLORA finetuning method could help close the resource gap between large corporations and small teams, making state-of-the-art NLP technology more accessible. They believe that QLORA could lead to more independent analysis and auditing of LLMs, which could have a positive impact by ensuring models align with societal values and consensus. The method may also enable further investigations into the tradeoffs between simple cross-entropy loss and reinforcement learning from human feedback, potentially leading to more efficient training methods for LLMs.', 'gold_answer_marketing': 'The potential societal impacts of the widespread use of the QLORA finetuning method for LLMs include making state-of-the-art NLP technology more accessible to researchers with limited resources, closing the resource gap between large corporations and small teams, and enabling independent analysis of LLMs.'}\n",
            "62 {'question': 'What methods or approaches are being investigated or utilized to optimize or adapt machine learning models?', 'gold_answer_research': 'Some of the methods or approaches being investigated or utilized to optimize or adapt machine learning models according to the listed references include large-scale pre-training on general domain data followed by adaptation to specific tasks or domains, adding adapter layers, optimizing certain forms of adaptations, and exploring the use of large language models for reference-free text quality evaluation. Additionally, strategies like fine-tuning continuous prompts for generation and learning overparameterized neural networks via stochastic gradient descent on structured data are being explored for model adaptation. These approaches aim to make model adaptation more parameter- and compute-efficient.', 'gold_answer_marketing': 'The methods or approaches being investigated or utilized include large-scale pre-training on general domain data and adaptation to particular tasks or domains, adding adapter layers, and optimizing some forms of adaptations.'}\n",
            "63 {'question': 'What are the advantages of applying LoRA to transformer models in terms of computational efficiency during training and deployment?', 'gold_answer_research': 'Applying LoRA to transformer models offers several advantages in terms of computational efficiency. By using LoRA, a pre-trained model can be shared and used for multiple tasks, reducing the storage requirements and task-switching overhead significantly. Additionally, LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers, as gradients do not need to be calculated for all parameters. This can lead to faster model training and deployment times, improving overall computational efficiency.', 'gold_answer_marketing': 'Applying LoRA to transformer models can reduce the number of trainable parameters, making training more efficient and lowering the hardware barrier to entry. It also allows for quick task-switching during deployment by sharing the majority of model parameters.'}\n",
            "64 {'question': 'How does the training cost impact the evaluation of performance metrics across the different methods of adaptation for large Transformer models like GPT-3?', 'gold_answer_research': 'The training cost has a significant impact on the evaluation of performance metrics across different adaptation methods for large Transformer models like GPT-3. The cost of training our 175B SFT model is 4.9 petaflops/s-days, while training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3. This difference in training cost affects the efficiency and effectiveness of adaptation methods, as it determines the resources required for training and evaluating the models. Additionally, the training cost influences the scalability and feasibility of implementing these adaptation methods on a larger scale. It is essential to consider the trade-offs between training cost and performance metrics when evaluating and selecting adaptation methods for large Transformer models like GPT-3.', 'gold_answer_marketing': 'The training cost has a significant impact on the evaluation of performance metrics for large Transformer models like GPT-3. By achieving the largest reduction of trainable parameters, we can adapt the model while maintaining task performances, but the training cost for different adaptation methods varies. This cost can affect the overall efficiency and effectiveness of the adaptation process, as seen in the empirical studies conducted.'}\n",
            "65 {'question': 'What are the advantages of using low-rank adaptations during the fine-tuning process of pre-trained transformer models?', 'gold_answer_research': 'Using low-rank adaptations during the fine-tuning process of pre-trained transformer models can potentially amplify important features for specific downstream tasks that were not emphasized in the general pre-training model. This approach allows for a reduction in the number of trainable parameters, leading to a more efficient use of computational resources without sacrificing task performance. Additionally, low-rank adaptations can help in identifying the trade-off between performance and the number of trainable parameters, aiding in better optimization strategies for adaptation in low-data regimes.', 'gold_answer_marketing': 'Low-rank adaptations potentially amplify important features for specific downstream tasks that were not emphasized in the general pre-training model.'}\n",
            "67 {'question': 'What are the implications of fine-tuning large language models on datasets of varying sizes in contrast to using few-shot learning techniques?', 'gold_answer_research': 'Fine-tuning large language models on datasets of varying sizes compared to using few-shot learning techniques can significantly improve model performance across different tasks. Fine-tuning with DPO has shown to exceed PPO-based RLHF in controlling sentiment of generations and matching or improving response quality in summarization and single-turn dialogue. Additionally, fine-tuning is simpler to implement and train, making it a more practical approach for performance-sensitive applications with curated training datasets. The ability to fine-tune models for specific tasks on larger datasets can lead to substantial performance improvements compared to few-shot learning methods.', 'gold_answer_marketing': 'Fine-tuning with DPO surpasses PPO-based RLHF in sentiment control across generations and improves response quality in summarization and single-turn dialogue while being simpler to implement and train.'}\n",
            "69 {'question': 'How does the RoFormer model perform on semantic text matching tasks, such as those found in the CAIL2019-SCM dataset?', 'gold_answer_research': 'Based on the results from the experiments conducted on the CAIL2019-SCM dataset, RoFormer consistently outperforms its alternatives, including BERT and WoBERT, in dealing with long texts and semantic text matching tasks. When compared with BERT and WoBERT on the same pre-training data with short text cut-offs, RoFormer shows comparable performance to WoBERT and slightly better performance than BERT. However, when the maximum input text length is increased to 1024, RoFormer significantly outperforms WoBERT by an absolute improvement of 1.5%, showcasing its effectiveness in semantic text matching tasks.', 'gold_answer_marketing': 'The RoFormer model outperforms other pre-trained models on semantic text matching tasks, such as those in the CAIL2019-SCM dataset, especially when the maximum input text length is increased to 1024.'}\n",
            "73 {'question': 'What is the purpose of using a regularization term (like the epsilon multiplied by the identity matrix) in the linear regression model described in the code?', 'gold_answer_research': 'The purpose of using a regularization term (epsilon multiplied by the identity matrix) in the linear regression model described in the code is to prevent overfitting by adding a penalty term to the loss function. This regularization term helps to control the complexity of the model by discouraging large coefficients, leading to a more stable and generalizable solution. By incorporating this regularization, the model can better handle noise and outliers in the data, ultimately improving its performance on unseen data points.', 'gold_answer_marketing': 'The purpose of using a regularization term in the linear regression model is to mitigate or resolve catastrophic extrapolation error, making the extrapolated values comparable to those within the original range.'}\n",
            "74 {'question': 'What novel attention mechanisms does the language model discussed employ to improve performance and efficiency in processing long sequences?', 'gold_answer_research': 'The language model discussed employs sliding window attention to improve performance and efficiency in processing long sequences. This mechanism helps reduce the number of operations and memory usage in comparison to vanilla attention, leading to lower latency and increased throughput during inference. Additionally, the model incorporates linear attention as an alternative mechanism to avoid quadratic computation costs associated with input sequence length, further enhancing its ability to handle longer sequences effectively.', 'gold_answer_marketing': 'The language model discussed employs sliding window attention and linear attention mechanisms to improve performance and efficiency in processing long sequences.'}\n",
            "75 {'question': 'How does the performance of the Mistral 7B model in mathematical benchmarks compare to that of larger pretrained models?', 'gold_answer_research': 'Based on the provided information, the Mistral 7B model outperforms the Llama 34B model in mathematics and code generation benchmarks. Additionally, Mistral 7B approaches the performance of the larger Code-Llama 7B model in coding tasks. This indicates that Mistral 7B demonstrates high performance in mathematical benchmarks compared to larger pretrained models.', 'gold_answer_marketing': 'The Mistral 7B model outperforms larger pretrained models in mathematics benchmarks.'}\n",
            "76 {'question': 'What are the key differences between HALO and non-HALO loss functions, and what impact do these differences have on language model alignment?', 'gold_answer_research': 'HALO loss functions, such as DPO and KTO, explicitly model human biases like loss aversion, while non-HALO loss functions, like CSFT and SLiC, do not. These differences can impact language model alignment by influencing how well the model captures human preferences and perceptions in text generation tasks. HALOs may be more effective in aligning language models to human expectations and behaviors, potentially leading to more accurate and natural language generation outputs. Further research is needed to fully understand the implications of using HALO versus non-HALO loss functions in language model alignment.', 'gold_answer_marketing': 'HALO loss functions implicitly model human biases, such as loss aversion, while non-HALO loss functions do not. These biases can impact language model alignment by influencing the generation of text outputs that better match human perceptions of language quality.'}\n",
            "78 {'question': 'What implications does the lemma from Rafailov et al. (2023) have on the relationship between optimal policies and reward functions within the same equivalence class?', 'gold_answer_research': 'The lemma from Rafailov et al. (2023) implies that reward functions within the same equivalence class will induce the same optimal policy under the constrained RL problem. This means that even though the reward functions may differ by an input-specific component, they will lead to the same decision-making strategy. This has important implications for understanding the relationship between reward functions and optimal policies, showing that certain variations in the reward functions do not impact the final decision-making process. Understanding this relationship can help in designing more efficient and effective RL algorithms.', 'gold_answer_marketing': 'The lemma from Rafailov et al. (2023) implies that optimal policies and reward functions within the same equivalence class induce the same optimal policy under the constrained RL problem.'}\n",
            "80 {'question': 'What is the significance of the term open-domain in the context of question answering?', 'gold_answer_research': \"Answer: The term 'open-domain' in the context of question answering refers to the lack of specific context or relevant information provided to the model when answering a factual question. This means that the model must rely solely on the question itself without additional articles or background knowledge. In open-domain question answering, the model's task is to produce answers to factoid questions based solely on the input question without any accompanying context or information. Additionally, open-domain question answering systems typically aim to provide objective answers to factual questions, making it easier to evaluate the model's performance.\", 'gold_answer_marketing': \"Answer: The term 'open-domain' in question answering refers to the lack of specific context or relevant information provided for any arbitrarily asked factual question. The model must generate answers solely based on the question itself.\"}\n",
            "81 {'question': 'How do various models handle the retrieval of relevant context for question answering from external knowledge sources?', 'gold_answer_research': 'Various models handle the retrieval of relevant context for question answering from external knowledge sources in different ways. Some models, like DenSPI, encode all text in the knowledge corpus at the phrase level and rely on a retriever to identify the most relevant phrase as the predicted answer. Others, like ORQA, jointly learn a retriever and reader QA model to optimize correct answers without needing ground-truth context passages during training. Additionally, methods like Self-Ask and IRCoT combine iterative prompting and queries to external sources to construct the thought process iteratively and retrieve relevant content. The effectiveness of these models depends on the retrieval quality and the integration of retrieved content into the prompt.', 'gold_answer_marketing': 'Various models handle the retrieval of relevant context for question answering from external knowledge sources by using retrieval models to surface context based on relevance, recency, and importance. The dense representations of a question and context passage are extracted using language models, and the dot-product of these representations is used as the retrieval score to rank and select the most relevant passages. Additionally, some models use bi-directional LSTM and reader models to extract answers from context documents.'}\n",
            "82 {'question': 'What are the differences between open-book and closed-book question answering?', 'gold_answer_research': 'In open-book question answering, the retriever-generator approach involves two stages where the second stage is to generate free text directly to answer the question. This is also known as generative question answering. On the other hand, closed-book question answering involves the use of generative language models that are pre-trained on a large collection of textual data and can answer questions without explicit context, similar to a closed-book exam. These models produce free text responses to questions without requiring explicit reading comprehension. Additionally, swapping the question with the evidence in closed-book QA demonstrations has been found to consistently yield lower results across all datasets.', 'gold_answer_marketing': 'Open-book question answering involves generating free text directly to answer a question, while closed-book question answering uses pre-trained language models to produce free text responses without explicit context.'}\n",
            "84 {'question': 'What are some of the concerns related to fine-tuning QA models with common datasets?', 'gold_answer_research': 'One concern related to fine-tuning QA models with common datasets is the significant overlap between questions in the train and test sets of public QA datasets. This overlap can potentially lead to biased performance evaluations and limit the generalizability of the model. Additionally, using datasets that primarily focus on tasks like classification and question answering may not fully capture the diverse range of tasks that language models are used for in practice. Furthermore, efforts to modify language models to mitigate harms, such as reducing toxicity, can inadvertently impact their ability to model text from under-represented groups.', 'gold_answer_marketing': \"One concern of fine-tuning QA models with common datasets is the significant overlap between questions in the training and test sets, which can impact the model's performance.\"}\n",
            "85 {'question': 'How does the performance of question answering models tend to vary with the size of the model?', 'gold_answer_research': 'The performance of question answering models tends to improve with the size of the model, with larger models like T5 with 11B parameters outperforming smaller models like DPR with 3 BERT-base models. However, increasing the size of the model may negatively affect its performance in certain cases. Research has shown that larger models have the capability to memorize answers seen during training and perform well on novel questions at test time, but dataset suitability is also important for optimal performance. Additionally, extensive analysis of chatbot performance using both human raters and AI evaluation has been conducted to determine the effectiveness of different models.', 'gold_answer_marketing': 'The performance of question answering models tends to improve with larger model sizes.'}\n",
            "86 {'question': 'What are the advantages and limitations of using generative language models for closed-book question answering?', 'gold_answer_research': 'The advantages of using generative language models for closed-book question answering include the ability to memorize factual knowledge within parameter weights and produce free text responses without explicit context. Additionally, models like RAG can be fine-tuned on various tasks for better performance. However, limitations may arise from the reliance on supervised learning, which can be costly and time-consuming, and may restrict the use on datasets that are not well-annotated. Furthermore, the performance of generative models may vary based on the model size and the quality of the retrieved relevant context.', 'gold_answer_marketing': 'Advantages: Generative language models can memorize factual knowledge and answer questions without explicit context, similar to a closed-book exam. Limitations: Reliance on supervised learning limits use on datasets that are not well-annotated and can be expensive and time-consuming to train extremely large models.'}\n",
            "87 {'question': 'What are the main components that complement the central controller in an autonomous agent system?', 'gold_answer_research': 'In an autonomous agent system powered by LLM, the main components that complement the central controller include Planning, Memory, and Task Decomposition. Planning involves breaking down tasks into subgoals and reflecting on past actions for refinement. Memory allows the agent to store and retrieve information for decision-making. Task Decomposition focuses on the reliability of the natural language interface and the use of expert modules for routing inquiries. Together, these components enhance the overall functionality and efficiency of the autonomous agent system.', 'gold_answer_marketing': 'The main components that complement the central controller in an autonomous agent system are Planning and Memory.'}\n",
            "88 {'question': 'How can subgoals and task decomposition improve the handling of complex tasks in autonomous systems?', 'gold_answer_research': 'Subgoals and task decomposition can improve the handling of complex tasks in autonomous systems by breaking down large tasks into smaller, more manageable subtasks. This enables the agent to efficiently navigate through the various steps required to complete the overall task. By reflecting on past actions and refining their approach, the agent can learn from mistakes and continuously improve the quality of their final results. This iterative process of breaking down tasks and refining actions ultimately leads to enhanced safety and reliability in autonomous systems.', 'gold_answer_marketing': 'Subgoals and task decomposition can break down large tasks into smaller, manageable steps, enabling efficient handling of complex tasks in autonomous systems. This allows the agent to plan ahead, reflect on past actions, learn from mistakes, and refine strategies for future steps, ultimately improving the quality of final results.'}\n",
            "89 {'question': 'What types of memory are leveraged in autonomous agents, and how do they differ?', 'gold_answer_research': 'In autonomous agents, two main types of memory are leveraged: short-term memory and long-term memory. Short-term memory is utilized for in-context learning, while long-term memory allows for the retention and recall of information over extended periods. The agents can interact with other agents and retain past experiences using these memory mechanisms. Short-term memory is essential for immediate learning, while long-term memory enables agents to store and retrieve a vast amount of information over time. These memory systems enable agents to make informed decisions based on past experiences and interactions.', 'gold_answer_marketing': 'Short-term memory and long-term memory are utilized in autonomous agents. Short-term memory is used for in-context learning, while long-term memory allows agents to retain and recall information over extended periods. Long-term memory often leverages external storage for fast retrieval.'}\n",
            "91 {'question': 'How does planning and reflection contribute to the iterative improvement of autonomous agents?', 'gold_answer_research': 'Planning and reflection play a crucial role in the iterative improvement of autonomous agents by enabling them to break down tasks into manageable subgoals, learn from past actions, and refine their strategies for future steps. Through task decomposition, agents can efficiently handle complex tasks, while self-criticism and self-reflection allow them to identify mistakes and make necessary adjustments, ultimately improving the quality of their final results. By synthesizing memories and guiding future behavior based on past events, the reflection mechanism helps agents make higher-level inferences over time, leading to continuous learning and enhancement in performance.', 'gold_answer_marketing': 'Planning and reflection contribute to the iterative improvement of autonomous agents by allowing them to break down tasks into manageable subgoals, learn from past actions, correct mistakes, and refine their approach for future steps. This process leads to better decision-making, higher-quality results, and overall improvement in performance over time.'}\n",
            "92 {'question': \"In what ways can an agent's performance be evaluated and refined over time?\", 'gold_answer_research': \"An agent's performance can be evaluated and refined over time by continuously reviewing and analyzing actions, constructively self-criticizing behavior, reflecting on past decisions and strategies, and aiming to complete tasks in the least number of steps. Additionally, the agent can evaluate the results of API calls and refine inputs if necessary, as well as break down large tasks into smaller subgoals for more efficient handling. Self-reflection, learning from mistakes, and refining actions for future steps can also improve the quality of final results.\", 'gold_answer_marketing': \"The speaker of the dialogue is 'agent'. An agent's performance can be evaluated and refined over time by continuously reviewing and analyzing actions, constructively self-criticizing big-picture behavior, reflecting on past decisions and strategies, being smart and efficient in completing tasks, breaking down tasks into smaller subgoals, self-reflection and refinement of actions, learning from mistakes, calling external APIs for additional information, and using memory, planning, and reflection mechanisms to improve behavior based on past experiences.\"}\n",
            "93 {'question': 'What challenges are faced in enhancing the long-term planning capabilities of autonomous agents?', 'gold_answer_research': 'Enhancing long-term planning capabilities of autonomous agents faces challenges such as effectively exploring the solution space over a lengthy history and adjusting plans when unexpected errors occur. LLMs struggle to adapt plans in response to errors, making them less robust compared to human learning from trial and error. To address this, agents need to improve task decomposition by breaking down large tasks into manageable subgoals and incorporate reflection mechanisms for self-criticism and learning from past actions. Additionally, the integration of external classical planners like PDDL can aid in long-horizon planning for complex tasks.', 'gold_answer_marketing': 'Challenges in long-term planning for autonomous agents include difficulty in task decomposition, adjusting plans when faced with unexpected errors, and lack of robustness compared to human learning from trial and error.'}\n",
            "94 {'question': 'How does the use of tools and external resources extend the capabilities of large language models in practical applications?', 'gold_answer_research': 'The use of tools and external resources extends the capabilities of large language models by fine-tuning them to leverage external tool APIs. This process involves expanding the dataset based on the potential improvement in model outputs from newly added API call annotations. By incorporating external resources, such as APIs, large language models can enhance their performance and adaptability for a wide range of practical applications, including speech recognition, machine translation, and natural language generation. This approach helps create more efficient and high-performing language models that can effectively handle diverse tasks and scenarios.', 'gold_answer_marketing': 'The use of tools and external resources allows large language models to access additional information and functions, improving their performance in various tasks such as speech recognition, machine translation, and natural language generation. By fine-tuning the models to utilize external APIs, the dataset is expanded and enriched, leading to more accurate and efficient model outputs. This approach enhances the adaptability and effectiveness of large language models in practical applications.'}\n",
            "95 {'question': 'What are the primary techniques involved in steering the behavior of language models without modifying their underlying architectures?', 'gold_answer_research': 'The primary techniques involved in steering the behavior of language models without modifying their underlying architectures include using language models to generate toxic outputs as part of a data augmentation pipeline, making models refuse certain user instructions, and combining Reinforcement Learning from Human Feedback (RLHF) with architectures that heavily use self-attention. Additionally, techniques such as supervised fine-tuning (SFT) and few-shot learning, or prompt engineering, are utilized to achieve precise control over the behavior of language models. These techniques focus on aligning language models with human intentions through iterative improvements and alignment research.', 'gold_answer_marketing': 'The primary techniques involved in steering the behavior of language models without modifying their underlying architectures include using language models to generate toxic outputs as part of a data augmentation pipeline, making models refuse certain user instructions, and combining reinforcement learning with architecture that heavily uses self-attention.'}\n",
            "96 {'question': 'How can the alignment and steerability of language models be influenced through specific engineering methods?', 'gold_answer_research': 'Specific engineering methods that can influence the alignment and steerability of language models include fine-tuning data selection, alignment method choice, and incorporating pretraining data into reinforcement learning from human feedback (RLHF) processes. Additionally, designing interfaces for human labelers to provide feedback to language models can play a crucial role in improving alignment. It is also important to consider factors like model response editing, generating critiques, and using diverse feedback mechanisms to enhance alignment and steerability in language models.', 'gold_answer_marketing': 'Promote alignment and steerability of language models through specific engineering methods like prompt engineering, fine-tuning data, and incorporating pretraining data into reinforcement learning from human feedback (RLHF).'}\n",
            "101 {'question': \"What innovative approaches are being explored to enhance language model's reasoning capabilities, specifically for complex reasoning tasks?\", 'gold_answer_research': \"Some innovative approaches being explored to enhance language model's reasoning capabilities for complex tasks include ReAct by Yao et al. (2023), which synergizes reasoning and acting in language models, Complexity-based prompting for multi-step reasoning by Fu et al. (2022), Rationale-augmented ensembles in language models by Wang et al. (2022), and Automatic chain of thought prompting in large language models by Zhang et al. (2022). These approaches focus on improving the model's ability to handle complex reasoning tasks by incorporating advanced techniques such as prompt-based learning, rationale augmentation, and automatic chaining of thoughts. Additionally, models like PAL and PoT offload complex computation and reasoning tasks by generating programming language statements to resolve natural language problems, enhancing the efficiency and performance of the language model.\", 'gold_answer_marketing': \"Researchers are exploring approaches like ReAct, complexity-based prompting, rationale-augmented ensembles, and automatic chain of thought prompting to enhance language model's reasoning capabilities for complex tasks.\"}\n",
            "102 {'question': 'How can external tools and APIs be integrated with language models to extend their capabilities and applications?', 'gold_answer_research': \"External tools and APIs can be integrated with language models by fine-tuning the models to learn how to use these external resources effectively. This involves expanding the dataset to determine if adding new API call annotations can enhance the model's output quality. By calling external APIs for additional information that may be missing from the model weights, such as current information or access to proprietary sources, the language model can be prompted further to generate more accurate and insightful responses. Additionally, incorporating tool use capabilities, like those seen in ChatGPT Plugins and OpenAI API function calling, can provide practical examples of how language models can benefit from external resources.\", 'gold_answer_marketing': \"External tools and APIs can be integrated with language models through techniques like fine-tuning the models to use the APIs for extra information, accessing external data sources, executing code, and enhancing the model's capabilities for tasks like speech recognition, machine translation, and natural language generation. This integration helps extend the applications of language models by providing access to additional resources and functionalities.\"}\n",
            "103 {'question': 'What kind of learning challenges does the attention mechanism address in neural machine translation?', 'gold_answer_research': 'The attention mechanism in neural machine translation addresses the challenge of memorizing long source sentences by creating shortcuts between the context vector and the entire source input. This allows for more efficient learning of dependencies between source and target sequences, regardless of the in-between distance. Additionally, the attention mechanism enables the model to learn correlations between current words and previous parts of the sentence, improving the overall translation quality. By adapting different types of attention weight matrices, the model can further enhance its learning capabilities.', 'gold_answer_marketing': 'The attention mechanism helps address the challenge of memorizing long source sentences and creating shortcuts between context vectors and entire source inputs in neural machine translation.'}\n",
            "104 {'question': 'How is the encoder-decoder architecture in seq2seq models affected by long input sequences?', 'gold_answer_research': 'In seq2seq models, the encoder-decoder architecture can be affected by long input sequences. This can lead to challenges in attending to the relevant parts of the input during decoding, potentially resulting in erroneous generation. Additionally, the design of the decoding strategy itself, such as using top-k sampling for improved generation diversity, can also contribute to issues like hallucinations. Therefore, careful consideration of handling long input sequences and refining decoding strategies is crucial for effective seq2seq model performance.', 'gold_answer_marketing': 'The encoder-decoder architecture in seq2seq models may struggle with long input sequences, leading to difficulties in generating accurate target sequences.'}\n",
            "105 {'question': 'What are the differences between soft and hard attention in the context of image caption generation?', 'gold_answer_research': \"Soft attention in image caption generation allows the model to distribute its focus over all patches in the source image, similar to the attention mechanism proposed by Bahdanau et al. in 2015. On the other hand, hard attention restricts the model to only attend to a specific patch of the image when generating a word. This distinction between soft and hard attention impacts how the model learns to align image features with generated words, influencing the quality and interpretability of the generated captions. Additionally, the choice between soft and hard attention can affect the model's ability to capture global dependencies within the image.\", 'gold_answer_marketing': \"Soft attention in image caption generation refers to alignment weights that are learned and placed 'softly' over all patches in the source image. This allows the model to focus on different regions of the image simultaneously. On the other hand, hard attention involves selecting a single aligned position for the current target word, making it more like a spotlight that focuses on specific areas of the image at a time.\"}\n",
            "106 {'question': 'Can you describe the multi-head self-attention mechanism in the transformer model?', 'gold_answer_research': \"In the transformer model, the multi-head self-attention mechanism runs the scaled dot-product attention multiple times in parallel instead of just once. This allows the model to jointly attend to information from different representation subspaces at different positions. The independent attention outputs are then concatenated and linearly transformed to the expected dimensions. This approach enhances the model's ability to capture relationships and dependencies across different parts of the input sequence, leading to improved performance in tasks such as machine reading and summarization.\", 'gold_answer_marketing': 'The multi-head self-attention mechanism in the transformer model runs scaled dot-product attention multiple times in parallel, allowing the model to jointly attend to information from different representation subspaces at different positions. The independent attention outputs are concatenated and linearly transformed into the expected dimensions.'}\n",
            "107 {'question': 'In what ways does SNAIL address the issue of positioning in the transformer model?', 'gold_answer_research': \"SNAIL addresses the issue of weakly incorporating sequential order in the transformer model by using self-attention to enhance performance. It formulates relative position using vector production and encodes absolute position information through a rotation matrix. Additionally, SNAIL mathematically illustrates the advantageous properties of this method when applied to the Transformer. This approach improves the model's ability to handle positional dependencies, particularly important for tasks like reinforcement learning.\", 'gold_answer_marketing': 'SNAIL addresses the issue of positioning in the transformer model by using self-attention to enhance the performance and incorporating relative position naturally through vector production and absolute position information through a rotation matrix.'}\n",
            "108 {'question': 'How does the Pointer Network differ from standard seq2seq models in dealing with sequential data?', 'gold_answer_research': 'The Pointer Network differs from standard seq2seq models in that it is specifically designed to handle problems where the output elements correspond to positions in an input sequence. Instead of using attention to blend hidden units like in seq2seq models, the Pointer Network directly points to the elements in the input sequence. This allows for greater flexibility in determining the output elements, which is particularly useful in tasks like sorting or the traveling salesman problem where the output categories are not predetermined. The architecture of a Pointer Network model includes an encoder-decoder setup with a focus on predicting or inferring specific elements in the input sequence based on their importance weights.', 'gold_answer_marketing': 'The Pointer Network differs from standard seq2seq models by being able to handle problems where the output elements correspond to positions in an input sequence, rather than using attention to blend hidden units.'}\n",
            "110 {'question': 'How does Neural Turing Machine (NTM) simulate the infinite memory characteristic of Turing machines?', 'gold_answer_research': \"Neural Turing Machine (NTM) simulates the infinite memory characteristic of Turing machines by coupling a neural network with external memory storage. The memory in NTM mimics the Turing machine tape, allowing the neural network to control operation heads to read from or write to the tape. However, the memory in NTM is finite, resembling more of a 'Neural von Neumann Machine' due to practical limitations in real modern computers. This limitation is addressed by various works that add memory capabilities to Transformers through recurrence, enhancing the model's ability to handle very long sequences while still maintaining attention mechanisms for efficient memory access.\", 'gold_answer_marketing': \"Neural Turing Machine (NTM) uses external memory storage that mimics the Turing machine tape, allowing the neural network to control operation heads to read from or write to the tape. However, the memory in NTM is finite, making it more similar to a 'Neural von Neumann Machine' rather than having infinite memory like a Turing machine.\"}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open(\"../data/validation_question_answers.json\", \"r\") as f:\n",
        "    validation_question_answers = json.load(f)\n",
        "    \n",
        "for i, sample in validation_question_answers.items():\n",
        "    print(i, sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code will only work the first time you run it. Subsequent runs will throw a 409, indicating that the datasets have already been created on LangSmith, in which case you can either (A) rip and replace them, or (B) load them from LangSmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "LangSmithConflictError",
          "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/w267-final-project-rag-pipeline/.venv/lib/python3.11/site-packages/langsmith/utils.py:150\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/w267-final-project-rag-pipeline/.venv/lib/python3.11/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/w267-final-project-rag-pipeline/.venv/lib/python3.11/site-packages/langsmith/client.py:765\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    759\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.session.request(\n\u001b[32m    760\u001b[39m         method,\n\u001b[32m    761\u001b[39m         _construct_url(\u001b[38;5;28mself\u001b[39m.api_url, pathname),\n\u001b[32m    762\u001b[39m         stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    763\u001b[39m         **request_kwargs,\n\u001b[32m    764\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m \u001b[43mls_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/w267-final-project-rag-pipeline/.venv/lib/python3.11/site-packages/langsmith/utils.py:152\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m requests.HTTPError(\u001b[38;5;28mstr\u001b[39m(e), response.text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mHTTPError\u001b[39m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mLangSmithConflictError\u001b[39m                    Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create dataset for Engineering\u001b[39;00m\n\u001b[32m     17\u001b[39m engineering_dataset_name = \u001b[33m\"\u001b[39m\u001b[33mw267-rag-validation-engineering\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m engineering_dataset = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengineering_dataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m client.create_examples(\n\u001b[32m     20\u001b[39m     dataset_id=engineering_dataset.id,\n\u001b[32m     21\u001b[39m     examples=examples_engineering\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Create dataset for Marketing\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/w267-final-project-rag-pipeline/.venv/lib/python3.11/site-packages/langsmith/client.py:3273\u001b[39m, in \u001b[36mClient.create_dataset\u001b[39m\u001b[34m(self, dataset_name, description, data_type, inputs_schema, outputs_schema, transformations, metadata)\u001b[39m\n\u001b[32m   3270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outputs_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3271\u001b[39m     dataset[\u001b[33m\"\u001b[39m\u001b[33moutputs_schema_definition\u001b[39m\u001b[33m\"\u001b[39m] = outputs_schema\n\u001b[32m-> \u001b[39m\u001b[32m3273\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3274\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3275\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/datasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_orjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3278\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3279\u001b[39m ls_utils.raise_for_status_with_text(response)\n\u001b[32m   3281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas.Dataset(\n\u001b[32m   3282\u001b[39m     **response.json(),\n\u001b[32m   3283\u001b[39m     _host_url=\u001b[38;5;28mself\u001b[39m._host_url,\n\u001b[32m   3284\u001b[39m     _tenant_id=\u001b[38;5;28mself\u001b[39m._get_optional_tenant_id(),\n\u001b[32m   3285\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/w267-final-project-rag-pipeline/.venv/lib/python3.11/site-packages/langsmith/client.py:810\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithNotFoundError(\n\u001b[32m    806\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    807\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    808\u001b[39m     )\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithConflictError(\n\u001b[32m    811\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConflict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    812\u001b[39m     )\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    814\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithError(\n\u001b[32m    815\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in LangSmith\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    816\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m API. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    817\u001b[39m     )\n",
            "\u001b[31mLangSmithConflictError\u001b[39m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
          ]
        }
      ],
      "source": [
        "# First, transform the existing dataset into LangSmith compatible examples:\n",
        "examples_engineering = []\n",
        "examples_marketing = []\n",
        "\n",
        "for sample in validation_question_answers.values():\n",
        "    examples_engineering.append({\n",
        "        \"inputs\": {\"question\": sample[\"question\"]},\n",
        "        \"outputs\": {\"answer\": sample[\"gold_answer_research\"]}\n",
        "    })\n",
        "\n",
        "    examples_marketing.append({\n",
        "        \"inputs\": {\"question\": sample[\"question\"]},\n",
        "        \"outputs\": {\"answer\": sample[\"gold_answer_marketing\"]}\n",
        "    })\n",
        "\n",
        "# Create dataset for Engineering\n",
        "engineering_dataset_name = \"w267-rag-validation-engineering\"\n",
        "engineering_dataset = client.create_dataset(dataset_name=engineering_dataset_name)\n",
        "client.create_examples(\n",
        "    dataset_id=engineering_dataset.id,\n",
        "    examples=examples_engineering\n",
        ")\n",
        "\n",
        "# Create dataset for Marketing\n",
        "marketing_dataset_name = \"w267-rag-validation-marketing\"\n",
        "marketing_dataset = client.create_dataset(dataset_name=marketing_dataset_name)\n",
        "client.create_examples(\n",
        "    dataset_id=marketing_dataset.id,\n",
        "    examples=examples_marketing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's how we can gracefully load them from LangSmith if they are already created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'w267-rag-validation-engineering' already exists, loading existing dataset.\n",
            "Dataset 'w267-rag-validation-marketing' already exists, loading existing dataset.\n"
          ]
        }
      ],
      "source": [
        "def get_or_create_dataset(client, dataset_name, examples):\n",
        "    if client.has_dataset(dataset_name=dataset_name):\n",
        "        print(f\"Dataset '{dataset_name}' already exists, loading existing dataset.\")\n",
        "        dataset = client.read_dataset(dataset_name=dataset_name)\n",
        "    else:\n",
        "        print(f\"Dataset '{dataset_name}' does not exist, creating it now.\")\n",
        "        dataset = client.create_dataset(dataset_name=dataset_name)\n",
        "        client.create_examples(dataset_id=dataset.id, examples=examples)\n",
        "    return dataset\n",
        "\n",
        "# Engineering dataset\n",
        "engineering_dataset_name = \"w267-rag-validation-engineering\"\n",
        "engineering_dataset = get_or_create_dataset(client, engineering_dataset_name, examples_engineering)\n",
        "\n",
        "# Marketing dataset\n",
        "marketing_dataset_name = \"w267-rag-validation-marketing\"\n",
        "marketing_dataset = get_or_create_dataset(client, marketing_dataset_name, examples_marketing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "# Grade output schema\n",
        "class CorrectnessGrade(TypedDict):\n",
        "    # Note that the order in the fields are defined is the order in which the model will generate them.\n",
        "    # It is useful to put explanations before responses because it forces the model to think through\n",
        "    # its final response before generating it:\n",
        "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
        "    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]\n",
        "\n",
        "# Grade prompt\n",
        "correctness_instructions = \"\"\"You are a teacher grading a quiz. \n",
        "\n",
        "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. \n",
        "\n",
        "Here is the grade criteria to follow:\n",
        "(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
        "(2) Ensure that the student answer does not contain any conflicting statements.\n",
        "(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
        "\n",
        "Correctness:\n",
        "A correctness value of True means that the student's answer meets all of the criteria.\n",
        "A correctness value of False means that the student's answer does not meet all of the criteria.\n",
        "\n",
        "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
        "\n",
        "Avoid simply stating the correct answer at the outset.\"\"\"\n",
        "\n",
        "# Grader LLM\n",
        "grader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(CorrectnessGrade, method=\"json_schema\", strict=True)\n",
        "\n",
        "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
        "    \"\"\"An evaluator for RAG answer accuracy\"\"\"\n",
        "    answers = f\"\"\"\\\n",
        "QUESTION: {inputs['question']}\n",
        "GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
        "STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
        "\n",
        "    # Run evaluator\n",
        "    grade = grader_llm.invoke([\n",
        "        {\"role\": \"system\", \"content\": correctness_instructions}, \n",
        "        {\"role\": \"user\", \"content\": answers}\n",
        "    ])\n",
        "    return grade[\"correct\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grade output schema\n",
        "class RelevanceGrade(TypedDict):\n",
        "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
        "    relevant: Annotated[bool, ..., \"Provide the score on whether the answer addresses the question\"]\n",
        "\n",
        "# Grade prompt\n",
        "relevance_instructions=\"\"\"You are a teacher grading a quiz. \n",
        "\n",
        "You will be given a QUESTION and a STUDENT ANSWER. \n",
        "\n",
        "Here is the grade criteria to follow:\n",
        "(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION\n",
        "(2) Ensure the STUDENT ANSWER helps to answer the QUESTION\n",
        "\n",
        "Relevance:\n",
        "A relevance value of True means that the student's answer meets all of the criteria.\n",
        "A relevance value of False means that the student's answer does not meet all of the criteria.\n",
        "\n",
        "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
        "\n",
        "Avoid simply stating the correct answer at the outset.\"\"\"\n",
        "\n",
        "# Grader LLM\n",
        "relevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(RelevanceGrade, method=\"json_schema\", strict=True)\n",
        "\n",
        "# Evaluator\n",
        "def relevance(inputs: dict, outputs: dict) -> bool:\n",
        "    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"\n",
        "    answer = f\"QUESTION: {inputs['question']}\\nSTUDENT ANSWER: {outputs['answer']}\"\n",
        "    grade = relevance_llm.invoke([\n",
        "        {\"role\": \"system\", \"content\": relevance_instructions}, \n",
        "        {\"role\": \"user\", \"content\": answer}\n",
        "    ])\n",
        "    return grade[\"relevant\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grade output schema\n",
        "class GroundedGrade(TypedDict):\n",
        "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
        "    grounded: Annotated[bool, ..., \"Provide the score on if the answer hallucinates from the documents\"]\n",
        "\n",
        "# Grade prompt\n",
        "grounded_instructions = \"\"\"You are a teacher grading a quiz. \n",
        "\n",
        "You will be given FACTS and a STUDENT ANSWER. \n",
        "\n",
        "Here is the grade criteria to follow:\n",
        "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
        "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
        "\n",
        "Grounded:\n",
        "A grounded value of True means that the student's answer meets all of the criteria.\n",
        "A grounded value of False means that the student's answer does not meet all of the criteria.\n",
        "\n",
        "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
        "\n",
        "Avoid simply stating the correct answer at the outset.\"\"\"\n",
        "\n",
        "# Grader LLM \n",
        "grounded_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(GroundedGrade, method=\"json_schema\", strict=True)\n",
        "\n",
        "# Evaluator\n",
        "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
        "    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"\n",
        "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
        "    answer = f\"FACTS: {doc_string}\\nSTUDENT ANSWER: {outputs['answer']}\"\n",
        "    grade = grounded_llm.invoke([{\"role\": \"system\", \"content\": grounded_instructions}, {\"role\": \"user\", \"content\": answer}])\n",
        "    return grade[\"grounded\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grade output schema\n",
        "class RetrievalRelevanceGrade(TypedDict):\n",
        "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
        "    relevant: Annotated[bool, ..., \"True if the retrieved documents are relevant to the question, False otherwise\"]\n",
        "\n",
        "# Grade prompt\n",
        "retrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. \n",
        "\n",
        "You will be given a QUESTION and a set of FACTS provided by the student. \n",
        "\n",
        "Here is the grade criteria to follow:\n",
        "(1) You goal is to identify FACTS that are completely unrelated to the QUESTION\n",
        "(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant\n",
        "(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is met\n",
        "\n",
        "Relevance:\n",
        "A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.\n",
        "A relevance value of False means that the FACTS are completely unrelated to the QUESTION.\n",
        "\n",
        "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
        "\n",
        "Avoid simply stating the correct answer at the outset.\"\"\"\n",
        "\n",
        "# Grader LLM\n",
        "retrieval_relevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\", strict=True)\n",
        "\n",
        "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
        "    \"\"\"An evaluator for document relevance\"\"\"\n",
        "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
        "    answer = f\"FACTS: {doc_string}\\nQUESTION: {inputs['question']}\"\n",
        "\n",
        "    # Run evaluator\n",
        "    grade = retrieval_relevance_llm.invoke([\n",
        "        {\"role\": \"system\", \"content\": retrieval_relevance_instructions}, \n",
        "        {\"role\": \"user\", \"content\": answer}\n",
        "    ])\n",
        "    return grade[\"relevant\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# These should have already been initialized way up above - but in case not here are the RAG initializations:\n",
        "\n",
        "# rag_system_cohere = RAGSystem(\n",
        "#     vector_db_manager=vdm,\n",
        "#     engineering_template_path=engineering_template,\n",
        "#     marketing_template_path=marketing_template,\n",
        "#     cohere_api_key=cohere_api_key,\n",
        "#     use_mistral=False,\n",
        "#     use_cohere=True,\n",
        "#     mistral_model_name=SupportedGeneratorModels.MistralInstructV2,\n",
        "#     top_k=4,\n",
        "# )\n",
        "# rag_system_mistral = RAGSystem(\n",
        "#     vector_db_manager=vdm,\n",
        "#     engineering_template_path=engineering_template,\n",
        "#     marketing_template_path=marketing_template,\n",
        "#     cohere_api_key=cohere_api_key,\n",
        "#     use_mistral=True,\n",
        "#     use_cohere=False,\n",
        "#     mistral_model_name=SupportedGeneratorModels.MistralInstructV2,\n",
        "#     top_k=4,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rag_system = rag_system_cohere\n",
        "rag_system = rag_system_mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def engineering_target(inputs: dict) -> dict:\n",
        "    question = inputs[\"question\"]\n",
        "    answer = rag_system.invoke(Team.Engineering, question)\n",
        "    retrieved_docs = rag_system.query_vectorstore(question)\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"documents\": retrieved_docs\n",
        "    }\n",
        "\n",
        "def marketing_target(inputs: dict) -> dict:\n",
        "    question = inputs[\"question\"]\n",
        "    answer = rag_system.invoke(Team.Marketing, question)\n",
        "    retrieved_docs = rag_system.query_vectorstore(question)\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"documents\": retrieved_docs\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'rag-mistral-eval-engineering-c771cc65' at:\n",
            "https://smith.langchain.com/o/b90a129f-4968-4c94-8e16-5eba475d05e8/datasets/b1cd9c51-0a5b-40f9-9bc9-d26f8ad660be/compare?selectedSessions=2199a4db-d1df-4a7f-8c96-6e014c27396e\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "736f9a1bbe8945af8b35b2b42d35a048",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'rag-mistral-eval-marketing-d0bbca6c' at:\n",
            "https://smith.langchain.com/o/b90a129f-4968-4c94-8e16-5eba475d05e8/datasets/fffe62a0-c961-4cfc-8600-436128b8dbbb/compare?selectedSessions=0d115463-1e3d-4663-a5af-c0ee202db797\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9ebc36946604e7dbde21f7d76117e2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# Engineering evaluation\n",
        "engineering_experiment_results = client.evaluate(\n",
        "    engineering_target,\n",
        "    data=engineering_dataset_name,\n",
        "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
        "    experiment_prefix=\"rag-mistral-eval-engineering\",\n",
        "    metadata=rag_system.get_config(),\n",
        ")\n",
        "\n",
        "# Marketing evaluation\n",
        "marketing_experiment_results = client.evaluate(\n",
        "    marketing_target,\n",
        "    data=marketing_dataset_name,\n",
        "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
        "    experiment_prefix=\"rag-mistral-eval-marketing\",\n",
        "    metadata=rag_system.get_config(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore results locally as a dataframe if you have pandas installed\n",
        "experiment_results.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset(name='w267-rag-validation-engineering', description=None, data_type=<DataType.kv: 'kv'>, id=UUID('b1cd9c51-0a5b-40f9-9bc9-d26f8ad660be'), created_at=datetime.datetime(2025, 3, 23, 14, 44, 43, 573827, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 3, 23, 14, 44, 43, 573827, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None)"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "engineering_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'generator' object has no attribute 'count'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\n",
            "\u001b[31mAttributeError\u001b[39m: 'generator' object has no attribute 'count'"
          ]
        }
      ],
      "source": [
        "client.list_examples()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "012e63ef86894290a11080a8b22bb903": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08c4c2618862443494abc48f8557f955": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a93364cd1034f87a1ad939510bd13c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f2eb670a6bf4423ab6c6642cbca8d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "100da52bb40f4e56b7ab446419347669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6245ccaefe141e883b96e871524a876",
            "placeholder": "​",
            "style": "IPY_MODEL_ece2931f44e349c39794259a530d05e3",
            "value": "config.json: 100%"
          }
        },
        "147f9868c769437c987a965d1100026c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14896be7b47d41bfac1dd5c932beff0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee83dd1d169940c18301c0bb9958098e",
            "placeholder": "​",
            "style": "IPY_MODEL_6f826325488e4933a4df3a87b9a31e51",
            "value": "model.safetensors: 100%"
          }
        },
        "1639f2d1172746c2b4c32f033ce6625f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16e0fa8f37dd46d6811f2c160bf05547",
            "placeholder": "​",
            "style": "IPY_MODEL_08c4c2618862443494abc48f8557f955",
            "value": "config.json: 100%"
          }
        },
        "16a1008d9c134f798e2d23bbb3477b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16e0fa8f37dd46d6811f2c160bf05547": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d730dff53b141a48fe12b13d8440257": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dbdc9d94b4e4186b204c845eddb0beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fb65402b1a646ddb85b7fecb9655677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_575341da64d5420081312aab56010045",
              "IPY_MODEL_c528f458fa124c67a4193dacb604816e",
              "IPY_MODEL_cf65282b0fa34e0ab410ce0de848cca3"
            ],
            "layout": "IPY_MODEL_0f2eb670a6bf4423ab6c6642cbca8d3b"
          }
        },
        "20fa5573029f4fd7a9f78ca7bb67309f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "224b7fab89744239a8dd747e5be89aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27678faf333f408788e3269b37bcec0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2788ddc0936e4e14b17108eae5861e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14896be7b47d41bfac1dd5c932beff0a",
              "IPY_MODEL_31d69c318150444eabca6bf57abe1ec8",
              "IPY_MODEL_b4634cba00df4a91a0cc06ba70bb07c6"
            ],
            "layout": "IPY_MODEL_f1c451ffac75420092bd6e6488cca4c8"
          }
        },
        "29e4769caac44fd0acbcbeaa19fed14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c16ea58087d44dda0a307934e50912b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31747ba9223f4d61806a7a34216330b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31d69c318150444eabca6bf57abe1ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba1659f4589f46089ba640165dc4d181",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f09141374824d3ea7569c3982f8ae50",
            "value": 437971872
          }
        },
        "341b7510fa9b433392bcffad7114eeee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35892a2030304c99a3a32f676633482b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d61a6fade84aeeb2554bda794b08f4",
            "placeholder": "​",
            "style": "IPY_MODEL_488080359cf041d18894f7d40988a21d",
            "value": " 363/363 [00:00&lt;00:00, 35.7kB/s]"
          }
        },
        "360688c187f5463cb9bb8dd2157cefa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27678faf333f408788e3269b37bcec0f",
            "placeholder": "​",
            "style": "IPY_MODEL_9355d75a5e364e88a021ef8077bfc4b0",
            "value": " 229/229 [00:00&lt;00:00, 18.6kB/s]"
          }
        },
        "36579715748f42eab854917f3484ee08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40e76af80291407a951555fd57e99149",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dbdc9d94b4e4186b204c845eddb0beb",
            "value": 190
          }
        },
        "387bd51ec7e645e88e74c80f6f12e78a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c181dc7cab144be08b4cabe38879aefe",
            "placeholder": "​",
            "style": "IPY_MODEL_0a93364cd1034f87a1ad939510bd13c5",
            "value": "modules.json: 100%"
          }
        },
        "3dbadc5dd0f44239bd049ed07b6a669b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e48cbd2e3f4456cbc8b2d2cf8c2994a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40e76af80291407a951555fd57e99149": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45e9c00c10b447c88df63f18efb52632": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf20ba7837ed4d6bbc47d49e442ad45f",
              "IPY_MODEL_e4dc7967c2ae4620974eda53f724be7d",
              "IPY_MODEL_4fc52afe944d41b69c8bdc3a84effa54"
            ],
            "layout": "IPY_MODEL_1d730dff53b141a48fe12b13d8440257"
          }
        },
        "47e5ffc30ef04e4a8df75850b1ba1f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc694ff92d324d6084504808aad50956",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4f2b72c2bbd49f0b03d1b92540d2030",
            "value": 571
          }
        },
        "488080359cf041d18894f7d40988a21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fc52afe944d41b69c8bdc3a84effa54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dbadc5dd0f44239bd049ed07b6a669b",
            "placeholder": "​",
            "style": "IPY_MODEL_9528790f946949e596eb5c8d3cf601f8",
            "value": " 53.0/53.0 [00:00&lt;00:00, 4.12kB/s]"
          }
        },
        "5268c3134818448eaa0731511b2cd7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53436395dff1406ab0fe30b82f1a8aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d0ff8830704b6ebde0ee6b97a61027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54a79bce342e4a72901998f90992f3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a02e6283104a4a6786e4b65321b956b8",
            "max": 212,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be3a3ddde3584583a0fe60aecfc63563",
            "value": 212
          }
        },
        "56bd1b6b90e24a7cba6bd503e6656939": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "575341da64d5420081312aab56010045": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e080fb08f4c541c49dd85d2e76790fa2",
            "placeholder": "​",
            "style": "IPY_MODEL_af5ef67c23a44bc68c3b95b30c3f055e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5ad4674dd3914c90a4c63c8466f008fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b231cf463c844fcb520e1055acde69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bacc640b4edc42069130e49a98048829",
            "max": 229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fc2ed9b65664d5e95d3acf7eaef0159",
            "value": 229
          }
        },
        "5c52155aa938499cb7df5c766e050d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b10b17d92ab14134999d5913084aaad2",
              "IPY_MODEL_95eb41cb46a74e62879827bfe2c1bb4c",
              "IPY_MODEL_f1242bf788f34a548e16f58cdee2d04a"
            ],
            "layout": "IPY_MODEL_69b8a35bfada4eb9a37f47ce4865c296"
          }
        },
        "5d9fdb26d12f40d2bd428b7f86a9b45b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6212177143bf4c2eb9e66934efb49d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e48cbd2e3f4456cbc8b2d2cf8c2994a",
            "placeholder": "​",
            "style": "IPY_MODEL_2c16ea58087d44dda0a307934e50912b",
            "value": " 8.71k/8.71k [00:00&lt;00:00, 836kB/s]"
          }
        },
        "626b1c5f885a4a16b16cb99572dad3b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69b8a35bfada4eb9a37f47ce4865c296": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b681faa8ddf48d2ab966be163ad3e40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b691b1d777f4d348223bb09afc0643a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_100da52bb40f4e56b7ab446419347669",
              "IPY_MODEL_36579715748f42eab854917f3484ee08",
              "IPY_MODEL_c4f5f399f1384aeea27f03017a2ab6dc"
            ],
            "layout": "IPY_MODEL_5d9fdb26d12f40d2bd428b7f86a9b45b"
          }
        },
        "6f09141374824d3ea7569c3982f8ae50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f826325488e4933a4df3a87b9a31e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71dd30b8f023404684ef8439f603b675": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "731220e2bdcf46139f4370eac93ce5ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74745948e0e24210a0fc928052c2c299": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7688794547ec427989ee05f8f86823f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f898238cd8a448838d626679bf4114c7",
            "placeholder": "​",
            "style": "IPY_MODEL_29e4769caac44fd0acbcbeaa19fed14a",
            "value": " 571/571 [00:00&lt;00:00, 54.2kB/s]"
          }
        },
        "797fd7c0ae104bc4a9f5ca7a46b5f562": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8085f68b3f9e4651a95b7362053d0802": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8511aea1b0d4374a82f0595061f46c7",
            "placeholder": "​",
            "style": "IPY_MODEL_866bed08b3a749b7b052e6ef03d4599d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "816c91be94db435c92b6f88b46776de6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86511ea338cf4b728c26c59c77355999": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e95a45d783834274998e5226994e3f0b",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a06059d0740540959ef4b732875487ab",
            "value": 231536
          }
        },
        "866bed08b3a749b7b052e6ef03d4599d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d6cce2936ab4e6aa33cca2c3687ca20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_797fd7c0ae104bc4a9f5ca7a46b5f562",
            "max": 8714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffcaf0176810401d8baf5a709408be58",
            "value": 8714
          }
        },
        "8fc2ed9b65664d5e95d3acf7eaef0159": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9355d75a5e364e88a021ef8077bfc4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9528790f946949e596eb5c8d3cf601f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95eb41cb46a74e62879827bfe2c1bb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be20bc4f71e744fda9c25788f244b8a5",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ece26bd7586142a3be5db5ab84dfb039",
            "value": 466021
          }
        },
        "989460806f7c423f96741bc259a4e29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98b498a56adc4198a56dd83e8ee0102c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a02e6283104a4a6786e4b65321b956b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a043c64ce355492988a41a4b893ecd8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06059d0740540959ef4b732875487ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4245fd8f28d47d5bad13aa2a5748a50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6245ccaefe141e883b96e871524a876": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af5ef67c23a44bc68c3b95b30c3f055e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b051a369955c4ab5adcbe5363b3b53ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d93d30d9cfac47a0ab6bf1581780d8cc",
              "IPY_MODEL_8d6cce2936ab4e6aa33cca2c3687ca20",
              "IPY_MODEL_6212177143bf4c2eb9e66934efb49d11"
            ],
            "layout": "IPY_MODEL_224b7fab89744239a8dd747e5be89aaa"
          }
        },
        "b0e73b5521554aff8b6cb738d7b6d57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b10b17d92ab14134999d5913084aaad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20fa5573029f4fd7a9f78ca7bb67309f",
            "placeholder": "​",
            "style": "IPY_MODEL_989460806f7c423f96741bc259a4e29b",
            "value": "tokenizer.json: 100%"
          }
        },
        "b4634cba00df4a91a0cc06ba70bb07c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16a1008d9c134f798e2d23bbb3477b7e",
            "placeholder": "​",
            "style": "IPY_MODEL_c0584e8a421f40de9f4d01cc70f18123",
            "value": " 438M/438M [00:02&lt;00:00, 232MB/s]"
          }
        },
        "b51e3615d19a4beb93eb8a7713db108b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5796395032c4ab79b79bd21b4a1b2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98b498a56adc4198a56dd83e8ee0102c",
            "placeholder": "​",
            "style": "IPY_MODEL_5268c3134818448eaa0731511b2cd7dc",
            "value": " 212/212 [00:00&lt;00:00, 14.2kB/s]"
          }
        },
        "b5a654cc87d24eafb6996aacb1869e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8511aea1b0d4374a82f0595061f46c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba1659f4589f46089ba640165dc4d181": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bacc640b4edc42069130e49a98048829": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc694ff92d324d6084504808aad50956": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd394eb35cb9403184914c64a68b91d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_341b7510fa9b433392bcffad7114eeee",
            "placeholder": "​",
            "style": "IPY_MODEL_56bd1b6b90e24a7cba6bd503e6656939",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "be20bc4f71e744fda9c25788f244b8a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3a3ddde3584583a0fe60aecfc63563": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be7bc2efd17943449852bee5661c02b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4245fd8f28d47d5bad13aa2a5748a50",
            "placeholder": "​",
            "style": "IPY_MODEL_b0e73b5521554aff8b6cb738d7b6d57b",
            "value": " 232k/232k [00:00&lt;00:00, 3.38MB/s]"
          }
        },
        "c0584e8a421f40de9f4d01cc70f18123": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c17de30438604cb1871664935ee20e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c82268cf3f44349b552c1a21a53a0b",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4dbb25b16c247648c54f5c3a55460af",
            "value": 363
          }
        },
        "c181dc7cab144be08b4cabe38879aefe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4dbb25b16c247648c54f5c3a55460af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4f5f399f1384aeea27f03017a2ab6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_626b1c5f885a4a16b16cb99572dad3b7",
            "placeholder": "​",
            "style": "IPY_MODEL_74745948e0e24210a0fc928052c2c299",
            "value": " 190/190 [00:00&lt;00:00, 18.9kB/s]"
          }
        },
        "c528f458fa124c67a4193dacb604816e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc7c681203ba4c2a86a8b3124c7c4715",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d79ecc22b0204a14841c9214a8da732e",
            "value": 239
          }
        },
        "c5640836ba76486dbcd322797c65aa56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8085f68b3f9e4651a95b7362053d0802",
              "IPY_MODEL_c17de30438604cb1871664935ee20e7d",
              "IPY_MODEL_35892a2030304c99a3a32f676633482b"
            ],
            "layout": "IPY_MODEL_147f9868c769437c987a965d1100026c"
          }
        },
        "c8d61a6fade84aeeb2554bda794b08f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7c681203ba4c2a86a8b3124c7c4715": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf20ba7837ed4d6bbc47d49e442ad45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9d63e9475274a4d80ccfad50caa2130",
            "placeholder": "​",
            "style": "IPY_MODEL_012e63ef86894290a11080a8b22bb903",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "cf65282b0fa34e0ab410ce0de848cca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_816c91be94db435c92b6f88b46776de6",
            "placeholder": "​",
            "style": "IPY_MODEL_b5a654cc87d24eafb6996aacb1869e3e",
            "value": " 239/239 [00:00&lt;00:00, 25.4kB/s]"
          }
        },
        "d79ecc22b0204a14841c9214a8da732e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7f8bce676ed47c6bfa79a7c469102b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3427b9fd72e49b0b31c5b693aa265f0",
              "IPY_MODEL_86511ea338cf4b728c26c59c77355999",
              "IPY_MODEL_be7bc2efd17943449852bee5661c02b3"
            ],
            "layout": "IPY_MODEL_5ad4674dd3914c90a4c63c8466f008fd"
          }
        },
        "d93d30d9cfac47a0ab6bf1581780d8cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_731220e2bdcf46139f4370eac93ce5ae",
            "placeholder": "​",
            "style": "IPY_MODEL_f6251266f701456a97b8cab8472d0027",
            "value": "README.md: 100%"
          }
        },
        "e080fb08f4c541c49dd85d2e76790fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e312eac78a374af88fc156e5a9e87f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3427b9fd72e49b0b31c5b693aa265f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71dd30b8f023404684ef8439f603b675",
            "placeholder": "​",
            "style": "IPY_MODEL_31747ba9223f4d61806a7a34216330b2",
            "value": "vocab.txt: 100%"
          }
        },
        "e4dc7967c2ae4620974eda53f724be7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f18f9b575f934fe9a5ba75e98bce11c7",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e312eac78a374af88fc156e5a9e87f7a",
            "value": 53
          }
        },
        "e4f2b72c2bbd49f0b03d1b92540d2030": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e95a45d783834274998e5226994e3f0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9d63e9475274a4d80ccfad50caa2130": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea33c75804854721b3856293c85da72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1639f2d1172746c2b4c32f033ce6625f",
              "IPY_MODEL_47e5ffc30ef04e4a8df75850b1ba1f39",
              "IPY_MODEL_7688794547ec427989ee05f8f86823f8"
            ],
            "layout": "IPY_MODEL_a043c64ce355492988a41a4b893ecd8f"
          }
        },
        "ece26bd7586142a3be5db5ab84dfb039": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ece2931f44e349c39794259a530d05e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edad1644d5804c61bc16163b9d6f0cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd394eb35cb9403184914c64a68b91d6",
              "IPY_MODEL_54a79bce342e4a72901998f90992f3e9",
              "IPY_MODEL_b5796395032c4ab79b79bd21b4a1b2d0"
            ],
            "layout": "IPY_MODEL_53436395dff1406ab0fe30b82f1a8aaa"
          }
        },
        "ee83dd1d169940c18301c0bb9958098e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1242bf788f34a548e16f58cdee2d04a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b51e3615d19a4beb93eb8a7713db108b",
            "placeholder": "​",
            "style": "IPY_MODEL_53d0ff8830704b6ebde0ee6b97a61027",
            "value": " 466k/466k [00:00&lt;00:00, 5.83MB/s]"
          }
        },
        "f18f9b575f934fe9a5ba75e98bce11c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c451ffac75420092bd6e6488cca4c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c82268cf3f44349b552c1a21a53a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f61bb98c3b3348dab028a665ae8f0c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_387bd51ec7e645e88e74c80f6f12e78a",
              "IPY_MODEL_5b231cf463c844fcb520e1055acde69c",
              "IPY_MODEL_360688c187f5463cb9bb8dd2157cefa2"
            ],
            "layout": "IPY_MODEL_6b681faa8ddf48d2ab966be163ad3e40"
          }
        },
        "f6251266f701456a97b8cab8472d0027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f898238cd8a448838d626679bf4114c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffcaf0176810401d8baf5a709408be58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
